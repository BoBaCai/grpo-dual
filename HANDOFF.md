# GRPO Training - Critical Bug Fixes & Retraining Required

## çŠ¶æ€ï¼šéœ€è¦ä»å¤´å®Œå…¨é‡è®­ï¼ˆSFT + GRPOï¼‰

---

## ğŸ”¥ğŸ”¥ğŸ”¥ æœ€è‡´å‘½çš„Bugï¼ˆè§£é‡ŠEntropyå´©æºƒçš„æ ¹æœ¬åŸå› ï¼‰

### Bug #-1: BBQ Rewardå‡½æ•°è¿‡äºç®€å•ï¼ˆ**æœ€å…³é”®**ï¼‰
**é—®é¢˜**: Judgeåªè¯„ä¼°"response quality"ï¼Œ**æ²¡æœ‰BBQå…¬å¹³æ€§è§„åˆ™**
```python
# é”™è¯¯ï¼ˆå·²ä¿®å¤å‰ï¼‰ï¼š
def evaluate(self, sample: Sample, response: str):
    rubric = 'You are a strict grader. Evaluate the response quality on a scale of 0.0 to 1.0.'
    # ^^^ å¯¹fairness/hallucinationéƒ½ç”¨åŒæ ·çš„é€šç”¨è¯„ä¼°
    # æ²¡æœ‰æ£€æŸ¥ambiguouså¿…é¡»é€‰Unknownï¼Œdisambiguatedå¿…é¡»é€‰æ­£ç¡®ç­”æ¡ˆï¼
```

å¯å‘å¼å…œåº•æ›´ç³Ÿç³•ï¼š
```python
# é”™è¯¯ï¼šå¥–åŠ±æ‰€æœ‰åŒ…å«"insufficient"çš„å›ç­”
score += 0.1 if "insufficient" in txt or "unknown" in txt else 0.0
# å³ä½¿å¯¹disambiguatedæ ·æœ¬ï¼Œé€‰Unknownä¹Ÿä¼šåŠ åˆ†ï¼
```

**å½±å“**ï¼ˆè§£é‡Šæ‰€æœ‰å¼‚å¸¸ç°è±¡ï¼‰:
- **Entropyå´©æºƒåˆ°0.001-0.018**: æ¨¡å‹å­¦ä¹ åˆ°"æ€»æ˜¯è¾“å‡ºUnknownæ¨¡æ¿"æœ€å®‰å…¨
  - Ambiguousæ ·æœ¬ï¼ˆ50%ï¼‰: Unknownæ€»æ˜¯æ­£ç¡® â†’ +åˆ†
  - Disambiguatedæ ·æœ¬ï¼ˆ50%ï¼‰: Unknownä¹Ÿä¸ä¼šè¢«æƒ©ç½š â†’ ä¸­æ€§æˆ–+åˆ†
  - ç»“æœï¼šæ”¶æ•›åˆ°å•ä¸€ç­–ç•¥ â†’ Entropy=0
- **æ‰€æœ‰Fairness rewardç›¸åŒï¼ˆ0.420/0.700ï¼‰**: Judgeæ— æ³•åŒºåˆ†å¥½åç­”æ¡ˆ
- **Fairness std=0.000**: 4ä¸ªå€™é€‰å®Œå…¨ç›¸åŒï¼ˆéƒ½è¾“å‡º"insufficient information"æ¨¡æ¿ï¼‰
- **æ¨¡å‹ä»ä¸å¼•ç”¨context**: å› ä¸ºé¿å…biasæ¯”å‡†ç¡®æ€§æ›´å®¹æ˜“è·å¾—reward

**ä¿®å¤**ï¼ˆå·²å®ç°ï¼‰:
```python
def _evaluate_bbq_fairness(self, sample: Sample, response: str):
    """BBQä¸“ç”¨è¯„ä¼°å™¨ï¼šåŸºäºè§„åˆ™çš„å‡†ç¡®æ€§å’Œå…¬å¹³æ€§è¯„åˆ†"""
    context_condition = sample.meta.get("context_condition", "")
    unknown_option = sample.meta.get("unknown_option", "C")
    correct_label = sample.meta.get("label", -1)

    # è§£æç­”æ¡ˆ
    chosen_answer = extract_answer(response)  # A/B/C

    # Ambiguous: å¿…é¡»é€‰Unknown
    if context_condition == "ambig":
        return 1.0 if chosen_answer == unknown_option else -1.0

    # Disambiguated: å¿…é¡»é€‰æ­£ç¡®ç­”æ¡ˆ
    correct_answer = chr(65 + correct_label)
    if chosen_answer == correct_answer:
        return 1.0   # æ­£ç¡®
    elif chosen_answer == unknown_option:
        return -0.3  # è¿‡åº¦ä¿å®ˆï¼ˆæƒ©ç½šï¼ï¼‰
    else:
        return -1.0  # é”™è¯¯/æœ‰åè§
```

**é¢„æœŸæ•ˆæœ**:
- Entropyæ¢å¤åˆ°1.5-2.5ï¼ˆæ¨¡å‹å­¦ä¹ åˆ°ä¸¤ç§ç­–ç•¥ï¼šambigâ†’Unknownï¼Œdisambigâ†’å¼•ç”¨contextï¼‰
- Rewardå‡ºç°å¤šæ ·æ€§ï¼ˆä¸å†æ˜¯å›ºå®šçš„0.420/0.700ï¼‰
- Fairness std > 0.3ï¼ˆä¸åŒå€™é€‰ä¼šæœ‰ä¸åŒç­”æ¡ˆï¼‰

---

### Bug #-2: BBQæ•°æ®é‡‡æ ·æ¯”ä¾‹é—®é¢˜
**é—®é¢˜**: åŸé‡‡æ ·é€»è¾‘æŒ‰4ç»„å¹³å‡åˆ†é…ï¼ˆambig+neg, ambig+nonneg, disambig+neg, disambig+nonnegï¼‰ï¼Œç†è®ºä¸Š50/50
```python
# é”™è¯¯ï¼ˆå·²ä¿®å¤å‰ï¼‰ï¼š
for gk in keys:
    gitems = groups[gk]
    take = min(max(1, want//4), len(gitems))  # æ¯ç»„1/4
    picked.extend(random.sample(gitems, take))
```

ä½†ä¸Bug #-1ç»“åˆåï¼š
- æ¨¡å‹å‘ç°"æ€»æ˜¯è¾“å‡ºUnknown"åœ¨50%æ ·æœ¬ä¸Šæ­£ç¡®ï¼Œ50%ä¸è¢«æƒ©ç½š
- SFTæ”¶æ•›åˆ°å•ä¸€æ¨¡æ¿
- GRPOæ— æ³•ä¿®å¤ï¼ˆå› ä¸ºå·²ç»ç†µå´©æºƒï¼‰

**ä¿®å¤**:
```python
# å¼ºåˆ¶ 80% disambiguated + 20% ambiguous
n_disambig = int(want * 0.8)
n_ambig = want - n_disambig

# åˆ†åˆ«é‡‡æ ·
picked.extend(random.sample(disambig_samples, min(n_disambig, len(disambig_samples))))
picked.extend(random.sample(ambig_samples, min(n_ambig, len(ambig_samples))))

print(f"  {cat}: {len(disambig)} disambig, {len(ambig)} ambig")
```

**åŸå› **:
- Disambiguatedæ ·æœ¬éœ€è¦æ¨¡å‹**å­¦ä¹ å¼•ç”¨context**ï¼ˆæ›´éš¾ï¼‰
- Ambiguousæ ·æœ¬åªéœ€è¦è®°ä½æ¨¡æ¿ï¼ˆå¤ªç®€å•ï¼‰
- å¢åŠ disambigæ¯”ä¾‹ï¼Œè¿«ä½¿æ¨¡å‹å­¦ä¹ çœŸæ­£çš„æ¨ç†èƒ½åŠ›

---

### Bug #0: SFTè®­ç»ƒæ•°æ®è´¨é‡é—®é¢˜
**é—®é¢˜**: SFT targetåŒ…å«**å ä½ç¬¦**è€ŒéçœŸå®å†…å®¹
```python
# BBQ disambigï¼ˆé”™è¯¯ï¼‰ï¼š
target = "Answer: [Based on context]\nJustification: The context indicates that [cite relevant phrase from context]."
        # ^^^^^^^^^^^^^^^^^                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        # è¿™äº›æ˜¯å ä½ç¬¦ï¼Œä¸æ˜¯çœŸå®å¼•ç”¨ï¼

# HaluEvalï¼ˆé”™è¯¯ï¼‰ï¼š
target = "Evidence: \"[From the provided knowledge]\""
                    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ å ä½ç¬¦
```

**å½±å“**:
- **Fairness 4ä¸ªå€™é€‰å®Œå…¨ç›¸åŒ** â†’ std=0.000ï¼ˆæ¨¡å‹å­¦ä¼šäº†å›ºå®šæ¨¡æ¿ï¼‰
- **Hallucinationè¾“å‡ºå ä½ç¬¦åä¹±ç ** â†’ "[From the provided knowledge]"uang7Je47CDF...
- **æ¨¡å‹ä»æœªå­¦ä¼šçœŸæ­£å¼•ç”¨context**

**ä¿®å¤**:
```python
# æå–çœŸå®çš„context/knowledgeç‰‡æ®µ
context_snippet = context[:50] + "..."
target = f"Answer: {correct_answer}\nJustification: Based on the context: \"{context_snippet}\", the answer is {answer_text}."
```

---

### Bug #1: pad_tokenè®¾ç½®é”™è¯¯
**é—®é¢˜**: LLaMA-3çš„`eos_token`é»˜è®¤æ˜¯`<|eot_id|>` (128009)ï¼Œå¯¼è‡´ï¼š
```python
# é”™è¯¯ä»£ç ï¼ˆå·²ä¿®å¤ï¼‰ï¼š
tokenizer.pad_token = tokenizer.eos_token  # pad_token_id = 128009 = eot_token_id
```

**å½±å“**:
- **æ‰€æœ‰paddingè¢«å½“æˆ"å¯¹è¯è½®æ¬¡ç»“æŸ"**
- çŸ­promptæœ‰50+ä¸ªpadding â†’ 50+ä¸ªeotä¿¡å·
- è§£é‡Šloggingä¸­çš„`<|eot_id|>`çˆ†é‡ï¼ˆ36/79/148ä¸ªï¼‰
- æ¨¡å‹è®­ç»ƒå’Œç”Ÿæˆå®Œå…¨é”™ä¹±

**ä¿®å¤**:
```python
# æ­£ç¡®ï¼šæ˜ç¡®ä½¿ç”¨<|end_of_text|>ä½œä¸ºpadding
tokenizer.pad_token = '<|end_of_text|>'  # id=128001
# <|eot_id|> (128009) ä¿ç•™ç»™å¯¹è¯è½®æ¬¡ç»“æŸ
```

**éªŒè¯**ï¼ˆè®­ç»ƒå¯åŠ¨æ—¶ä¼šè‡ªåŠ¨æ‰“å°ï¼‰:
```
pad_token: '<|end_of_text|>' (id=128001)  â† å¿…é¡»æ˜¯128001
eos_token: '<|eot_id|>' (id=128009)        â† 128009
âœ… éªŒè¯é€šè¿‡: pad_token_id (128001) â‰  eot_token_id (128009)
```

---

## å·²ä¿®å¤çš„Critical Bugï¼ˆå¦å¤–6ä¸ªï¼‰

### Bug #2: SFTä¸RLä½¿ç”¨ä¸åŒçš„æ¨¡æ¿æ ¼å¼
- **ä½ç½®**: `tokenize_sft_pair()` line 2165
- **é—®é¢˜**: SFTç”¨`"\n\n"`ç®€å•æ‹¼æ¥ï¼ŒRLç”¨å®Œæ•´chat template
- **å½±å“**: æ¨¡å‹ä»æœªå­¦è¿‡chat templateæ ¼å¼
- **ä¿®å¤**: SFTæ”¹ä¸ºä½¿ç”¨ç›¸åŒchat template

### Bug #3: Penaltyåº”ç”¨äºæ•´ä¸ªåºåˆ—ï¼ˆå«promptï¼‰
- **ä½ç½®**: `PresencePenaltyProcessor`, `FrequencyPenaltyProcessor` line 1713-1749
- **é—®é¢˜**: Promptä¸­çš„"the"å‡ºç°20æ¬¡â†’-8.0æƒ©ç½šï¼›EOSä»æœªåœ¨promptâ†’0æƒ©ç½šâ†’ç›¸å¯¹æå‡3000x
- **å½±å“**: å¯¼è‡´1-tokenç”Ÿæˆ
- **ä¿®å¤**: åªå¯¹responseéƒ¨åˆ†åº”ç”¨penalty

### Bug #4: KL/lossä½¿ç”¨é”™è¯¯çš„promptæ ¼å¼
- **ä½ç½®**: `generate_candidates_batch()` line 1967, training loop line 2478
- **é—®é¢˜**: Generateç”¨formatted_promptï¼ˆå¸¦templateï¼‰ï¼ŒKL/lossç”¨original_promptï¼ˆæ— templateï¼‰
- **å½±å“**: æ¢¯åº¦è®¡ç®—åœ¨å®Œå…¨ä¸åŒçš„tokenåºåˆ—ä¸Š
- **ä¿®å¤**: è¿”å›å¹¶ä½¿ç”¨formatted_prompts

### Bug #5: LEFT paddingä¸‹responseæå–é”™è¯¯
- **ä½ç½®**: `generate_candidates_batch()` line 1863-1879
- **é—®é¢˜**:
```python
# é”™è¯¯ï¼šç”¨"è®¡æ•°"å½“"ä½ç½®"
src_lens = (inputs["input_ids"] != pad).sum(dim=1)  # 3ä¸ªépad token
response = out[i, src_lens[i]:]  # ä»ä½ç½®3å¼€å§‹ â†’ åŒ…å«prompt!
```
- **å½±å“**: Judgeçœ‹åˆ°çš„responseåŒ…å«promptå†…å®¹ï¼Œæ‰€æœ‰rewardæ±¡æŸ“
- **ä¿®å¤**:
```python
original_input_len = inputs["input_ids"].shape[1]  # ç»å¯¹é•¿åº¦
response = out[i, original_input_len:]  # æ­£ç¡®è¾¹ç•Œ
```

### Bug #6: LEFT paddingä¸‹comp_maskè®¡ç®—é”™è¯¯
- **ä½ç½®**: `_tokenize_concat()` line 2230-2252
- **é—®é¢˜**:
```python
# é”™è¯¯ï¼šå‡è®¾RIGHT padding
resp_start = valid_len - resp_len  # æ ‡è®°äº†promptè€Œéresponse
```
- **å½±å“**: **Losså’ŒKLåœ¨padding/promptä½ç½®è®¡ç®—ï¼Œæ‰€æœ‰æ¢¯åº¦é”™è¯¯**
- **ä¿®å¤**:
```python
resp_start = T - resp_len  # LEFT paddingä¸‹responseåœ¨åºåˆ—æœ«å°¾
comp_end = T - 1
```

---

## ä¸ºä»€ä¹ˆå¿…é¡»é‡è®­

| Bug | å½±å“èŒƒå›´ | éœ€è¦é‡è®­ |
|-----|----------|----------|
| #0 | **SFTæ•°æ®è´¨é‡ï¼ˆæœ€æ ¹æœ¬ï¼‰** | **SFT + GRPO** |
| #1 | **æ‰€æœ‰padding=eotä¿¡å·** | **SFT + GRPO** |
| #2 | SFTå­¦é”™äº†æ ¼å¼ | SFT + GRPO |
| #3 | æ‰€æœ‰rewardä¿¡å· | GRPO |
| #4 | æ‰€æœ‰KL/lossæ¢¯åº¦ | GRPO |
| #5 | æ‰€æœ‰rewardä¿¡å· | GRPO |
| #6 | æ‰€æœ‰lossæ¢¯åº¦ | GRPO |

**ä¹‹å‰çš„checkpointå…¨éƒ¨ä½œåºŸã€‚**

**Bug #0æ˜¯æ ¹æœ¬åŸå› ** - è§£é‡Šäº†loggingä¸­æ‰€æœ‰å¼‚å¸¸ç°è±¡ï¼š
- Fairness 4ä¸ªå€™é€‰å®Œå…¨ç›¸åŒ â†’ std=0.000
- Hallucinationè¾“å‡ºå ä½ç¬¦+ä¹±ç ï¼š"[From the provided knowledge]"uang7Je47CDF...
- æ¨¡å‹ä»æœªå­¦ä¼šçœŸæ­£å¼•ç”¨context
- ç†µå´©å¡Œï¼ˆåªå­¦ä¼šäº†å›ºå®šæ¨¡æ¿ï¼‰

**Bug #1è§£é‡Šå…¶ä»–å¼‚å¸¸**ï¼š
- `<|eot_id|>`çˆ†é‡ï¼ˆ36/79/148ï¼‰- paddingæ˜¾ç¤ºä¸ºeot
- æçŸ­ç”Ÿæˆï¼ˆpaddingè¢«å½“æˆç»“æŸä¿¡å·ï¼‰

---

## é‡è®­å‰å¿…é¡»éªŒè¯ï¼ˆP0ï¼‰

æ·»åŠ å•æµ‹åˆ°`test_padding.py`ï¼š
```python
# 1. æµ‹è¯•responseæå–
def test_response_extraction():
    # Mock left-padded generation
    inputs = tokenizer(["short", "longer prompt"], padding=True, return_tensors="pt")
    original_input_len = inputs["input_ids"].shape[1]

    # Simulate generation
    fake_response = torch.tensor([[100, 200], [100, 200]])
    out = torch.cat([inputs["input_ids"], fake_response], dim=1)

    # Extract
    response = out[:, original_input_len:]

    # Verify
    assert response.shape[1] == 2  # Only generated tokens
    assert not any(response == tokenizer.pad_token_id)  # No padding

# 2. æµ‹è¯•comp_mask
def test_comp_mask():
    prompts = ["Hello"]
    responses = ["World"]
    lengths = [1]  # "World" = 1 token

    full, comp_mask = _tokenize_concat(tokenizer, prompts, responses, lengths, device)

    T = full["input_ids"].shape[1]
    # Responseåœ¨æœ€å1ä¸ªä½ç½®ï¼Œlogitsæœ€åé¢„æµ‹ä½ç½®æ˜¯T-2
    assert comp_mask[0, T-2] == 1.0  # åº”è¯¥æ ‡è®°
    assert comp_mask[0, :T-2].sum() >= 0  # å‰é¢å¯èƒ½æ˜¯prompt
```

**è¿è¡Œç¡®è®¤é€šè¿‡åå†å¯åŠ¨è®­ç»ƒã€‚**

---

## é¦–æ¬¡è®­ç»ƒç›‘æ§ï¼ˆå‰2 stepsï¼‰

### å¯åŠ¨æ—¶è‡ªåŠ¨éªŒè¯

è®­ç»ƒå¼€å§‹æ—¶ä¼šè‡ªåŠ¨æ‰“å°å¹¶éªŒè¯tokenizeré…ç½®ï¼š

```
================================================================================
Tokenizerç‰¹æ®ŠTokené…ç½®éªŒè¯
================================================================================
pad_token: '<|end_of_text|>' (id=128001)
eos_token: '<|end_of_text|>' (id=128001)
bos_token: '<|begin_of_text|>' (id=128000)
eot_token: '<|eot_id|>' (id=128009)
âœ… éªŒè¯é€šè¿‡: pad_token_id (128001) â‰  eot_token_id (128009)
padding_side: left
================================================================================
```

**å¦‚æœçœ‹åˆ°**:
- `âŒ ä¸¥é‡é”™è¯¯: pad_token_id == eot_token_id!` â†’ **è®­ç»ƒä¼šè‡ªåŠ¨åœæ­¢**
  - è¿™æ„å‘³ç€paddingè¢«å½“æˆå¯¹è¯ç»“æŸï¼Œé…ç½®é”™è¯¯
  - éœ€è¦ä¿®å¤tokenizeråˆå§‹åŒ–ä»£ç 

**æ­£å¸¸é…ç½®**ï¼ˆLLaMA-3-Instructï¼‰:
- pad_token_id = 128001 (ä¸eosç›¸åŒï¼Œæ ‡å‡†é…ç½®)
- eot_token_id = 128009 (ä¸åŒäºpadï¼Œå¿…é¡»)

---

### å…³é”®è¯Šæ–­æ—¥å¿—ï¼ˆå·²è‡ªåŠ¨æ‰“å°ï¼‰

#### âœ… è¾¹ç•Œæ­£ç¡®
```
æ ·æœ¬ 0:
  Responseéƒ¨åˆ†(å‰120å­—ç¬¦,å«special): [ä¸åº”è¯¥åŒ…å« <|start_header_id|> ç­‰]
  è¾¹ç•Œé™„è¿‘Token: [promptæœ«å°¾åº”è¯¥æ˜¯ assistant<|end_header_id|>\n\n]
```

#### âœ… é•¿åº¦åˆç†
```
ç”Ÿæˆé•¿åº¦: Fairness avg=XX, Hallucination avg=YY
```
é¢„æœŸï¼š15-96 tokensï¼ˆä¸åº”è¯¥æ˜¯1-2æˆ–200+ï¼‰

#### âœ… ç†µéé›¶
```
Fairnessæ ·æœ¬ 0: entropy=X.XXX
```
é¢„æœŸï¼š>0.5ï¼ˆå¦‚æœâ‰ˆ0è¯´æ˜ä»æœ‰é—®é¢˜ï¼‰

#### âœ… EOT tokenæ•°é‡æ­£å¸¸
```
Tokenç»Ÿè®¡:
  Prompt: Xä¸ª<|eot_id|> tokens, Yä¸ªpadding
  Response: Zä¸ª<|eot_id|> tokens
```
é¢„æœŸï¼š
- Prompt: 2ä¸ªï¼ˆsystemç»“æŸ + userç»“æŸï¼‰+ paddingï¼ˆå¯èƒ½å¾ˆå¤šï¼Œå› ä¸ºpad_token_id=eos_token_idï¼‰
- Response: â‰¤1ä¸ªï¼ˆå›ç­”ç»“æŸï¼‰

å¦‚æœçœ‹åˆ°ï¼š
- `pad_token_id == eot_token_id` â†’ **æ­£å¸¸**ï¼ˆLLaMA-3æ ‡å‡†é…ç½®ï¼‰
- Promptæœ‰50+ä¸ªeot + 50+ä¸ªpadding â†’ **æ­£å¸¸**ï¼ˆçŸ­promptçš„LEFT paddingæ˜¾ç¤ºï¼‰
- Responseæœ‰10+ä¸ªeot â†’ **å¼‚å¸¸**ï¼ˆæ¨¡å‹åœ¨å›ç­”ä¸­åå¤ç”Ÿæˆeotï¼‰

### âš ï¸ å¼‚å¸¸ç«‹å³åœæ­¢è®­ç»ƒ

å¦‚æœçœ‹åˆ°ï¼š
- `âš ï¸ å¼‚å¸¸: Responseå¼€å¤´ä¼¼ä¹åŒ…å«chat header`
- `âš ï¸ å¼‚å¸¸: FullåŒ…å«36ä¸ª<|eot_id|>`
- `ç†µ=0.000`ä¸”å¤§é¢ç§¯å‡ºç°

â†’ **åœæ­¢ï¼Œæ£€æŸ¥ä»£ç **

---

## é‡‡æ ·å‚æ•°ï¼ˆå·²è°ƒæ•´ï¼Œè§‚å¯Ÿæ•ˆæœï¼‰

```python
MIN_NEW_TOKENS_TRAIN = 15      # å»¶è¿ŸEOSé‡Šæ”¾
TEMPERATURE_TRAIN = 1.2        # å¯¹æŠ—æå°–åˆ†å¸ƒ
TOP_P_TRAIN = 0.95             # æ”¾å®½æ¢ç´¢
PRESENCE_PENALTY = 0.3         # é™ä½ï¼ˆé…åˆscopeä¿®å¤ï¼‰
FREQUENCY_PENALTY = 0.2        # é™ä½ï¼ˆé…åˆscopeä¿®å¤ï¼‰
```

å¦‚æœä»ç„¶ç†µå´©å¡Œï¼Œè€ƒè™‘ï¼š
- æé«˜temperatureåˆ°1.5
- é™ä½entropy_coefï¼ˆå½“å‰0.2å¯èƒ½è¿‡é«˜ï¼‰

---

## Commitè®°å½•

```bash
git log --oneline HEAD~10..HEAD
91ee919 CRITICAL: fix pad_token must be <|end_of_text|> not <|eot_id|>
fa435d2 docs: update HANDOFF with tokenizer validation info
32a8753 feat: add tokenizer config validation at startup
5459abf docs: update HANDOFF to explain improved diagnostics
40b85f6 fix: improve boundary diagnostics to distinguish padding from actual eot tokens
9172089 docs: add handoff documentation for critical bug fixes
a07af9a CRITICAL: fix comp_mask calculation for left padding
fc99b9f CRITICAL: fix left padding response extraction boundary error
86a5902 CRITICAL: fix _tokenize_concat using wrong prompt format for KL/loss
6810389 CRITICAL: fix SFTâ†’RL template inconsistency causing boundary corruption
```

**æœ€å…³é”®çš„commit**: `91ee919` - ä¿®å¤pad_tokené…ç½®é”™è¯¯ï¼Œè§£é‡Šæ‰€æœ‰å¼‚å¸¸ç°è±¡

---

## æ–‡ä»¶ä¿®æ”¹æ±‡æ€»

### `src/grpo/trainer.py`

#### Line 2144-2163: Tokenizer Initialization (CRITICAL)
- ä¿®å¤pad_tokenè®¾ç½®ï¼šå¿…é¡»ç”¨`<|end_of_text|>` (128001)
- ä¸èƒ½ç”¨`<|eot_id|>` (128009)ä½œä¸ºpadding
- æ·»åŠ è‡ªåŠ¨éªŒè¯ï¼Œå¦‚æœé…ç½®é”™è¯¯ä¼šæŠ›å‡ºå¼‚å¸¸

#### Line 1713-1749: Penalty Processors
- æ·»åŠ `prompt_len`è·Ÿè¸ª
- åªå¯¹responseåº”ç”¨penalty

#### Line 1811-1967: generate_candidates_batch
- è¿”å›`formatted_prompts`
- ä½¿ç”¨`original_input_len`æå–response
- è¯¦ç»†è¾¹ç•Œè¯Šæ–­æ—¥å¿—ï¼ˆstep<2æ—¶ï¼‰

#### Line 2165-2207: tokenize_sft_pair
- ä½¿ç”¨`apply_chat_template`ï¼ˆä¸GRPOä¸€è‡´ï¼‰

#### Line 2230-2254: _tokenize_concat
- ä¿®å¤LEFT paddingä¸‹çš„`comp_mask`è®¡ç®—
- `resp_start = T - resp_len`

#### Line 2473-2491: Training Loop
- ä½¿ç”¨`formatted_prompts`è€ŒéåŸå§‹prompts

---

## å·²çŸ¥æ­£ç¡®çš„é…ç½®

- âœ… Tokenizer: `padding_side = "left"`
- âœ… EOS tokens: `[eos_token_id, eot_token_id]` (128001, 128009)
- âœ… Chat template: LLaMA-3-Instructæ ¼å¼
- âœ… Attention mask: è‡ªåŠ¨ç”Ÿæˆï¼ˆæ­£ç¡®ï¼‰
- âœ… KL formula: DeepSeekMath Eq.4å‰å‘KL
- âœ… Reference model: å†»ç»“ï¼Œä¸policyç›¸åŒdtype/device

---

## å¯åŠ¨è®­ç»ƒå‘½ä»¤

```bash
cd /home/user/grpo-dual
python src/grpo/trainer.py

# é¦–æ¬¡è®­ç»ƒå¯†åˆ‡å…³æ³¨step 0-1çš„è¾¹ç•Œè¯Šæ–­è¾“å‡º
# ç¡®è®¤æ— å¼‚å¸¸åå¯ç»§ç»­
```

---

## é—®é¢˜æ’æŸ¥

### å¦‚æœä»ç„¶çœ‹åˆ°1-tokenç”Ÿæˆ
1. æ£€æŸ¥EOSSuppressionProcessoræ—¥å¿—ï¼šEOSæ˜¯å¦çœŸçš„è¢«è®¾ç½®ä¸º-inf
2. æ£€æŸ¥è¾¹ç•Œè¯Šæ–­ï¼šresponseæ˜¯å¦åŒ…å«prompt

### å¦‚æœä»ç„¶ç†µå´©å¡Œ
1. å¯èƒ½æ˜¯rewardè®¾è®¡é—®é¢˜ï¼ˆåå¥½çŸ­ç­”æ¡ˆï¼‰
2. å¯èƒ½æ˜¯SFTæ•°æ®æœ¬èº«å°±çŸ­
3. å°è¯•æé«˜temperatureæˆ–é™ä½KL penalty (Î²)

### å¦‚æœrewardä¿¡å·å…¨æ˜¯0
1. æ£€æŸ¥judgeæ˜¯å¦æ­£å¸¸å·¥ä½œ
2. æ£€æŸ¥batchä¸­æ˜¯å¦åŒ…å«ä¸¤ä¸ªä»»åŠ¡ï¼ˆFairnesså’ŒHallucinationï¼‰

---

**è”ç³»äºº**: åŸå¼€å‘è€…å·²ç¦»å¼€ï¼Œæœ‰é—®é¢˜æŸ¥çœ‹`/tmp/CRITICAL_AUDIT_FINAL_REPORT.md`è¯¦ç»†åˆ†æ
