# GRPO Multi-Objective Training - Handoff Document

**Last Updated:** 2025-11-08
**Current Branch:** `claude/open-trainer-py-011CUp9RqkPbRBQPMVzBRuJ3`
**Status:** Plan Cå…¨é¢ä¿®å¤å·²å®Œæˆï¼ˆAdvantageè®¡ç®—+ç†µå¥–åŠ±+KLçº¦æŸ+æ¨¡æ¿æ£€æµ‹å¢å¼ºï¼‰

---

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### ç›®æ ‡
ä½¿ç”¨ GRPO (Group Relative Policy Optimization) å¯¹ Llama-3-8B-Instruct è¿›è¡Œå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼š
- **Fairness (BBQæ•°æ®é›†)**: å‡å°‘åè§ï¼Œå…¬å¹³å›ç­”é—®é¢˜
- **Hallucination (HaluEvalæ•°æ®é›†)**: å‡å°‘å¹»è§‰ï¼ŒåŸºäºè¯æ®å›ç­”

### æŠ€æœ¯æ ˆ
- Base Model: `meta-llama/Meta-Llama-3-8B-Instruct`
- Method: GRPO + LoRA + Branched KL Control
- Framework: PyTorch + Transformers + PEFT

---

## ğŸ”¥ å…³é”®é—®é¢˜å†å²

### é—®é¢˜1ï¼šç†µå¡Œé™·ï¼ˆEntropy Collapseï¼‰
**ç—‡çŠ¶ï¼š**
- æ¨¡å‹è¾“å‡ºé«˜åº¦ç¡®å®šæ€§ï¼ˆmax prob â‰ˆ 0.99999ï¼‰
- ç†µå€¼æä½ï¼ˆ0.2-0.7ï¼Œæ­£å¸¸åº” >1.5ï¼‰
- æ‰€æœ‰ç”Ÿæˆéƒ½æ˜¯ç›¸åŒæ¨¡æ¿ï¼š"The context does not provide sufficient information..."
- åŒä¸€promptçš„Kä¸ªå€™é€‰å‡ ä¹ç›¸åŒ â†’ advantage=0 â†’ æ— æ¢¯åº¦ä¿¡å·

**æ ¹æœ¬åŸå› ï¼š**
1. `MIN_NEW_TOKENS_TRAIN=30` å¼ºåˆ¶æ‰€æœ‰å›ç­”â‰¥30 tokens
2. Judgeå¯¹"å®‰å…¨åºŸè¯æ¨¡æ¿"ç»™æ­£åˆ†
3. å¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°å•ä¸€æ¨¡æ¿è¾“å‡º

**ä¿®å¤æ–¹æ¡ˆï¼ˆA+Bï¼Œå·²å®Œæˆï¼‰ï¼š**
- âœ… A: `MIN_NEW_TOKENS_TRAIN: 30 â†’ 5` (trainer.py:226)
- âœ… B: æ¨¡æ¿æ£€æµ‹å™¨ï¼Œæƒ©ç½šé€ƒé¿å›ç­” (trainer.py:1594-1621)

**ä»£ç ä½ç½®ï¼š**
```
grpo-dual/src/grpo/trainer.py
  - Line 226: MIN_NEW_TOKENS_TRAIN = 5
  - Line 1586-1621: MultiCloudJudge.evaluate() ä¸­çš„æ¨¡æ¿æ£€æµ‹å™¨
```

---

### é—®é¢˜2ï¼šæ•°æ®é›†ä½¿ç”¨é—®é¢˜ï¼ˆPiä¸“å®¶åˆ†æï¼‰

#### BBQæ•°æ®é›†ï¼ˆå·²æ­£ç¡®å¤„ç† âœ…ï¼‰
**å…³é”®ç‚¹ï¼š**
- Unknowné€‰é¡¹ä½ç½®æ˜¯åŠ¨æ€çš„ï¼ˆä¸å›ºå®šåœ¨A/B/CæŸä¸ªä½ç½®ï¼‰
- å¿…é¡»é€šè¿‡ `answer_info[*][1]=="unknown"` åˆ¤å®š
- Ambiguousæ ·æœ¬ï¼šå¿…é¡»é€‰Unknown
- Disambiguatedæ ·æœ¬ï¼šæœ‰æ˜ç¡®æ­£ç¡®ç­”æ¡ˆ

**ä»£ç éªŒè¯ï¼š**
```python
# Line 1126-1133: _find_unknown_option æ­£ç¡®ä½¿ç”¨answer_info
if val[1]=="unknown":
    return chr(65+idx)  # åŠ¨æ€ç¡®å®šA/B/C
```
âœ… å·²æ­£ç¡®å®ç°

#### HaluEvalæ•°æ®é›†ï¼ˆéƒ¨åˆ†é—®é¢˜ âš ï¸ï¼‰

**é—®é¢˜2.1: Generalå­é›†å™ªå£°ä¸¥é‡**
- "å¹»è§‰"æ¦‚å¿µæ··ç”¨ï¼šäº‹å®é”™è¯¯ã€ä¸å®Œæ•´å›ç­”ã€èƒ½åŠ›å£°æ˜ã€æ ¼å¼é—®é¢˜å…¨æ··åœ¨ä¸€èµ·
- 815ä¸ªyesæ ‡æ³¨ä¸­ï¼Œçº¦13ä¸ªhallucination_spansä¸ºç©º
- çº¦200+ä¸ªæ¶‰åŠ"As an AI language model..."è¢«æ ‡ä¸ºå¹»è§‰
- **å½±å“ï¼š** rewardä¿¡å·äº’ç›¸çŸ›ç›¾ï¼Œæ¨¡å‹å€¾å‘ä¿å®ˆæ¨¡æ¿

**é—®é¢˜2.2: é…å¯¹æ ·æœ¬æœªå……åˆ†åˆ©ç”¨**
- qa/dialogueå­é›†æœ‰ `right_answer` å’Œ `hallucinated_answer`
- å½“å‰åªç”¨äº† `right_answer` åšSFT/target
- æœªåšå¯¹æ¯”å­¦ä¹ ï¼ˆpositive vs negativeï¼‰
- **å½±å“ï¼š** æ¨¡å‹åªçŸ¥é“"æ­£ç¡®"ï¼Œä¸çŸ¥é“"å¹»è§‰"é•¿ä»€ä¹ˆæ ·

**ä»£ç ä½ç½®ï¼š**
```
grpo-dual/src/grpo/trainer.py
  - Line 1163-1276: HaluEvalAdapter.load_samples()
  - Line 1220: åªç”¨right_answerï¼ˆå¾…æ”¹è¿›ï¼‰
  - Line 1234: åªç”¨right_responseï¼ˆå¾…æ”¹è¿›ï¼‰
  - Line 1255-1274: generalå­é›†å¤„ç†ï¼ˆå¯èƒ½éœ€è¦è¿‡æ»¤ï¼‰
```

---

### é—®é¢˜3ï¼šAdvantageè®¡ç®—æŠ¹å¹³æ¢¯åº¦ä¿¡å·ï¼ˆPiä¸“å®¶å‘ç°ï¼Œæœ€è‡´å‘½ï¼ï¼‰

**ç—‡çŠ¶ï¼š**
- è®­ç»ƒæ—¥å¿—æ˜¾ç¤ºï¼š`Reward (å½’ä¸€åŒ–å): 0.000`, `ä¿¡å·å¼ºåº¦: 0.0000`
- å¤§é‡æ­¥éª¤çš„Fairnesså’ŒHallucinationä¿¡å·å¼ºåº¦éƒ½<1e-5
- å³ä½¿rewardæœ‰å·®å¼‚ï¼Œadvantageä»ä¸º0
- è®­ç»ƒå®é™…ä¸Š"åŸåœ°è¸æ­¥"

**æ ¹æœ¬åŸå› ï¼š**
Pié€šè¿‡åˆ†æè®­ç»ƒæ—¥å¿—å‘ç°ï¼š`compute_group_advantages` ä½¿ç”¨**ç»„å†…æ ‡å‡†åŒ–**ï¼š

```python
# Line 2569-2577: compute_group_advantages
r = rewards.view(B, k)  # [batch_size, Kä¸ªå€™é€‰]
mean = r.mean(dim=1, keepdim=True)  # ç»„å†…å‡å€¼
std = r.std(dim=1, keepdim=True).clamp_min(1e-6)  # ç»„å†…æ ‡å‡†å·®
adv = ((r - mean) / std).view(-1)  # ç»„å†…z-scoreæ ‡å‡†åŒ–
```

**é—®é¢˜æœºåˆ¶ï¼š**
1. å½“åŒä¸€promptçš„K=4ä¸ªå€™é€‰éƒ½è¾“å‡ºç›¸åŒæ¨¡æ¿ï¼ˆç†µå¡Œé™·ï¼‰
2. å®ƒä»¬çš„rewardå®Œå…¨ç›¸åŒï¼ˆå¦‚éƒ½æ˜¯-0.7ï¼‰
3. `std = 0` (clampåˆ°1e-6)
4. `adv = (r - mean) / 1e-6 â‰ˆ 0`
5. **è¿™ä¸€ç»„çš„4ä¸ªæ ·æœ¬æ¢¯åº¦å…¨éƒ¨ä¸º0ï¼**
6. å¦‚æœ50%ä»¥ä¸Šçš„ç»„éƒ½è¿™æ · â†’ æ•´ä¸ªbatchå‡ ä¹æ²¡æœ‰å­¦ä¹ ä¿¡å·

**ä¸é—®é¢˜1çš„å…³ç³»ï¼š**
- é—®é¢˜1ï¼ˆç†µå¡Œé™·ï¼‰å¯¼è‡´Kä¸ªå€™é€‰ç›¸åŒ
- é—®é¢˜3ï¼ˆç»„å†…æ ‡å‡†åŒ–ï¼‰å°†"ç›¸åŒ"è½¬åŒ–ä¸º"æ¢¯åº¦ä¸º0"
- å½¢æˆæ¶æ€§å¾ªç¯ï¼šç†µå¡Œé™· â†’ æ— æ¢¯åº¦ â†’ ç­–ç•¥ä¸åŠ¨ â†’ ç»§ç»­å¡Œé™·

**ä¿®å¤æ–¹æ¡ˆï¼ˆC2ï¼Œå·²å®Œæˆï¼‰ï¼š**
- âœ… C2: ç»„å†…stdç›‘æ§å’Œè­¦å‘Š (trainer.py:2933-2965)
- ğŸ”„ é¢„æœŸA+Bä¿®å¤èƒ½è®©å¤§éƒ¨åˆ†ç»„äº§ç”Ÿå·®å¼‚ï¼ˆstd>0.01ï¼‰
- âš ï¸ å¦‚æœ>50%ç»„ä»ç„¶std<0.01ï¼Œéœ€è¦Plan C1ï¼ˆå…¨å±€baselineé‡æ„ï¼‰

**ä»£ç ä½ç½®ï¼š**
```
grpo-dual/src/grpo/trainer.py
  - Line 2569-2577: compute_group_advantages (é—®é¢˜æ ¹æº)
  - Line 2933-2965: C2ç›‘æ§é€»è¾‘ï¼ˆå·²æ·»åŠ ï¼‰
```

**Piçš„å…¶ä»–å‘ç°ï¼š**
1. **KLæ§åˆ¶è¿‡ä¸¥ï¼š** ç›®æ ‡KL=0.035è¿‡å°ï¼Œå®é™…KL=2.xï¼ŒÎ²å´ç»§ç»­å¢å¤§ â†’ é”æ­»ç­–ç•¥
2. **Judgeé€»è¾‘å¯ç–‘ï¼š** å¯¹æ¨¡æ¿çŸ­è¯­çš„å¥–åŠ±ä¸ä¸€è‡´
3. **ç¼ºå°‘ç†µæ­£åˆ™åŒ–ï¼š** åº”è¯¥ç»™policyåŠ entropy bonus

---

## âœ… å·²å®Œæˆçš„ä¿®å¤

### Commit: f140a1c "Fix entropy collapse: reduce MIN_NEW_TOKENS and penalize template responses"

**ä¿®æ”¹1: æ”¾æ¾è§£ç ç­–ç•¥**
```python
# trainer.py:226
MIN_NEW_TOKENS_TRAIN = 5  # ä»30é™åˆ°5
```
**æ•ˆæœï¼š** å…è®¸çŸ­å›ç­”ï¼ŒåŒä¸€promptçš„Kä¸ªå€™é€‰äº§ç”Ÿå·®å¼‚ â†’ æ¢å¤æ¢¯åº¦ä¿¡å·

**ä¿®æ”¹2: æ¨¡æ¿æ£€æµ‹å™¨**
```python
# trainer.py:1594-1621
# æ£€æµ‹6ç§æ¨¡æ¿çŸ­è¯­ï¼š
template_phrases = [
    "does not provide sufficient information",
    "cannot be determined",
    "not enough information",
    "insufficient information",
    "unable to determine",
    "context does not"
]

# åˆ†å±‚æƒ©ç½šï¼š
- BBQ disambiguated: -0.7ï¼ˆæœ‰æ˜ç¡®ç­”æ¡ˆå´é€ƒé¿ï¼‰
- HaluEval qa/dialogue/summarization: -0.5ï¼ˆæœ‰knowledgeå´é€ƒé¿ï¼‰
- Ambiguous/general: 0.0ï¼ˆå‹‰å¼ºå¯ä»¥ï¼Œä½†ä¸ç»™æ­£åˆ†ï¼‰
```
**æ•ˆæœï¼š** æ¨¡æ¿ä¸å†æ˜¯"å®‰å…¨æœ€ä¼˜ç­–ç•¥"

---

### Commit: (å¾…æäº¤) "Add C2 fix: monitor and warn about zero-gradient groups"

**ä¿®æ”¹3: C2ç»„å†…stdç›‘æ§**
```python
# trainer.py:2933-2965
# åœ¨compute_group_advantagesä¹‹åæ·»åŠ ç›‘æ§é€»è¾‘

# æ£€æµ‹æ¯ç»„çš„reward std
for i in range(B):
    group_rewards = rewards_list[i*K : (i+1)*K]
    group_std = np.std(group_rewards)

    if group_std < 0.01:  # ç»„å†…å‡ ä¹ç›¸åŒ
        zero_gradient_groups += 1
        # å‰20æ­¥è¯¦ç»†æ‰“å°è¯¥ç»„çš„rewardså’Œresponses

# ç»Ÿè®¡å¹¶è­¦å‘Š
if zero_gradient_groups > 0:
    ratio = zero_gradient_groups / B
    print(f"âš ï¸ {zero_gradient_groups}/{B} ç»„({ratio:.1%})çš„reward std<0.01ï¼Œæ¢¯åº¦ä¿¡å·è¢«æŠ¹å¹³")

    if ratio > 0.5:
        print("âš ï¸âš ï¸âš ï¸ è¶…è¿‡50%çš„ç»„æ— æ¢¯åº¦ï¼A+Bä¿®å¤å¯èƒ½æœªç”Ÿæ•ˆ")
```

**æ•ˆæœï¼š**
1. **å®æ—¶ç›‘æ§**ï¼šç«‹å³å‘ç°æœ‰å¤šå°‘ç»„çš„æ¢¯åº¦è¢«æŠ¹å¹³
2. **æ—©æœŸé¢„è­¦**ï¼šå¦‚æœ>50%æ— æ¢¯åº¦ï¼Œè¯´æ˜A+Bæœªç”Ÿæ•ˆï¼Œéœ€è¦Plan C1
3. **è¯¦ç»†è¯Šæ–­**ï¼šå‰20æ­¥æ‰“å°æ¯ä¸ªæ— æ¢¯åº¦ç»„çš„rewardså’Œresponsesï¼Œæ–¹ä¾¿å®šä½é—®é¢˜
4. **ä¸å½±å“è®­ç»ƒ**ï¼šåªæ˜¯ç›‘æ§å’Œè­¦å‘Šï¼Œä¸ä¿®æ”¹æ¢¯åº¦è®¡ç®—ï¼ˆA+Båº”è¯¥èƒ½è®©å¤§éƒ¨åˆ†ç»„äº§ç”Ÿå·®å¼‚ï¼‰

---

### Commit: (å¾…æäº¤) "Implement Plan C: fix advantage calculation and enhance exploration"

**åŸºäºPiä¸“å®¶çš„è®­ç»ƒæ—¥å¿—è¯Šæ–­ï¼Œå®æ–½å…¨é¢ä¿®å¤ï¼š**

#### ä¿®æ”¹4: Advantageè®¡ç®—ä¿®å¤ï¼ˆæœ€æ ¸å¿ƒï¼ï¼‰
```python
# trainer.py:2569-2608
# åŸé€»è¾‘ï¼šç»„å†…z-scoreæ ‡å‡†åŒ–
adv = (r - mean) / std  # std=0æ—¶æ¢¯åº¦ä¸º0

# æ–°é€»è¾‘ï¼šæ£€æµ‹stdï¼Œé€€åŒ–åˆ°å®‰å…¨æ¨¡å¼
if group_std < 0.01:
    # æ•´ç»„åŒå¥–ï¼Œç›´æ¥ç”¨rewardï¼ˆå·²è¿‡å…¨å±€å½’ä¸€åŒ–ï¼‰
    group_adv = group_rewards
else:
    # æœ‰å¤šæ ·æ€§ï¼Œç”¨ä¸­å¿ƒåŒ–ï¼ˆä¸é™¤stdï¼Œä¿ç•™scaleï¼‰
    group_adv = group_rewards - group_mean
```

**æ•ˆæœï¼š**
- âœ… é¿å…é™¤ä»¥0å¯¼è‡´çš„æ¢¯åº¦æŠ¹å¹³ï¼ˆå³ä½¿Kä¸ªå€™é€‰å®Œå…¨ç›¸åŒï¼‰
- âœ… ä¿ç•™GRPOç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ¦‚å¿µï¼ˆæœ‰å¤šæ ·æ€§æ—¶ï¼‰
- âœ… rewardå·²è¿‡å…¨å±€å½’ä¸€åŒ–ï¼Œå¯ç›´æ¥å½“advantageä½¿ç”¨
- âœ… é€€åŒ–æ¨¡å¼ï¼šæ— å¤šæ ·æ€§æ—¶ï¼Œè‡³å°‘æœ‰ä¸€è‡´çš„æ¢¯åº¦æ–¹å‘ï¼ˆé¼“åŠ±/æŠ‘åˆ¶ï¼‰

#### ä¿®æ”¹5: å¢å¼ºç†µæ­£åˆ™åŒ–
```python
# trainer.py:203
ENTROPY_COEF = 2.0  # ä»0.5â†’2.0
```

**åŸå› ï¼š** ç­–ç•¥æåº¦å°–é”(top-1 prob 0.94~0.999999)ï¼Œéœ€è¦æ›´å¼ºçš„ç†µå¥–åŠ±å¯¹æŠ—å¡Œé™·

**æ•ˆæœï¼š** Lossä¸­entropyé¡¹æƒé‡å¢å¤§4å€ï¼Œæ˜¾è‘—é¼“åŠ±æ¢ç´¢

#### ä¿®æ”¹6: é™ä½KLçº¦æŸ
```python
# trainer.py:2786-2788
beta_f_init = 0.05  # ä»0.30â†’0.05
beta_h_init = 0.05  # ä»0.30â†’0.05
```

**åŸå› ï¼š**
- ä¸¥æ ¼KLçº¦æŸ(Î²=0.30)é”ä½æ¨¡å‹ï¼Œå‡ ä¹ä¸æ›´æ–°
- å‚è€ƒDeepSeekMathä½¿ç”¨0.04
- ç»™æ¨¡å‹æ›´å¤šè‡ªç”±åº¦åç¦»å‚è€ƒæ¨¡å‹

**æ•ˆæœï¼š** KLæƒ©ç½šé™ä½6å€ï¼Œæ¨¡å‹å¯ä»¥æ›´å¤§èƒ†åœ°æ¢ç´¢

#### ä¿®æ”¹7: æ¨¡æ¿æ£€æµ‹å™¨å¢å¼º
```python
# trainer.py:1595-1628
# æ‰©å±•çŸ­è¯­åˆ—è¡¨ï¼š6ç§â†’13ç§
template_phrases = [
    "does not provide sufficient information",
    "cannot be determined",
    # ... æ–°å¢7ç§
    "ambiguous", "unclear from the context", "not specified", ...
]

# åŠ å¤§æƒ©ç½šåŠ›åº¦ï¼š
- BBQ disambiguated: -0.7 â†’ -1.0ï¼ˆæœ€å¤§è´Ÿåˆ†ï¼‰
- HaluEval qa/dialogue/summarization: -0.5 â†’ -0.8
- Ambiguous/general: 0.0 â†’ -0.2ï¼ˆè½»å¾®è´Ÿåˆ†ï¼‰
```

**æ•ˆæœï¼š**
- æ›´å…¨é¢çš„æ¨¡æ¿è¯†åˆ«
- æ›´å¼ºçš„æƒ©ç½šä¿¡å·
- é…åˆAdvantageä¿®å¤ï¼Œå³ä½¿å…¨ç»„æ˜¯æ¨¡æ¿ä¹Ÿèƒ½äº§ç”Ÿæ¢¯åº¦

---

## ğŸ“Š å½“å‰è®­ç»ƒçŠ¶æ€

### é…ç½® (trainer.py:126-286)
```python
# åŸºç¡€
BASE_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"
GRPO_STEPS = 500
GRPO_BATCH_SIZE = 2
K_ROLLOUTS = 4  # æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªå€™é€‰
GRADIENT_ACCUMULATION_STEPS = 2

# ç”Ÿæˆé…ç½®ï¼ˆå·²ä¿®å¤ï¼‰
MAX_NEW_TOKENS_TRAIN = 128
MIN_NEW_TOKENS_TRAIN = 5  # âš¡ å·²ä¿®å¤
TEMPERATURE_TRAIN = 0.9
TOP_K_TRAIN = 100
TOP_P_TRAIN = 0.9
REP_PENALTY_TRAIN = 1.18

# æ•°æ®
N_BBQ_TRAIN = 1100
N_HALU_TRAIN = 400

# Reward Scaleï¼ˆå¤šç›®æ ‡å¹³è¡¡ï¼‰
FAIRNESS_REWARD_SCALE = 0.7
HALLUCINATION_REWARD_SCALE = 1.0

# ç†µæ­£åˆ™åŒ–ï¼ˆPlan Cå¢å¼ºï¼‰
ENTROPY_COEF = 2.0  # âš¡ ä»0.5æå‡åˆ°2.0ï¼Œå¯¹æŠ—ä¸¥é‡ç†µå¡Œé™·

# KLæ§åˆ¶ï¼ˆåˆ†æ”¯åŒ–ï¼ŒPlan Cé™ä½ï¼‰
beta_f_init = 0.05  # âš¡ ä»0.30é™åˆ°0.05ï¼Œç»™æ¨¡å‹æ›´å¤šè‡ªç”±åº¦
beta_h_init = 0.05  # âš¡ ä»0.30é™åˆ°0.05
```

### å¾…è§‚å¯ŸæŒ‡æ ‡ï¼ˆå‰50æ­¥æœ€å…³é”®ï¼‰

#### ğŸ”¥ğŸ”¥ğŸ”¥ ä¼˜å…ˆçº§0ï¼šè®­ç»ƒæ˜¯å¦çœŸæ­£å¼€å§‹å­¦ä¹ ï¼ˆè§‚å¯Ÿå‰10æ­¥ï¼‰
```
âš ï¸ [Step X] Y/B ç»„(Z%)çš„reward std<0.01
```
**Plan Cå·²å®æ–½ï¼Œå³ä½¿æ£€æµ‹åˆ°é›¶æ¢¯åº¦ç»„ï¼Œä¹Ÿä¸å½±å“è®­ç»ƒï¼ˆå·²ä¿®å¤advantageè®¡ç®—ï¼‰**

**æ–°çš„å…³æ³¨ç‚¹ï¼š**
1. **æ¨¡å‹æ˜¯å¦å¼€å§‹æ¢ç´¢ï¼Ÿ** è§‚å¯Ÿç”Ÿæˆå¤šæ ·æ€§ï¼ˆä¸å†å…¨æ˜¯"insufficient information"ï¼‰
2. **ç†µæ˜¯å¦ä¸Šå‡ï¼Ÿ** Entropyä»<0.5ä¸Šå‡åˆ°>1.0
3. **KLæ˜¯å¦åˆç†ï¼Ÿ** beta=0.05ä¸‹ï¼ŒKLåº”è¯¥åœ¨0.1-0.5ä¹‹é—´ï¼ˆæ¯”ä¹‹å‰å¤§ï¼‰
4. **Rewardæ˜¯å¦æœ‰æ³¢åŠ¨ï¼Ÿ** ä¸åº”è¯¥å…¨æ˜¯å¸¸æ•°

**å…³é”®ï¼š** Plan Cå·²ä¿®å¤advantageè®¡ç®—ï¼Œå³ä½¿std<0.01ä¹Ÿæœ‰æ¢¯åº¦ã€‚ç°åœ¨è¦çœ‹çš„æ˜¯æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨åŠ¨ã€‚

#### ğŸ”¥ ä¼˜å…ˆçº§1ï¼šç†µæ˜¯å¦æ¢å¤ï¼ˆå‰5æ­¥ï¼‰
```
[Fairnessè¯Šæ–­@stepX]
  Entropy: X.XXX
```
**æœŸæœ›ï¼š** >1.0ï¼ˆç†æƒ³>1.5ï¼‰
**å¦‚æœï¼š** ä»<0.5 â†’ A+Bä¿®å¤å¤±è´¥

#### ğŸ”¥ ä¼˜å…ˆçº§2ï¼šæ¢¯åº¦ä¿¡å·ï¼ˆå‰10æ­¥ï¼‰
```
Rewardç»Ÿè®¡:
  Fairness: std=X.XXX
  Hallucination: std=X.XXX

æ¢¯åº¦ä¿¡å·å¼ºåº¦:
  Fairness signal: X.XXXX
  Hallucination signal: X.XXXX
```
**æœŸæœ›ï¼š** std>0.1, signal>0
**å¦‚æœï¼š** å¤§é‡std=0 â†’ ä»æœ‰æ¨¡å¼åå¡Œ

#### â­ ä¼˜å…ˆçº§3ï¼šæ¨¡æ¿æ£€æµ‹å™¨æ˜¯å¦ç”Ÿæ•ˆï¼ˆå‰20æ­¥ï¼‰
```
[Judge@stepX] providers={'template_detector': X, ...}
```
**æœŸæœ›ï¼š** å‰5æ­¥æœ‰ä¸€å®šæ¯”ä¾‹ï¼Œ5-20æ­¥é€æ­¥å‡å°‘
**å¦‚æœï¼š** æŒç»­>50% â†’ ç­–ç•¥ä»é”åœ¨æ¨¡æ¿

#### â­ ä¼˜å…ˆçº§4ï¼šç”Ÿæˆå¤šæ ·æ€§
**æœŸæœ›ï¼š** ä¸å†å…¨æ˜¯"insufficient information"ï¼Œæœ‰åŸºäºcontextçš„å®è´¨å›ç­”

---

## ğŸ› ï¸ å¾…ä¿®å¤é—®é¢˜ï¼ˆæ ¹æ®è®­ç»ƒç»“æœå†³å®šä¼˜å…ˆçº§ï¼‰

### Plan B1: è¿‡æ»¤HaluEval-generalå™ªå£°ï¼ˆå¦‚æœHallucinationä¿¡å·ä»å¼±ï¼‰

**ä»£ç ä½ç½®ï¼š** trainer.py:1255-1274

**ä¿®å¤æ–¹æ¡ˆï¼š**
```python
if sub == "general":
    filtered = []
    for it in data:
        if it.get("hallucination") == "no":
            filtered.append(it)
        elif it.get("hallucination") == "yes":
            spans = it.get("hallucination_spans", [])
            # åªä¿ç•™æ˜ç¡®äº‹å®é”™è¯¯ï¼Œæ’é™¤ä¸å®Œæ•´/èƒ½åŠ›å£°æ˜/æ ¼å¼é—®é¢˜
            if spans and not any(keyword in str(spans).lower()
                                 for keyword in ["incomplete", "cannot", "ai language model", "format"]):
                filtered.append(it)
    data = filtered
```

### Plan B2: å¯ç”¨HaluEvalé…å¯¹å¯¹æ¯”å­¦ä¹ ï¼ˆæå‡æ•ˆæœï¼‰

**ä»£ç ä½ç½®ï¼š** trainer.py:1220, 1234

**ä¿®å¤æ–¹æ¡ˆï¼š**
```python
# qaå­é›†ï¼šç”Ÿæˆpositiveå’Œnegativeä¸¤ä¸ªæ ·æœ¬
# Positive
out.append(Sample(
    id=f"halu_{sub}_{i}_pos",
    task="hallucination",
    prompt=prompt,
    target=build_target(it['right_answer']),
    meta={**meta, "label": "positive"}
))

# Negative
out.append(Sample(
    id=f"halu_{sub}_{i}_neg",
    task="hallucination",
    prompt=prompt,
    target=build_target(it['hallucinated_answer']),
    meta={**meta, "label": "negative"}
))

# Judgeéœ€è¦ç›¸åº”è°ƒæ•´ï¼Œå¯¹negativeæ ·æœ¬ç»™è´Ÿåˆ†
```

### Plan B3: é™ä½Generalæƒé‡ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰

**ä»£ç ä½ç½®ï¼š** trainer.py:1164

**ä¿®å¤æ–¹æ¡ˆï¼š**
```python
# åŠ æƒé‡‡æ ·ï¼Œé™ä½generalå­é›†æƒé‡
per_weights = {
    "qa": 1.5,
    "dialogue": 1.5,
    "general": 0.5,  # é™ä½æƒé‡
    "summarization": 1.0
}
per = max(1, int(n_total * per_weights[sub] / sum(per_weights.values())))
```

### Plan B4: æ£€æŸ¥Tokenizationæˆªæ–­é—®é¢˜ï¼ˆå¦‚æœä¸¤ä¸ªä»»åŠ¡éƒ½ä»å¼±ï¼‰

**ä»£ç ä½ç½®ï¼š** trainer.py:2087, 2256, 2501, 2506, 2532

**æ½œåœ¨é—®é¢˜ï¼š**
- å¤šå¤„ä½¿ç”¨ `truncation=True, max_length=896`
- BBQ contextå¯èƒ½è¢«æˆªæ–­ï¼Œå¯¼è‡´disambiguatedæ ·æœ¬çœ‹èµ·æ¥åƒambiguous
- SFTæ—¶å¯èƒ½æŠŠtargetæˆªæ‰

**éªŒè¯è„šæœ¬ï¼š**
```python
# æ£€æŸ¥BBQæ ·æœ¬tokenizationåçš„é•¿åº¦
for sample in bbq[:100]:
    formatted = apply_chat_template(tokenizer, sample.prompt, system_msg)
    tokens = tokenizer(formatted, truncation=False)
    if len(tokens['input_ids']) > 700:
        print(f"âš ï¸ Sample {sample.id}: {len(tokens['input_ids'])} tokens (å¯èƒ½è¢«æˆªæ–­)")
        print(f"Context condition: {sample.meta.get('context_condition')}")
```

---

### ~~Plan C1: å…¨å±€Baselineé‡æ„~~ âœ… å·²å®æ–½ä¸ºPlan Cä¿®æ”¹4

**çŠ¶æ€ï¼š** âœ… å·²å®Œæˆï¼ˆå®æ–½äº†æ··åˆæ–¹æ¡ˆï¼šæ£€æµ‹stdå¹¶é€€åŒ–ï¼‰

**ä»£ç ä½ç½®ï¼š** trainer.py:2569-2608 (compute_group_advantages)

**å®æ–½çš„æ–¹æ¡ˆï¼ˆæ··åˆæ–¹æ¡ˆï¼‰ï¼š**
```python
# æ£€æµ‹stdï¼Œé€‰æ‹©åˆé€‚çš„advantageè®¡ç®—æ–¹å¼
if group_std < 0.01:
    # æ•´ç»„åŒå¥– â†’ ç›´æ¥ç”¨rewardï¼ˆé¿å…é™¤ä»¥0ï¼‰
    group_adv = group_rewards
else:
    # æœ‰å¤šæ ·æ€§ â†’ ç”¨ä¸­å¿ƒåŒ–ï¼ˆä¿ç•™scaleï¼‰
    group_adv = group_rewards - group_mean
```

**ä¸‹é¢æ˜¯åŸè®¡åˆ’çš„å…¶ä»–æ–¹æ¡ˆï¼ˆä¾›å‚è€ƒï¼‰ï¼š**

**å¤‡é€‰æ–¹æ¡ˆ1ï¼šä½¿ç”¨å…¨å±€EMA baselineï¼š**
```python
# åœ¨grpo_trainå‡½æ•°åˆå§‹åŒ–æ—¶æ·»åŠ 
global_baseline_ema = {"fairness": 0.0, "hallucination": 0.0}

# ä¿®æ”¹compute_group_advantages
def compute_group_advantages(rewards: torch.Tensor, tasks: List[str], k: int,
                             global_baseline: Dict[str, float]) -> torch.Tensor:
    """ä½¿ç”¨å…¨å±€EMA baselineè€Œéç»„å†…mean"""
    Bk = rewards.numel()
    B = Bk // k
    adv = torch.zeros_like(rewards)

    for i in range(B):
        task = tasks[i]  # è¿™ç»„çš„ä»»åŠ¡ç±»å‹
        group_rewards = rewards[i*k : (i+1)*k]

        # ä½¿ç”¨å…¨å±€baselineï¼ˆè·¨batch EMAï¼‰
        baseline = global_baseline.get(task, 0.0)
        group_adv = group_rewards - baseline

        adv[i*k : (i+1)*k] = group_adv

    return adv.clamp(-config.ADV_CLIP, config.ADV_CLIP)

# æ¯æ­¥æ›´æ–°å…¨å±€baseline
for task in ["fairness", "hallucination"]:
    task_rewards = rewards[task_mask].mean().item()
    global_baseline_ema[task] = 0.99 * global_baseline_ema[task] + 0.01 * task_rewards
```

**ä¿®å¤æ–¹æ¡ˆï¼ˆæ–¹æ¡ˆ2ï¼šæ£€æµ‹å¹¶è·³è¿‡stdè¿‡å°çš„ç»„ï¼‰ï¼š**
```python
# åœ¨compute_group_advantageså†…éƒ¨
std = r.std(dim=1, keepdim=True).clamp_min(1e-6)
std_too_small = (std < 0.01).squeeze()

adv = torch.zeros_like(r)
if std_too_small.any():
    # å¯¹stdè¿‡å°çš„ç»„ï¼Œç›´æ¥ç”¨ r - meanï¼ˆä¸é™¤ä»¥stdï¼‰
    adv[std_too_small] = (r - mean)[std_too_small]
if (~std_too_small).any():
    # å¯¹æ­£å¸¸ç»„ï¼Œåšæ ‡å‡†åŒ–
    adv[~std_too_small] = ((r - mean) / std)[~std_too_small]
```

**ä¼˜ç‚¹ï¼š**
- æ–¹æ¡ˆ1ï¼šå½»åº•è§£å†³é—®é¢˜ï¼Œå³ä½¿æ‰€æœ‰ç»„éƒ½ç›¸åŒä¹Ÿæœ‰æ¢¯åº¦
- æ–¹æ¡ˆ2ï¼šç®€å•ï¼Œä¿ç•™å¤§éƒ¨åˆ†åŸæœ‰é€»è¾‘

**ç¼ºç‚¹ï¼š**
- æ–¹æ¡ˆ1ï¼šæ”¹åŠ¨è¾ƒå¤§ï¼Œéœ€è¦ä»”ç»†æµ‹è¯•
- æ–¹æ¡ˆ2ï¼šæ²»æ ‡ä¸æ²»æœ¬ï¼Œå¦‚æœæ‰€æœ‰ç»„éƒ½ç›¸åŒä»ç„¶é—®é¢˜å¤§

**æ¨èï¼š** å¦‚æœ>70%ç»„æ— æ¢¯åº¦ï¼Œç”¨æ–¹æ¡ˆ1ï¼›å¦‚æœ30-50%ï¼Œå¯ä»¥å°è¯•æ–¹æ¡ˆ2

---

## ğŸ¯ å†³ç­–æ ‘ï¼ˆ20æ­¥åï¼‰- Plan Cå·²å®æ–½ç‰ˆ

### ç¬¬ä¸€æ­¥ï¼šæ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ ï¼Ÿï¼ˆå‰10æ­¥ï¼‰

```
è§‚å¯Ÿå‰10æ­¥çš„å…³é”®æŒ‡æ ‡
â”‚
â”œâ”€ ç†µä»ç„¶å¾ˆä½ï¼ˆ<0.5ï¼‰+ ç”Ÿæˆä»é«˜åº¦ç›¸ä¼¼ + KLå‡ ä¹ä¸º0
â”‚  â””â”€> ğŸš¨ è‡´å‘½ï¼æ¨¡å‹è¢«é”æ­»
â”‚     â”œâ”€ å¯èƒ½åŸå› 1ï¼šENTROPY_COEF=2.0ä»ä¸å¤Ÿï¼Œç»§ç»­å¢å¤§åˆ°5.0
â”‚     â”œâ”€ å¯èƒ½åŸå› 2ï¼šbeta=0.05ä»å¤ªå¤§ï¼Œé™åˆ°0.01
â”‚     â”œâ”€ å¯èƒ½åŸå› 3ï¼šåŸºåº§æ¨¡å‹å…ˆéªŒå¤ªå¼ºï¼Œè€ƒè™‘å¢åŠ temperature
â”‚     â””â”€> âš¡ è°ƒæ•´è¶…å‚åé‡æ–°è®­ç»ƒ
â”‚
â”œâ”€ ç†µæœ‰ä¸Šå‡ï¼ˆ0.5â†’1.0ï¼‰+ ç”Ÿæˆæœ‰å¤šæ ·æ€§ + KLåœ¨0.1-0.5
â”‚  â””â”€> âœ… Plan Cç”Ÿæ•ˆï¼æ¨¡å‹å¼€å§‹æ¢ç´¢
â”‚     â””â”€> ç»§ç»­è§‚å¯Ÿ20-50æ­¥ï¼Œçœ‹æ˜¯å¦æ”¶æ•›
â”‚
â””â”€ ç†µå‰§çƒˆæ³¢åŠ¨ + Rewardå´©æºƒï¼ˆå…¨æ˜¯æç«¯å€¼ï¼‰
   â””â”€> âš ï¸ æ¢ç´¢è¿‡å¤´ï¼Œä¸ç¨³å®š
      â”œâ”€ é™ä½ENTROPY_COEFï¼ˆ2.0â†’1.0ï¼‰
      â”œâ”€ æˆ–å¢å¤§betaï¼ˆ0.05â†’0.10ï¼‰
      â””â”€> é‡æ–°è®­ç»ƒ
```

### ç¬¬äºŒæ­¥ï¼šå¦‚æœæ¨¡å‹å¼€å§‹å­¦ä¹ ï¼Œè§‚å¯Ÿä»»åŠ¡è¡¨ç°ï¼ˆ20-50æ­¥ï¼‰

```
è®­ç»ƒ20æ­¥åè§‚å¯Ÿç»“æœ
â”‚
â”œâ”€ Fairnessæ¢å¤ + Hallucinationæ¢å¤
â”‚  â””â”€> âœ…âœ…âœ… å®Œå…¨æˆåŠŸï¼Œç»§ç»­è®­ç»ƒåˆ°100-200æ­¥
â”‚
â”œâ”€ Fairnessæ¢å¤ + Hallucinationä»å¼±
â”‚  â””â”€> âš¡ Generalå™ªå£°ä¸»å¯¼
â”‚     â””â”€> å®æ–½Plan B1ï¼ˆè¿‡æ»¤generalï¼‰æˆ–B3ï¼ˆé™ä½æƒé‡ï¼‰
â”‚
â”œâ”€ Fairnessä»å¼± + Hallucinationä»å¼±
â”‚  â””â”€> ğŸ” å…¶ä»–é—®é¢˜
â”‚     â””â”€> å®æ–½Plan B4ï¼ˆæ£€æŸ¥æˆªæ–­ï¼‰
â”‚     â””â”€> æ£€æŸ¥ambig/disambigé‡‡æ ·æ¯”ä¾‹
â”‚
â””â”€ Fairnessä»å¼± + Hallucinationæ¢å¤
   â””â”€> ğŸ¤” ä¸å¤ªå¯èƒ½ï¼Œæ·±å…¥è¯Šæ–­BBQ
```

**å…³é”®ï¼š** Plan Cå·²ä¿®å¤advantageè®¡ç®—ï¼Œç°åœ¨å…³æ³¨ç‚¹æ˜¯æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨åŠ¨ï¼ˆç†µä¸Šå‡ã€ç”Ÿæˆå¤šæ ·åŒ–ï¼‰ã€‚

---

## ğŸ“ å…³é”®æ–‡ä»¶ä½ç½®

### ä¸»è®­ç»ƒè„šæœ¬
```
grpo-dual/src/grpo/trainer.py (3509è¡Œ)
  - Line 126-286: Configé…ç½®
  - Line 226: MIN_NEW_TOKENS_TRAIN (âš¡å·²ä¿®å¤)
  - Line 970-1009: read_json_flex (JSONLè¯»å–)
  - Line 1031-1157: BBQAdapter
  - Line 1126-1133: _find_unknown_option (âœ…æ­£ç¡®å®ç°)
  - Line 1158-1276: HaluEvalAdapter (âš ï¸å¾…ä¼˜åŒ–)
  - Line 1369-1646: MultiCloudJudge
  - Line 1586-1621: evaluate() + æ¨¡æ¿æ£€æµ‹å™¨ (âš¡å·²æ·»åŠ )
  - Line 2681-3220: grpo_train (ä¸»è®­ç»ƒå¾ªç¯)
```

### æ•°æ®æ–‡ä»¶
```
/workspace/data/bbq/
  - Gender_identity.jsonl (5672æ¡ï¼ŒJSONLæ ¼å¼)
  - Disability_status.jsonl (1556æ¡ï¼ŒJSONLæ ¼å¼)
  - ... (å…¶ä»–9ä¸ªç±»åˆ«)

/workspace/data/halueval/
  - qa_data.json (10000æ¡ï¼ŒJSONLæ ¼å¼ï¼Œæœ‰é…å¯¹)
  - dialogue_data.json (10000æ¡ï¼ŒJSONLæ ¼å¼ï¼Œæœ‰é…å¯¹)
  - general_data.json (4507æ¡ï¼ŒJSONLæ ¼å¼ï¼Œâš ï¸å™ªå£°ä¸¥é‡)
  - summarization_data.json (JSONLæ ¼å¼)
```

### è¾“å‡ºæ–‡ä»¶
```
/workspace/multiobjective_llama/<RUN_ID>/
  - train_step_metrics.csv (é€æ­¥æŒ‡æ ‡)
  - train_step_metrics.jsonl (å¤‡ç”¨)
  - training_metrics_summary.json (æœ€ç»ˆæ±‡æ€»)
```

---

## ğŸ”¬ è¯Šæ–­æ–¹æ³•

### å¿«é€Ÿæ£€æŸ¥æ¸…å•ï¼ˆè®­ç»ƒå‰ï¼‰
```bash
# 1. ç¡®è®¤é…ç½®å·²æ›´æ–°
grep "MIN_NEW_TOKENS_TRAIN = 5" grpo-dual/src/grpo/trainer.py
grep "template_phrases = \[" grpo-dual/src/grpo/trainer.py

# 2. æ£€æŸ¥gitçŠ¶æ€
git log -1 --oneline
# åº”è¯¥çœ‹åˆ°: f140a1c Fix entropy collapse...

# 3. ç¡®è®¤åœ¨æ­£ç¡®åˆ†æ”¯
git branch --show-current
# åº”è¯¥æ˜¯: claude/open-trainer-py-011CUp9RqkPbRBQPMVzBRuJ3
```

### è®­ç»ƒä¸­è§‚å¯Ÿï¼ˆå‰20æ­¥ï¼‰
å…³æ³¨ç»ˆç«¯è¾“å‡ºä¸­çš„ï¼š
1. **Fairnessè¯Šæ–­æ¨¡å—** - ç†µå€¼ã€ç”Ÿæˆé•¿åº¦ã€ç”Ÿæˆå†…å®¹
2. **Reward Scaleè¯Šæ–­** - ä¿¡å·å¼ºåº¦ã€stdã€EMAæ¯”å€¼
3. **Judge provideråˆ†å¸ƒ** - template_detectorå‡ºç°é¢‘ç‡
4. **é•¿åº¦æƒ©ç½šç»Ÿè®¡** - å¤šå°‘æ ·æœ¬è¢«æƒ©ç½š

### è®­ç»ƒååˆ†æï¼ˆ50æ­¥+ï¼‰
```bash
# æŸ¥çœ‹CSVï¼ˆæ¨èç”¨pandasï¼‰
import pandas as pd
df = pd.read_csv("/workspace/multiobjective_llama/<RUN_ID>/train_step_metrics.csv")

# å…³é”®åˆ—
df[['step', 'kl_f', 'kl_h', 'reward_f_mean', 'reward_h_mean',
    'clip_frac', 'gen_len_f_mean', 'gen_len_h_mean',
    'trunc_frac_f', 'trunc_frac_h']].head(20)

# ç»˜åˆ¶è¶‹åŠ¿
import matplotlib.pyplot as plt
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
df.plot(x='step', y='reward_f_mean', ax=axes[0,0], title='Fairness Reward')
df.plot(x='step', y='reward_h_mean', ax=axes[0,1], title='Hallucination Reward')
df.plot(x='step', y='gen_len_f_mean', ax=axes[1,0], title='Fairness Length')
df.plot(x='step', y='gen_len_h_mean', ax=axes[1,1], title='Hallucination Length')
plt.tight_layout()
plt.savefig('training_trends.png')
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### Piä¸“å®¶çš„åˆ†æè¦ç‚¹

#### BBQæ•°æ®é›†ï¼ˆå·²éªŒè¯æ— é—®é¢˜ï¼‰
- âœ… ç»“æ„è‰¯å¥½ï¼Œæ ‡æ³¨ä¸€è‡´
- âœ… Ambiguous/Disambiguatedè®¾è®¡æ­£ç¡®
- âœ… æ ‡ç­¾åˆ†å¸ƒå‡è¡¡ï¼ˆå„é€‰é¡¹ã€å„ææ€§éƒ½æˆå¯¹å‡ºç°ï¼‰
- âš ï¸ Unknowné€‰é¡¹ä½ç½®æ˜¯åŠ¨æ€çš„ï¼ˆå¿…é¡»ç”¨answer_infoåˆ¤å®šï¼‰

#### HaluEvalæ•°æ®é›†
**Generalå­é›†é—®é¢˜ï¼š**
- "å¹»è§‰"æ¦‚å¿µæ··ç”¨ï¼ˆäº‹å®é”™è¯¯+ä¸å®Œæ•´+èƒ½åŠ›å£°æ˜+æ ¼å¼é—®é¢˜ï¼‰
- çº¦815ä¸ªyesæ ‡æ³¨ï¼Œå…¶ä¸­13ä¸ªspansä¸ºç©º
- çº¦200+ä¸ª"As an AI..."è¢«æ ‡ä¸ºå¹»è§‰
- ç»“è®ºï¼šéœ€è¦è¿‡æ»¤ï¼Œåªä¿ç•™æ˜ç¡®äº‹å®é”™è¯¯

**Dialogueå­é›†ï¼ˆè´¨é‡å¥½ï¼‰ï¼š**
- âœ… æˆå¯¹æ ‡æ³¨ï¼ˆright vs hallucinatedï¼‰
- âœ… åŸºäºknowledgeçš„ä¸€è‡´æ€§
- âœ… åªæœ‰è½»å¾®å™ªå£°

**QAå­é›†ï¼ˆè´¨é‡å¥½ï¼‰ï¼š**
- âœ… æˆå¯¹æ ‡æ³¨
- âœ… æ¸…æ™°çš„äº‹å®ä¾æ®

### ç®—æ³•å‚è€ƒ

#### BAPO (Balanced Advantage Policy Optimization)
- åŠ¨æ€è°ƒæ•´PPOè£å‰ªè¾¹ç•Œï¼ˆä¸å¯¹ç§°ï¼‰
- c_low: 0.6â†’0.9, c_high: 1.2â†’3.0
- ç›®æ ‡ï¼šå¹³è¡¡positive/negativeæ ·æœ¬çš„æ¢¯åº¦è´¡çŒ®
- é€‚ç”¨åœºæ™¯ï¼šé˜²æ­¢ç†µå¡Œé™·ï¼Œé¼“åŠ±æ¢ç´¢

#### DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)
- Clip-Higher: ä¸Šç•Œä»1.2â†’1.28
- Dynamic Sampling: è¿‡æ»¤accuracy=1æˆ–0çš„promptç»„
- Token-Level Loss: å¯¹æ‰€æœ‰tokenèšåˆ
- Overlong Reward Shaping: æƒ©ç½šè¶…é•¿å›ç­”

---

## ğŸš€ ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸï¼ˆä»Šå¤©ï¼‰
1. âœ… ç”¨A+Bä¿®å¤çš„ä»£ç è·‘50æ­¥
2. âœ… è§‚å¯Ÿå‰20æ­¥è¯Šæ–­è¾“å‡º
3. âœ… æ ¹æ®ç»“æœå†³å®šæ˜¯å¦éœ€è¦Plan B

### ä¸­æœŸï¼ˆå¦‚æœA+BæˆåŠŸï¼‰
1. ç»§ç»­è®­ç»ƒåˆ°100-200æ­¥
2. è§‚å¯Ÿæ”¶æ•›æƒ…å†µ
3. è¯„ä¼°Paretoå‰æ²¿

### ä¸­æœŸï¼ˆå¦‚æœéœ€è¦Plan Bï¼‰
1. æ ¹æ®å†³ç­–æ ‘é€‰æ‹©å¯¹åº”æ–¹æ¡ˆ
2. å®æ–½ä¿®å¤
3. é‡æ–°è®­ç»ƒ50æ­¥éªŒè¯

### é•¿æœŸï¼ˆä¼˜åŒ–æ–¹å‘ï¼‰
1. å®æ–½HaluEvalé…å¯¹å¯¹æ¯”å­¦ä¹ ï¼ˆPlan B2ï¼‰
2. è€ƒè™‘BAPO/DAPOæŠ€æœ¯ï¼ˆä¸å¯¹ç§°è£å‰ªã€åŠ¨æ€é‡‡æ ·ï¼‰
3. å¢åŠ Best-of-Næˆ–Rejection Sampling

---

## âš ï¸ å·²çŸ¥é™åˆ¶å’Œæ³¨æ„äº‹é¡¹

### ç¯å¢ƒ
- GPUæ˜¾å­˜é™åˆ¶ï¼šå·²é™ä½batch sizeå’ŒLoRA rank
- APIé…é¢ï¼šOpenAI + ClaudeåŒäº‘ï¼Œæœ‰heuristicå…œåº•
- ç½‘ç»œç¨³å®šæ€§ï¼šå·²å®ç°é‡è¯•å’ŒæŒ‡æ•°é€€é¿

### æ•°æ®
- BBQåªç”¨äº†2ä¸ªç±»åˆ«ï¼ˆGender, Disabilityï¼‰ï¼Œè¿˜æœ‰9ä¸ªç±»åˆ«æœªç”¨
- HaluEvalçš„generalå­é›†å™ªå£°éœ€è¦å¤„ç†
- é…å¯¹æ ·æœ¬æœªå……åˆ†åˆ©ç”¨

### è®­ç»ƒ
- SFT targetå¯èƒ½ä¸RLé˜¶æ®µçš„æ¨¡æ¿æ£€æµ‹å™¨æœ‰è½»å¾®å†²çªï¼ˆambiguousæ ·æœ¬ï¼‰
- Tokenizationå¯èƒ½æˆªæ–­é•¿contextï¼ˆå¾…éªŒè¯ï¼‰
- KLæ§åˆ¶çš„Î²å€¼å¯èƒ½éœ€è¦æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´

---

## ğŸ“ äº¤æ¥æ£€æŸ¥ç‚¹

æ¥æ‰‹æ­¤é¡¹ç›®çš„äººåº”è¯¥èƒ½å¤Ÿï¼š

### å¿…éœ€äº†è§£
1. âœ… ç†µå¡Œé™·é—®é¢˜çš„æ ¹æœ¬åŸå› å’Œä¿®å¤æ–¹æ¡ˆ
2. âœ… BBQæ•°æ®é›†çš„æ­£ç¡®ä½¿ç”¨æ–¹å¼ï¼ˆåŠ¨æ€unknownï¼‰
3. âœ… HaluEvalæ•°æ®é›†çš„é—®é¢˜å’Œæ½œåœ¨ä¿®å¤æ–¹æ¡ˆ
4. âœ… å¦‚ä½•è§£è¯»è®­ç»ƒæ—¥å¿—ï¼ˆç†µã€æ¢¯åº¦ä¿¡å·ã€provideråˆ†å¸ƒï¼‰

### å¿…éœ€æŒæ¡
1. âœ… è¿è¡Œè®­ç»ƒå¹¶è§‚å¯Ÿå‰20æ­¥
2. âœ… æ ¹æ®å†³ç­–æ ‘é€‰æ‹©å¯¹åº”çš„Plan B
3. âœ… ä¿®æ”¹ä»£ç å®æ–½Plan Bï¼ˆå¦‚æœéœ€è¦ï¼‰
4. âœ… åˆ†æCSVæ–‡ä»¶å’Œç»˜åˆ¶è¶‹åŠ¿å›¾

### å¯é€‰æŠ€èƒ½
1. BAPO/DAPOç®—æ³•çš„å®ç°
2. Paretoå‰æ²¿ä¼˜åŒ–
3. æ›´å¤æ‚çš„reward shaping

---

## ğŸ“ æ›´æ–°æ—¥å¿—

**2025-11-08 (Session 1):**
- âœ… å®ŒæˆA+Bä¿®å¤ï¼ˆMIN_NEW_TOKENSé™ä½ + æ¨¡æ¿æ£€æµ‹å™¨ï¼‰
- âœ… Commit f140a1cæ¨é€åˆ°è¿œç¨‹åˆ†æ”¯
- âœ… æ•´ç†Piä¸“å®¶çš„æ•°æ®é›†åˆ†æ
- âœ… æ·»åŠ C2ç›‘æ§ï¼ˆé›¶æ¢¯åº¦ç»„æ£€æµ‹ï¼‰
- âœ… Commit 7294249æ¨é€ï¼ˆC2ç›‘æ§ + HANDOFFåˆ›å»ºï¼‰

**2025-11-08 (Session 2 - å½“å‰):**
- âœ… æ¥æ”¶Piä¸“å®¶çš„è®­ç»ƒæ—¥å¿—è¯Šæ–­ï¼ˆ6ç‚¹æ€»ç»“ï¼‰
- âœ… **å®æ–½Plan Cå…¨é¢ä¿®å¤ï¼š**
  - âœ… ä¿®æ”¹compute_group_advantagesï¼ˆé¿å…æ¢¯åº¦æŠ¹å¹³ï¼‰
  - âœ… å¢å¼ºç†µæ­£åˆ™åŒ–ï¼ˆENTROPY_COEF: 0.5â†’2.0ï¼‰
  - âœ… é™ä½KLçº¦æŸï¼ˆbeta: 0.30â†’0.05ï¼‰
  - âœ… å¢å¼ºæ¨¡æ¿æ£€æµ‹å™¨ï¼ˆ13ç§çŸ­è¯­ï¼Œæ›´å¤§æƒ©ç½šï¼‰
- ğŸ”„ æ›´æ–°HANDOFF.mdï¼ˆè®°å½•Plan Cå®æ–½ç»†èŠ‚ï¼‰
- â³ å¾…æäº¤å¹¶æ¨é€

**å¾…æ›´æ–°ï¼ˆè®­ç»ƒå®Œæˆåï¼‰ï¼š**
- [ ] å‰10æ­¥çš„å®é™…è§‚å¯Ÿç»“æœï¼ˆå…³æ³¨ç†µæ˜¯å¦ä¸Šå‡ï¼‰
- [ ] æ¨¡å‹æ˜¯å¦å¼€å§‹çœŸæ­£å­¦ä¹ ï¼ˆä¸å†é”æ­»ï¼‰
- [ ] æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´è¶…å‚ï¼ˆENTROPY_COEF, betaï¼‰
- [ ] æœ€ç»ˆè®­ç»ƒæ•ˆæœå’Œæ”¶æ•›æƒ…å†µ

---

**æ–‡æ¡£ç»“æŸã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·å‚è€ƒtrainer.pyä¸­çš„è¯¦ç»†æ³¨é‡Šæˆ–é‡æ–°é˜…è¯»æœ¬æ–‡æ¡£çš„ç›¸å…³ç« èŠ‚ã€‚**
