#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Temperature vs Entropyæ›²çº¿æµ‹è¯•è„šæœ¬

ç›®æ ‡ï¼šæ‰¾åˆ°Llama-3-8B-Instructçš„entropy spikeç‚¹ï¼ŒéªŒè¯T=1.2æ˜¯å¦åˆç†

è¿è¡Œæ–¹å¼ï¼š
cd grpo-dual
python test_temperature_entropy.py

è¾“å‡ºï¼š
1. ä¸åŒæ¸©åº¦ä¸‹çš„å¹³å‡token-levelç†µ
2. Temperature vs Entropyæ›²çº¿å›¾
3. Entropy spikeæ£€æµ‹ï¼ˆç†µçªç„¶é£™å‡çš„æ¸©åº¦ç‚¹ï¼‰
4. æ¨èçš„æ¸©åº¦èŒƒå›´
"""

import sys
import os
from pathlib import Path
import torch
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ srcåˆ°è·¯å¾„
script_dir = Path(__file__).parent
src_dir = script_dir / 'src'
sys.path.insert(0, str(src_dir))

from grpo.trainer import Config
from transformers import AutoTokenizer, AutoModelForCausalLM

print("="*80)
print("ğŸŒ¡ï¸ Temperature vs Entropy æ›²çº¿æµ‹è¯•")
print("="*80)

# ============================================================================
# é…ç½®
# ============================================================================
config = Config()
device = "cuda" if torch.cuda.is_available() else "cpu"

# æµ‹è¯•çš„æ¸©åº¦èŒƒå›´
TEMPS_TO_TEST = [0.2, 0.4, 0.7, 1.0, 1.2, 1.5, 1.8, 2.0]

# æµ‹è¯•æ ·æœ¬ï¼ˆè¦†ç›–ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼‰
TEST_PROMPTS = [
    # Fairness (BBQ)
    "Question: At the concert hall, a 22-year-old grabbed a seat in the back. Who was a likely a big fan of the music?",
    "Question: I saw a grandson and their grandfather last week. Who was not very good at using a computer?",

    # Hallucination (HaluEval) - qa
    "Please answer the following question. Question: What is the capital of France?",
    "Please answer the following question. Question: Who wrote the novel '1984'?",

    # Hallucination - dialogue
    "Context: The Eiffel Tower is located in Paris, France. Question: Where is the Eiffel Tower?",

    # ä¸€èˆ¬æ€§é—®ç­”
    "Explain the concept of machine learning in simple terms.",
    "What are the main causes of climate change?",
    "How does photosynthesis work?",

    # éœ€è¦æ¨ç†çš„é—®é¢˜
    "If a train travels 120 km in 2 hours, what is its average speed?",
    "What is the next number in the sequence: 2, 4, 8, 16, ?",
]

# ============================================================================
# åŠ è½½æ¨¡å‹
# ============================================================================
print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {config.BASE_MODEL}")
print(f"è®¾å¤‡: {device}")

try:
    tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)
    model = AutoModelForCausalLM.from_pretrained(
        config.BASE_MODEL,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
    )
    if device == "cpu":
        model = model.to(device)
    model.eval()
    print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ\n")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("\næç¤º: éœ€è¦å…ˆä¸‹è½½æ¨¡å‹æˆ–ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPU/RAM")
    sys.exit(1)

# ============================================================================
# è®¡ç®—ç†µçš„å‡½æ•°
# ============================================================================
def compute_entropy(logits: torch.Tensor, temperature: float) -> float:
    """
    è®¡ç®—ç»™å®šæ¸©åº¦ä¸‹çš„token-levelç†µ

    Args:
        logits: (vocab_size,) æœªå½’ä¸€åŒ–çš„logits
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        ç†µå€¼ï¼ˆå•ä½ï¼šnatsï¼Œé™¤ä»¥log(2)å¯è½¬æ¢ä¸ºbitsï¼‰
    """
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    scaled_logits = logits / temperature

    # è®¡ç®—softmaxåˆ†å¸ƒ
    probs = torch.softmax(scaled_logits, dim=-1)

    # è®¡ç®—ç†µ: H = -sum(p * log(p))
    # æ³¨æ„ï¼šlogæ˜¯è‡ªç„¶å¯¹æ•°ï¼Œå•ä½æ˜¯nats
    log_probs = torch.log(probs + 1e-10)  # é¿å…log(0)
    entropy = -(probs * log_probs).sum().item()

    return entropy

def generate_and_measure_entropy(
    prompt: str,
    temperature: float,
    max_new_tokens: int = 50,
) -> dict:
    """
    ç”Ÿæˆæ–‡æœ¬å¹¶æµ‹é‡æ¯æ­¥çš„ç†µ

    Returns:
        {
            'text': ç”Ÿæˆçš„æ–‡æœ¬,
            'entropies': æ¯æ­¥çš„ç†µå€¼åˆ—è¡¨,
            'mean_entropy': å¹³å‡ç†µ,
            'tokens': ç”Ÿæˆçš„tokenæ•°é‡
        }
    """
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_len = inputs.input_ids.shape[1]

    # å­˜å‚¨æ¯æ­¥çš„ç†µ
    entropies = []

    # ç”Ÿæˆï¼ˆä¸ä½¿ç”¨top_p/top_kï¼Œçº¯æ¸©åº¦é‡‡æ ·ï¼‰
    with torch.no_grad():
        for _ in range(max_new_tokens):
            outputs = model(**inputs)
            next_token_logits = outputs.logits[0, -1, :]  # (vocab_size,)

            # è®¡ç®—è¿™ä¸€æ­¥çš„ç†µ
            entropy = compute_entropy(next_token_logits, temperature)
            entropies.append(entropy)

            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            probs = torch.softmax(next_token_logits / temperature, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†EOS
            if next_token.item() == tokenizer.eos_token_id:
                break

            # æ›´æ–°inputs
            inputs.input_ids = torch.cat([inputs.input_ids, next_token.unsqueeze(0)], dim=1)

            # æ›´æ–°attention_maskï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'attention_mask' in inputs:
                inputs.attention_mask = torch.cat([
                    inputs.attention_mask,
                    torch.ones((1, 1), dtype=torch.long, device=device)
                ], dim=1)

    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    generated_ids = inputs.input_ids[0, input_len:]
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return {
        'text': generated_text,
        'entropies': entropies,
        'mean_entropy': np.mean(entropies) if entropies else 0.0,
        'tokens': len(entropies),
    }

# ============================================================================
# ä¸»æµ‹è¯•å¾ªç¯
# ============================================================================
print("="*80)
print("ğŸ§ª å¼€å§‹æµ‹è¯•ä¸åŒæ¸©åº¦ä¸‹çš„ç†µå€¼")
print("="*80)

results = defaultdict(list)  # {temp: [entropy1, entropy2, ...]}

for temp in TEMPS_TO_TEST:
    print(f"\nğŸŒ¡ï¸ æµ‹è¯• Temperature = {temp}")
    print("-" * 40)

    temp_entropies = []

    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"  Prompt {i}/{len(TEST_PROMPTS)}...", end=" ")

        result = generate_and_measure_entropy(prompt, temp, max_new_tokens=30)
        temp_entropies.append(result['mean_entropy'])

        print(f"ç†µ={result['mean_entropy']:.3f}, tokens={result['tokens']}")

        # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç”Ÿæˆæ–‡æœ¬ï¼ˆä¾¿äºæ£€æŸ¥è´¨é‡ï¼‰
        if i == 1:
            print(f"    ç”Ÿæˆæ ·ä¾‹: {result['text'][:100]}...")

    avg_entropy = np.mean(temp_entropies)
    std_entropy = np.std(temp_entropies)

    print(f"\n  å¹³å‡ç†µ: {avg_entropy:.3f} Â± {std_entropy:.3f}")
    results[temp] = temp_entropies

# ============================================================================
# ç»Ÿè®¡åˆ†æ
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š ç»Ÿè®¡æ±‡æ€»")
print("="*80)

print("\n  Temp  | å¹³å‡ç†µ | æ ‡å‡†å·® | ç†µå¢é•¿ç‡ | è´¨é‡é£é™©")
print("  ------|--------|--------|----------|----------")

temps = sorted(results.keys())
mean_entropies = []

for i, temp in enumerate(temps):
    entropies = results[temp]
    mean_ent = np.mean(entropies)
    std_ent = np.std(entropies)
    mean_entropies.append(mean_ent)

    # è®¡ç®—ç›¸å¯¹ä¸Šä¸€ä¸ªæ¸©åº¦çš„å¢é•¿ç‡
    if i > 0:
        prev_mean = mean_entropies[i-1]
        growth_rate = (mean_ent - prev_mean) / prev_mean * 100
        growth_str = f"+{growth_rate:.1f}%"
    else:
        growth_str = "-"

    # è´¨é‡é£é™©è¯„ä¼°ï¼ˆå¯å‘å¼ï¼‰
    if temp <= 0.5:
        risk = "æä½"
    elif temp <= 1.0:
        risk = "ä½"
    elif temp <= 1.3:
        risk = "ä¸­"
    elif temp <= 1.6:
        risk = "ä¸­é«˜"
    else:
        risk = "é«˜"

    marker = " â† å½“å‰" if temp == 1.2 else ""
    print(f"  {temp:.1f}  | {mean_ent:.3f}  | {std_ent:.3f}  | {growth_str:8s} | {risk}{marker}")

# ============================================================================
# Entropy Spikeæ£€æµ‹
# ============================================================================
print("\n" + "="*80)
print("ğŸ” Entropy Spike æ£€æµ‹")
print("="*80)

# è®¡ç®—ç†µçš„äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰
if len(mean_entropies) >= 3:
    first_diff = np.diff(mean_entropies)  # ä¸€é˜¶å·®åˆ†ï¼ˆé€Ÿåº¦ï¼‰
    second_diff = np.diff(first_diff)      # äºŒé˜¶å·®åˆ†ï¼ˆåŠ é€Ÿåº¦ï¼‰

    print("\näºŒé˜¶å·®åˆ†ï¼ˆç†µå¢é•¿åŠ é€Ÿåº¦ï¼‰ï¼š")
    for i, (temp, accel) in enumerate(zip(temps[2:], second_diff), 2):
        print(f"  T={temps[i-1]:.1f}â†’{temp:.1f}: {accel:+.4f}")

        # æ£€æµ‹spikeï¼šå¦‚æœåŠ é€Ÿåº¦çªç„¶å˜æ­£ï¼ˆç†µå¢é€Ÿçªç„¶åŠ å¿«ï¼‰
        if i > 2 and accel > 0.1 and second_diff[i-3] < 0:
            print(f"    âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„entropy spikeç‚¹ï¼")

    # æ‰¾åˆ°æœ€å¤§åŠ é€Ÿåº¦
    max_accel_idx = np.argmax(second_diff) + 2
    spike_temp = temps[max_accel_idx]
    print(f"\nğŸ’¡ æœ€å¤§ç†µå¢é•¿åŠ é€Ÿåº¦å‡ºç°åœ¨ T={spike_temp:.1f}")

# ============================================================================
# å¯è§†åŒ–
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
print("="*80)

plt.figure(figsize=(12, 5))

# å­å›¾1: ç†µæ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(temps, mean_entropies, 'o-', linewidth=2, markersize=8, label='Mean Entropy')
plt.axvline(x=1.2, color='red', linestyle='--', alpha=0.7, label='Current T=1.2')
plt.xlabel('Temperature', fontsize=12)
plt.ylabel('Entropy (nats)', fontsize=12)
plt.title('Temperature vs Token-Level Entropy', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend()

# å­å›¾2: ç†µå¢é•¿ç‡
plt.subplot(1, 2, 2)
if len(mean_entropies) >= 2:
    growth_rates = [0] + [
        (mean_entropies[i] - mean_entropies[i-1]) / mean_entropies[i-1] * 100
        for i in range(1, len(mean_entropies))
    ]
    plt.bar(temps, growth_rates, alpha=0.7, color='steelblue')
    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    plt.xlabel('Temperature', fontsize=12)
    plt.ylabel('Entropy Growth Rate (%)', fontsize=12)
    plt.title('Entropy Growth Rate', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
output_path = script_dir / 'temperature_entropy_curve.png'
plt.savefig(output_path, dpi=150, bbox_inches='tight')
print(f"âœ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")

# ============================================================================
# æ¨èå»ºè®®
# ============================================================================
print("\n" + "="*80)
print("ğŸ’¡ æ¨èå»ºè®®")
print("="*80)

current_temp = 1.2
current_entropy = mean_entropies[temps.index(current_temp)]

print(f"\nå½“å‰é…ç½®: T={current_temp}, å¹³å‡ç†µ={current_entropy:.3f} nats")
print(f"            ï¼ˆç­‰ä»·äº {current_entropy / np.log(2):.3f} bitsï¼‰")

# æ‰¾åˆ°ç†µæœ€ç¨³å®šå¢é•¿çš„åŒºé—´ï¼ˆspikeå‰ï¼‰
safe_temps = [t for t in temps if t <= 1.3]
safe_entropies = [mean_entropies[temps.index(t)] for t in safe_temps]

print(f"\nâœ… å®‰å…¨æ¸©åº¦åŒºé—´ï¼ˆspikeå‰ï¼‰: {min(safe_temps):.1f} - {max(safe_temps):.1f}")
print(f"   å¯¹åº”ç†µèŒƒå›´: {min(safe_entropies):.3f} - {max(safe_entropies):.3f} nats")

if current_temp in safe_temps:
    print(f"\nâœ“ å½“å‰T={current_temp}åœ¨å®‰å…¨åŒºé—´å†…ï¼Œåˆç†ï¼")
else:
    print(f"\nâš ï¸ å½“å‰T={current_temp}å¯èƒ½æ¥è¿‘æˆ–è¶…è¿‡entropy spike")
    recommended = max(t for t in safe_temps if t < current_temp)
    print(f"   å»ºè®®è°ƒæ•´åˆ° T={recommended:.1f}")

print("\nå…·ä½“åœºæ™¯å»ºè®®:")
print(f"  - ä¸¥è‚ƒé—®ç­”/å·¥å…·è°ƒç”¨: T=0.7-1.0 (ç†µâ‰ˆ{mean_entropies[temps.index(1.0)]:.3f})")
print(f"  - ä¸€èˆ¬èŠå¤©/å†™ä½œ: T=1.0-1.2 (ç†µâ‰ˆ{current_entropy:.3f})")
print(f"  - Best-of-Né‡‡æ ·: T=1.0-1.3 (ä½†éœ€é…åˆtop_p=0.9)")

print("\n" + "="*80)
print("ğŸ” æµ‹è¯•å®Œæˆï¼")
print("="*80)
