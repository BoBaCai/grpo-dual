# Temperature Scheduler é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

åŸºäºä¸“å®¶å»ºè®®ï¼ˆå‚è€ƒ DeepSeek-R1, EDT, DAPOï¼‰ï¼Œå®ç°äº†ä¸€ä¸ª**ä¸‰é˜¶æ®µæ¸©åº¦è°ƒåº¦å™¨**ï¼Œé…åˆè½»é‡è‡ªé€‚åº”è§„åˆ™ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **Stage-wise é™æ¸©**ï¼šé«˜æ¢ç´¢ï¼ˆT=1.0-1.2ï¼‰ â†’ æ”¶æ•›ï¼ˆT=0.7-0.9ï¼‰ â†’ éƒ¨ç½²å¯¹é½ï¼ˆT=0.6-0.8ï¼‰
2. **Per-task å·®å¼‚åŒ–**ï¼šBBQ/Fairness ç•¥é«˜ï¼ŒHaluEval ç•¥ä½
3. **è½»é‡è‡ªé€‚åº”**ï¼šåŸºäºç†µå’Œæˆªæ–­ç‡åŠ¨æ€å¾®è°ƒï¼ˆæ­¥é•¿ Â±0.05ï¼‰
4. **é…å¥—è°ƒåº¦**ï¼šKL ç³»æ•°ã€max_new_tokensã€æˆªæ–­æƒ©ç½š

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Step 1: å¯¼å…¥è°ƒåº¦å™¨

åœ¨ `trainer.py` é¡¶éƒ¨æ·»åŠ ï¼š

```python
from temperature_scheduler import TemperatureScheduler, TemperatureConfig
```

### Step 2: åˆå§‹åŒ–è°ƒåº¦å™¨

åœ¨ `grpo_train` å‡½æ•°å¼€å§‹å¤„ï¼ˆLine ~2681ï¼‰ï¼š

```python
def grpo_train(...):
    # ... ç°æœ‰ä»£ç  ...

    # ========== æ–°å¢ï¼šåˆå§‹åŒ–æ¸©åº¦è°ƒåº¦å™¨ ==========
    temp_scheduler = TemperatureScheduler(
        total_steps=config.GRPO_STEPS,
        config=TemperatureConfig(
            # å¯ä»¥ä½¿ç”¨é»˜è®¤å€¼ï¼Œæˆ–è‡ªå®šä¹‰
            T_min=0.6,
            T_max=1.3,
            fairness_T_init=1.10,      # BBQ åˆå§‹æ¸©åº¦ç•¥é«˜
            hallucination_T_init=0.95  # HaluEval åˆå§‹æ¸©åº¦ä¸­ç­‰
        )
    )
    print(f"âœ… Temperature Scheduler initialized for {config.GRPO_STEPS} steps")
    # ==========================================
```

### Step 3: åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨

#### 3.1 è·å–å½“å‰æ­¥çš„æ¸©åº¦

åœ¨æ¯ä¸ªè®­ç»ƒæ­¥å¼€å§‹æ—¶ï¼ˆLine ~2935ï¼Œ`for step in range(start_step, config.GRPO_STEPS):`ï¼‰ï¼š

```python
for step in range(start_step, config.GRPO_STEPS):
    gc.collect()
    torch.cuda.empty_cache()

    # ========== æ–°å¢ï¼šè·å–å½“å‰æ­¥çš„æ¸©åº¦ ==========
    # æ–¹æ¡ˆAï¼šä¸æä¾›æŒ‡æ ‡ï¼Œä½¿ç”¨çº¯ stage-wise schedule
    temps = temp_scheduler.get_temperature(step=step)
    T_fairness = temps['fairness']
    T_hallucination = temps['hallucination']
    current_stage = temps['stage']

    # æ–¹æ¡ˆBï¼ˆæ¨èï¼‰ï¼šæä¾›ä¸Šä¸€æ­¥çš„æŒ‡æ ‡ï¼Œå¯ç”¨è‡ªé€‚åº”
    # éœ€è¦åœ¨ä¸‹é¢ 3.2 ä¸­æ”¶é›†æŒ‡æ ‡
    # ==========================================

    print(f"\n{'='*80}")
    print(f"ğŸ”¥ Step {step+1}/{config.GRPO_STEPS} (Stage {current_stage}) - "
          f"T_fair={T_fairness:.3f}, T_halu={T_hallucination:.3f}")
    print('='*80)
```

#### 3.2 æ”¶é›†æŒ‡æ ‡å¹¶å¯ç”¨è‡ªé€‚åº”ï¼ˆæ¨èï¼‰

åœ¨æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶ï¼Œæ”¶é›†ç†µå’Œæˆªæ–­ç‡æŒ‡æ ‡ï¼Œç”¨äºä¸‹ä¸€æ­¥çš„è‡ªé€‚åº”ï¼š

```python
# åœ¨è®­ç»ƒå¾ªç¯æœ«å°¾ï¼ˆè®¡ç®—å®ŒæŒ‡æ ‡åï¼ŒLine ~3150 é™„è¿‘ï¼‰

# ========== æ–°å¢ï¼šæ”¶é›†æŒ‡æ ‡ç”¨äºæ¸©åº¦è‡ªé€‚åº” ==========
# æå– Fairness å’Œ Hallucination çš„ç†µå’Œæˆªæ–­ç‡
fairness_mask = np.array([s.task == "fairness" for s in batch_samples])
halu_mask = ~fairness_mask

# è®¡ç®—å¹³å‡ç†µï¼ˆå¦‚æœæœ‰è®°å½•ï¼‰
fairness_entropy = policy_entropy[fairness_mask].mean() if fairness_mask.any() else None
halu_entropy = policy_entropy[halu_mask].mean() if halu_mask.any() else None

# è®¡ç®—æˆªæ–­ç‡
fairness_trunc_rate = truncation_frac_f  # å·²æœ‰å˜é‡
halu_trunc_rate = truncation_frac_h      # å·²æœ‰å˜é‡

# åœ¨ä¸‹ä¸€æ­¥å¼€å§‹æ—¶ä½¿ç”¨è¿™äº›æŒ‡æ ‡
# ï¼ˆå¯ä»¥å­˜å‚¨åˆ°å…¨å±€å˜é‡æˆ–ç¼“å†²åŒºï¼‰
if step < config.GRPO_STEPS - 1:  # è¿˜æœ‰ä¸‹ä¸€æ­¥
    temps_next = temp_scheduler.get_temperature(
        step=step + 1,
        fairness_entropy=fairness_entropy,
        fairness_trunc_rate=fairness_trunc_rate,
        hallucination_entropy=halu_entropy,
        hallucination_trunc_rate=halu_trunc_rate
    )
# ================================================
```

#### 3.3 ä½¿ç”¨ per-task æ¸©åº¦ç”Ÿæˆå€™é€‰

ä¿®æ”¹ `generate_candidates_batch` è°ƒç”¨ï¼ˆLine ~2945 é™„è¿‘ï¼‰ï¼š

```python
# æ—§ä»£ç ï¼ˆç»Ÿä¸€æ¸©åº¦ï¼‰
# grouped_texts, grouped_lengths, unique_prompt_lens, grouped_truncated, formatted_prompts = \
#     generate_candidates_batch(model, tokenizer, device, prompts, k=config.K_ROLLOUTS, step=step)

# ========== æ–°å¢ï¼šåˆ†ä»»åŠ¡ç”Ÿæˆï¼Œä½¿ç”¨ä¸åŒæ¸©åº¦ ==========
# æŒ‰ä»»åŠ¡åˆ†ç»„
fairness_samples = [s for s in batch_samples if s.task == "fairness"]
halu_samples = [s for s in batch_samples if s.task == "hallucination"]

fairness_prompts = [s.prompt for s in fairness_samples]
halu_prompts = [s.prompt for s in halu_samples]

# åˆ†åˆ«ç”Ÿæˆï¼ˆä¼ å…¥ä¸åŒæ¸©åº¦ï¼‰
fairness_results = generate_candidates_batch(
    model, tokenizer, device,
    fairness_prompts,
    k=config.K_ROLLOUTS,
    temperature=T_fairness,  # ä½¿ç”¨ Fairness æ¸©åº¦
    step=step
)

halu_results = generate_candidates_batch(
    model, tokenizer, device,
    halu_prompts,
    k=config.K_ROLLOUTS,
    temperature=T_hallucination,  # ä½¿ç”¨ Hallucination æ¸©åº¦
    step=step
)

# åˆå¹¶ç»“æœï¼ˆæŒ‰åŸé¡ºåºï¼‰
# ... (éœ€è¦å†™ä¸€ä¸ªåˆå¹¶é€»è¾‘)
# ================================================
```

**æ³¨æ„**ï¼šè¿™éœ€è¦ä¿®æ”¹ `generate_candidates_batch` å‡½æ•°ç­¾åï¼Œæ·»åŠ  `temperature` å‚æ•°ã€‚

#### 3.4 ä¿®æ”¹ `generate_candidates_batch` æ”¯æŒè‡ªå®šä¹‰æ¸©åº¦

åœ¨ `generate_candidates_batch` å‡½æ•°ï¼ˆLine ~2524ï¼‰æ·»åŠ å‚æ•°ï¼š

```python
def generate_candidates_batch(
    model, tokenizer, device,
    prompts: List[str],
    k: int,
    max_new_tokens: int = None,
    step: int = None,
    temperature: float = None  # ========== æ–°å¢ ==========
) -> Tuple[...]:
    """..."""

    if max_new_tokens is None:
        max_new_tokens = config.MAX_NEW_TOKENS_TRAIN

    if temperature is None:
        temperature = config.TEMPERATURE_TRAIN  # ä½¿ç”¨é»˜è®¤å€¼

    # ... ç°æœ‰ä»£ç  ...

    # åœ¨ model.generate è°ƒç”¨æ—¶ä½¿ç”¨ä¼ å…¥çš„ temperature
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        min_new_tokens=config.MIN_NEW_TOKENS_TRAIN,
        temperature=temperature,  # ========== ä½¿ç”¨ä¼ å…¥çš„æ¸©åº¦ ==========
        top_k=config.TOP_K_TRAIN,
        top_p=config.TOP_P_TRAIN,
        # ...
    )
```

### Step 4: é…å¥—åŠŸèƒ½ï¼ˆå¯é€‰ä½†æ¨èï¼‰

#### 4.1 åŠ¨æ€è°ƒæ•´ KL ç³»æ•°

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
current_kl_coef = temp_scheduler.get_kl_coefficient(step)

# åœ¨è®¡ç®— loss æ—¶ä½¿ç”¨
loss = ppo_loss + current_kl_coef * kl_penalty + ...
```

#### 4.2 åŠ¨æ€è°ƒæ•´ max_new_tokens

```python
# è·å–å½“å‰æ­¥çš„ max_new_tokens
max_tokens = temp_scheduler.get_max_new_tokens(step)

# ä¼ ç»™ generate_candidates_batch
generate_candidates_batch(..., max_new_tokens=max_tokens)
```

#### 4.3 æˆªæ–­æƒ©ç½š

```python
# å¯¹è¢«æˆªæ–­çš„æ ·æœ¬é™ä½ reward
trunc_penalty_coef = temp_scheduler.get_truncation_penalty(step)

for i, is_truncated in enumerate(all_truncated):
    if is_truncated:
        rewards[i] *= trunc_penalty_coef  # ä¹˜ä»¥æƒ©ç½šç³»æ•°ï¼ˆ0.3-0.7ï¼‰
```

#### 4.4 é•¿åº¦æ­£åˆ™åŒ–

```python
# å¯¹è¿‡é•¿çš„ç”Ÿæˆæ·»åŠ è´Ÿå¥–åŠ±
lambda_len = temp_scheduler.get_length_penalty_lambda(step)
L_target = 128

for i, length in enumerate(all_lengths):
    if length > L_target:
        len_penalty = -lambda_len * max(0, (length - L_target) / L_target)
        rewards[i] += len_penalty
```

### Step 5: ä¿å­˜å’Œå¯è§†åŒ–

åœ¨è®­ç»ƒç»“æŸæ—¶ï¼š

```python
# ä¿å­˜æ¸©åº¦å†å²
temp_scheduler.save_history(f"{run_dir}/temperature_history.csv")

# ç»˜åˆ¶æ¸©åº¦æ›²çº¿
temp_scheduler.plot_history(f"{run_dir}/temperature_history.png")
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### Stage 1 (0-150 æ­¥ï¼Œ30%)

**ç›®æ ‡**ï¼šé«˜æ¢ç´¢ï¼Œæš´éœ²é—®é¢˜

| æŒ‡æ ‡ | Fairness | Hallucination |
|------|----------|---------------|
| Temperature | 1.10 (èŒƒå›´ 1.0-1.25) | 0.95 (èŒƒå›´ 0.8-1.1) |
| KL coef | 0.003 | 0.003 |
| Max tokens | 256 | 256 |
| Trunc threshold | 40% | 40% |
| Adapt mode | truncation_only | truncation_only |

**æœŸæœ›**ï¼š
- ç†µä¸Šå‡åˆ° 2.0-4.0
- é›¶æ¢¯åº¦ç»„ <40%
- ç”Ÿæˆå¤šæ ·æ€§æå‡ï¼ˆä¸å†å…¨æ˜¯æ¨¡æ¿ï¼‰

### Stage 2 (150-400 æ­¥ï¼Œ50%)

**ç›®æ ‡**ï¼šæ”¶æ•›ï¼Œä¸»åŠ›å¯¹é½

| æŒ‡æ ‡ | Fairness | Hallucination |
|------|----------|---------------|
| Temperature | 1.05â†’0.90ï¼ˆçº¿æ€§ï¼‰ | 0.90â†’0.80ï¼ˆçº¿æ€§ï¼‰ |
| KL coef | 0.003â†’0.01 | 0.003â†’0.01 |
| Max tokens | 256â†’192 | 256â†’192 |
| Trunc threshold | 15% | 15% |
| Adapt mode | both | both |

**æœŸæœ›**ï¼š
- æˆªæ–­ç‡é™åˆ° 10-15%
- ç†µç¨³å®šåœ¨ 3.0-4.0
- Reward æŒç»­ä¸Šå‡

### Stage 3 (400-500 æ­¥ï¼Œ20%)

**ç›®æ ‡**ï¼šç²¾ä¿®ï¼Œæ¥è¿‘éƒ¨ç½²

| æŒ‡æ ‡ | Fairness | Hallucination |
|------|----------|---------------|
| Temperature | 0.80ï¼ˆèŒƒå›´ 0.75-0.9ï¼‰ | 0.75ï¼ˆèŒƒå›´ 0.7-0.8ï¼‰ |
| KL coef | 0.01â†’0.02 | 0.01â†’0.02 |
| Max tokens | 192 | 192 |
| Trunc threshold | 10% | 10% |
| Adapt mode | truncation_only | truncation_only |

**æœŸæœ›**ï¼š
- æˆªæ–­ç‡ <10%
- ç­–ç•¥ç¨³å®šï¼ŒKL ä¸é£™å‡
- Fairness å’Œ Hallucination æŒ‡æ ‡æ¥è¿‘ç›®æ ‡

---

## ğŸ”§ è°ƒè¯•å’Œç›‘æ§

### å…³é”®æ‰“å°ä¿¡æ¯

è°ƒåº¦å™¨ä¼šåœ¨æ¯ 5 ä¸ªçª—å£ï¼ˆé»˜è®¤ 250 æ­¥ï¼‰æ‰“å°ï¼š

```
ğŸŒ¡ï¸ [Step 250] Temperature Update (Stage 2):
  Fairness:      T=0.950 | Entropy=3.45 | Trunc=12.3% | Reason: stable
  Hallucination: T=0.850 | Trunc=8.7% | Reason: entropy_low(2.85<3.0)
```

### å…³é”®æ›²çº¿

è®­ç»ƒåæŸ¥çœ‹ `temperature_history.png`ï¼š

1. **Temperature vs Step**ï¼šæ˜¯å¦å¹³æ»‘é™æ¸©ï¼Ÿ
2. **Entropy vs Step**ï¼šæ˜¯å¦åœ¨ 3-4 åŒºé—´ç¨³å®šï¼Ÿ
3. **Truncation vs Step**ï¼šæ˜¯å¦é€æ­¥ä¸‹é™ï¼Ÿ
4. **T vs Entropy æ•£ç‚¹å›¾**ï¼šè‡ªé€‚åº”æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ

### å¼‚å¸¸æƒ…å†µå¤„ç†

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è°ƒæ•´æ–¹æ¡ˆ |
|------|----------|----------|
| ç†µæŒç»­ <2.0 | æ¸©åº¦è¿‡ä½æˆ– KL è¿‡ä¸¥ | æé«˜ `T_init`ï¼Œé™ä½ `kl_coef` |
| æˆªæ–­ç‡ >50% | max_tokens è¿‡å° | å¢å¤§ Stage 1-2 çš„ max_tokens åˆ° 256-384 |
| é›¶æ¢¯åº¦ç»„ >60% | å€™é€‰ä»é«˜åº¦ç›¸åŒ | æ£€æŸ¥ä¸²è¡Œç”Ÿæˆæ˜¯å¦æ­£ç¡®å®æ–½ |
| Reward å´©æºƒ | æ¢ç´¢è¿‡å¤´ | é™ä½ `T_max`ï¼Œæå‰è¿›å…¥ Stage 2 |

---

## ğŸ“ ä¸ç°æœ‰ HANDOFF.md çš„å…³ç³»

### æ›¿ä»£çš„éƒ¨åˆ†

1. **æ‰‹åŠ¨æ¸©åº¦è°ƒæ•´**ï¼ˆ1.0â†’1.3â†’1.15â†’1.0ï¼‰
   - æ›¿ä»£ä¸ºï¼šStage-wise schedule + è‡ªé€‚åº”

2. **å›ºå®š KL ç³»æ•°**ï¼ˆÎ²=0.05ï¼‰
   - æ›¿ä»£ä¸ºï¼šStage-wise KL schedule (0.003â†’0.02)

3. **å›ºå®š max_new_tokens**ï¼ˆ128ï¼‰
   - æ›¿ä»£ä¸ºï¼šåŠ¨æ€è°ƒæ•´ (256â†’192)

### ä¿ç•™çš„éƒ¨åˆ†

1. âœ… MIN_NEW_TOKENS = 5
2. âœ… ä¸²è¡Œç”Ÿæˆï¼ˆ`generate_candidates_batch`ï¼‰
3. âœ… ç»†ç²’åº¦ Reasoning Quality è¯„åˆ†
4. âœ… Evasive Phrases æ£€æµ‹ï¼ˆ27 ä¸ªå˜ä½“ï¼‰
5. âœ… Advantage è®¡ç®—ä¿®å¤ï¼ˆæ£€æµ‹ std<0.01ï¼‰

### æ–°å¢çš„éƒ¨åˆ†

1. âœ… Per-task æ¸©åº¦å·®å¼‚åŒ–
2. âœ… ç†µå’Œæˆªæ–­ç‡é©±åŠ¨çš„è‡ªé€‚åº”
3. âœ… æˆªæ–­æƒ©ç½šæœºåˆ¶
4. âœ… é•¿åº¦æ­£åˆ™åŒ–
5. âœ… æ¸©åº¦å†å²å¯è§†åŒ–

---

## ğŸ¯ å®æ–½ä¼˜å…ˆçº§

### Phase 1ï¼ˆç«‹å³å¯åšï¼‰ï¼šæœ€å°å¯è¡Œé›†æˆ

åªéœ€ä¿®æ”¹ 3 å¤„ï¼š

1. åˆå§‹åŒ–è°ƒåº¦å™¨
2. åœ¨è®­ç»ƒå¾ªç¯ä¸­è·å–æ¸©åº¦
3. ä¼ ç»™ `generate_candidates_batch`

**é¢„æœŸæ•ˆæœ**ï¼š
- è‡ªåŠ¨ stage-wise é™æ¸©
- å‡å°‘æ‰‹åŠ¨è°ƒå‚

### Phase 2ï¼ˆéªŒè¯åï¼‰ï¼šå¯ç”¨è‡ªé€‚åº”

éœ€è¦ä¿®æ”¹ 2 å¤„ï¼š

1. æ”¶é›†ç†µå’Œæˆªæ–­ç‡æŒ‡æ ‡
2. ä¼ ç»™ `get_temperature`

**é¢„æœŸæ•ˆæœ**ï¼š
- æ¸©åº¦æ ¹æ®å®é™…æŒ‡æ ‡å¾®è°ƒ
- æ›´ç¨³å®šçš„è®­ç»ƒæ›²çº¿

### Phase 3ï¼ˆä¼˜åŒ–ï¼‰ï¼šå®Œæ•´é›†æˆ

æ·»åŠ é…å¥—åŠŸèƒ½ï¼š

1. åŠ¨æ€ KL ç³»æ•°
2. åŠ¨æ€ max_new_tokens
3. æˆªæ–­æƒ©ç½š
4. é•¿åº¦æ­£åˆ™

**é¢„æœŸæ•ˆæœ**ï¼š
- æˆªæ–­ç‡é™åˆ° <10%
- é›¶æ¢¯åº¦ç»„é™åˆ° <30%
- æ•´ä½“è®­ç»ƒæ›´ç¨³å®š

---

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q1: æ˜¯å¦éœ€è¦åŒæ—¶ä¿®æ”¹ trainer.py çš„ configï¼Ÿ

**A**: å»ºè®®ä¿ç•™ `config.TEMPERATURE_TRAIN` ä½œä¸º fallbackï¼Œä½†ä¼˜å…ˆä½¿ç”¨è°ƒåº¦å™¨è¿”å›çš„å€¼ã€‚

### Q2: Per-task æ¸©åº¦ä¼šä¸ä¼šå¢åŠ å¤æ‚åº¦ï¼Ÿ

**A**: ä¼šå¢åŠ ä¸€ç‚¹ï¼Œä½†æ”¶ç›Šæ˜æ˜¾ï¼š
- BBQ éœ€è¦é«˜æ¸©æš´éœ²åè§
- HaluEval éœ€è¦ä¸­ä½æ¸©ä¿è¯å‡†ç¡®æ€§
- ä¸¤è€…æ··åœ¨ä¸€èµ·ç”¨ç»Ÿä¸€æ¸©åº¦æ˜¯æ¬¡ä¼˜çš„

### Q3: å¦‚æœæˆ‘åªæƒ³ç”¨ Stage-wiseï¼Œä¸è¦è‡ªé€‚åº”ï¼Ÿ

**A**: å®Œå…¨å¯ä»¥ï¼åªéœ€åœ¨ `get_temperature` æ—¶ä¸ä¼ æŒ‡æ ‡ï¼š

```python
temps = temp_scheduler.get_temperature(step=step)
# ä¸ä¼  fairness_entropy ç­‰å‚æ•°
```

æˆ–è€…è®¾ç½® `adapt_mode="none"`ã€‚

### Q4: å¦‚ä½•è°ƒæ•´ Stage åˆ’åˆ†æ¯”ä¾‹ï¼Ÿ

**A**: ä¿®æ”¹ `TemperatureConfig`ï¼š

```python
config = TemperatureConfig(
    stage1_end=0.25,  # 25% æ¢ç´¢
    stage2_end=0.85,  # 25-85% æ”¶æ•›
    # 85-100% ç²¾ä¿®
)
```

### Q5: DeepSeek-R1 ç”¨çš„æ˜¯ K=16ï¼Œæˆ‘ä»¬ K=4 å¤Ÿå—ï¼Ÿ

**A**: K=4 å¯¹äºä½ ä»¬çš„ä»»åŠ¡ï¼ˆBBQ+HaluEvalï¼‰æ˜¯åˆç†çš„ï¼š
- BBQ æ˜¯é€‰æ‹©é¢˜ï¼Œå€™é€‰ç©ºé—´æœ‰é™
- HaluEval æœ‰ ground truthï¼Œä¸éœ€è¦æå¤šæ ·æœ¬

å¦‚æœå‘ç°é›¶æ¢¯åº¦ç»„ä» >50%ï¼Œå¯ä»¥è€ƒè™‘å¢å¤§åˆ° K=8ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **DeepSeek-R1** ([Nature 2025](https://www.nature.com/articles/s41586-025-09422-z))
   - Stage 1: T=1.0, K=16, KL=0.001
   - Stage 2: T=0.7, å‡å°‘æ··è¯­å’Œä¸è¿è´¯

2. **EDT: Entropy-based Dynamic Temperature** ([arXiv 2024](https://arxiv.org/abs/2403.14541))
   - ç†µé©±åŠ¨åŠ¨æ€æ¸©åº¦é‡‡æ ·
   - å…¬å¼ï¼šT_new = T_base * exp(Î· * (H - H_0))

3. **DAPO: Open-Source LLM RL** ([arXiv 2025](https://arxiv.org/pdf/2503.14476))
   - å¤šç›®æ ‡ RL é•¿åº¦æ§åˆ¶
   - æˆªæ–­æƒ©ç½šå’Œé•¿åº¦æ­£åˆ™

4. **HaluEval** ([arXiv 2023](https://arxiv.org/abs/2305.11747))
   - å¹»è§‰è¯„ä¼°æ•°æ®é›†è®¾è®¡

---

## âœ… é›†æˆæ£€æŸ¥æ¸…å•

- [ ] å·²å¯¼å…¥ `TemperatureScheduler`
- [ ] å·²åœ¨ `grpo_train` åˆå§‹åŒ–è°ƒåº¦å™¨
- [ ] å·²åœ¨è®­ç»ƒå¾ªç¯ä¸­è·å–æ¸©åº¦
- [ ] å·²ä¿®æ”¹ `generate_candidates_batch` æ”¯æŒ `temperature` å‚æ•°
- [ ] ï¼ˆå¯é€‰ï¼‰å·²å¯ç”¨ç†µå’Œæˆªæ–­ç‡è‡ªé€‚åº”
- [ ] ï¼ˆå¯é€‰ï¼‰å·²å¯ç”¨åŠ¨æ€ KL ç³»æ•°
- [ ] ï¼ˆå¯é€‰ï¼‰å·²å¯ç”¨åŠ¨æ€ max_new_tokens
- [ ] ï¼ˆå¯é€‰ï¼‰å·²å¯ç”¨æˆªæ–­æƒ©ç½š
- [ ] ï¼ˆå¯é€‰ï¼‰å·²å¯ç”¨é•¿åº¦æ­£åˆ™
- [ ] å·²åœ¨è®­ç»ƒç»“æŸä¿å­˜æ¸©åº¦å†å²
- [ ] å·²ç»˜åˆ¶æ¸©åº¦æ›²çº¿å¹¶éªŒè¯

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€**
