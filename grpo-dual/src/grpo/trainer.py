#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
多目标 LoRA + GRPO（v2.2 - 反馈优化版）
核心改进：
- ✅ 训练/评测配置严格分离
- ✅ 截断率监控与自适应max_new_tokens
- ✅ clip_frac按PPO正确定义
- ✅ gen_len统计口径修正+越界检查
- ✅ 评测样本量提升（4→40）
- ✅ max_new_tokens增大（训练96/评测128）
- ⚠️ 训练采样适度放松（temp=0.9，保持一定约束）
- ⚠️ 坚持统一KL控制（不分支化β）

Claude verified: This is the 2624-line trainer.py file
"""

# =============================================================================
# 一键安装 & 冒烟自检（当前内核）
# =============================================================================
import sys as _sys, subprocess as _sp, importlib as _il, os as _os

def _bootstrap_sdks_and_check():
    pkgs = []
    try: _il.import_module("openai")
    except Exception: pkgs.append("openai>=2.3.0")
    try: _il.import_module("anthropic")
    except Exception: pkgs.append("anthropic>=0.69.0")
    if pkgs:
        try:
            _sp.check_call([_sys.executable, "-m", "pip", "install", "-U", *pkgs])
        except Exception as e:
            print("⚠️ SDK 自动安装失败：", e)

    ok = {"openai": False, "anthropic": False}
    print("ENV:", bool(_os.environ.get("OPENAI_API_KEY")), bool(_os.environ.get("ANTHROPIC_API_KEY")))
    # OpenAI 冒烟
    try:
        from openai import OpenAI
        cli = OpenAI()
        r = cli.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0,
            response_format={"type": "json_object"},
            messages=[{"role":"user","content": 'Return ONLY {"final":0.42} as JSON.'}],
            timeout=10,
        )
        print("openai", r.model)
        ok["openai"] = True
    except Exception as e:
        print("[FAIL] OpenAI ->", repr(e))
    # Anthropic 冒烟
    try:
        import anthropic, inspect
        client = anthropic.Anthropic()
        sig = inspect.signature(client.messages.create)
        length_kw = "max_output_tokens" if "max_output_tokens" in sig.parameters else "max_tokens"
        res = client.messages.create(
            model="claude-3-5-haiku-latest",
            temperature=0,
            messages=[{"role":"user","content": 'Return ONLY {"final":0.42} as JSON.'}],
            **{length_kw: 32},
        )
        print("anthropic", getattr(res, "model", "ok"))
        ok["anthropic"] = True
    except Exception as e:
        print("[FAIL] Anthropic ->", repr(e))
    print("SUMMARY:", ok)
    return ok

# =============================================================================
# 静音 gRPC / absl / GLOG 日志
# =============================================================================
import os as _early_os
_early_os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
_early_os.environ.setdefault("GLOG_minloglevel", "2")
_early_os.environ.setdefault("GRPC_TRACE", "")
_early_os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")

try:
    from absl import logging as _absl_logging
    _absl_logging.set_verbosity(_absl_logging.ERROR)
except Exception:
    pass

import os, re, json, time, hashlib, sqlite3, random, warnings, contextlib, threading, uuid
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict, deque

import numpy as np
import torch
import torch.nn.functional as F
from torch.optim import AdamW
from json import JSONDecodeError

# =============================================================================
# torch.compile() 配置优化（修复CUDAGraph动态shape警告）
# =============================================================================
# NLP任务中输入长度是动态的，会导致torch.compile记录过多CUDA图
# 解决方案：配置Inductor跳过动态shape的CUDA图，静默警告
try:
    if hasattr(torch, '_inductor') and hasattr(torch._inductor, 'config'):
        # 跳过动态shape的CUDA图（避免51个不同size的开销）
        torch._inductor.config.triton.cudagraph_skip_dynamic_graphs = True
        # 静默警告
        torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit = None
        # 【可选】启用更积极的fusion优化
        torch._inductor.config.coordinate_descent_tuning = True
except Exception:
    pass  # 旧版本PyTorch不支持这些配置，忽略即可

try:
    from scipy import stats as scipy_stats
except ImportError:
    scipy_stats = None
    print("⚠️ scipy 未安装，将跳过斜率计算")

warnings.filterwarnings("ignore")

# 小幅提速（A100 常用）
try:
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
except Exception:
    pass

print("="*80)
print("环境变量（如需）: ANTHROPIC_API_KEY, OPENAI_API_KEY, HF_TOKEN")
print("="*80)
HF_TOKEN = os.environ.get("HF_TOKEN", "")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"显存(GB): {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}")
else:
    print("⚠️ 无 GPU，将非常慢")

# =============================================================================
# 配置（v2.2 改进版）
# =============================================================================
class Config:
    # 基础模型
    BASE_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"
    HF_TOKEN = HF_TOKEN

    # 路径（增加 run_id 隔离）
    RUN_ID = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
    WORKSPACE = Path("/workspace")
    DATA_DIR = WORKSPACE / "data"
    BBQ_DIR = DATA_DIR / "bbq"
    HALUEVAL_DIR = DATA_DIR / "halueval"
    OUTPUT_DIR = WORKSPACE / "multiobjective_llama" / RUN_ID
    CACHE_DIR = WORKSPACE / "cache"
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

    # 数据文件
    BBQ_FILES = {
        "Age": "Age.jsonl",
        "Disability_status": "Disability_status.jsonl",
        "Gender_identity": "Gender_identity.jsonl",
        "Nationality": "Nationality.jsonl",
        "Physical_appearance": "Physical_appearance.jsonl",
        "Race_ethnicity": "Race_ethnicity.jsonl",
        "Race_x_gender": "Race_x_gender.jsonl",
        "Race_x_SES": "Race_x_SES.jsonl",
        "Religion": "Religion.jsonl",
        "SES": "SES.jsonl",
        "Sexual_orientation": "Sexual_orientation.jsonl",
    }
    HALUEVAL_FILES = {
        "dialogue": "dialogue_data.json",
        "qa": "qa_data.json",
        "general": "general_data.json",
        "summarization": "summarization_data.json",
    }

    # 训练规模
    N_BBQ_TRAIN = 1100
    N_HALU_TRAIN = 400

    # 开关
    DO_SFT_CONTINUE = True
    DO_GRPO = True

    # SFT
    SFT_STEPS = 200
    SFT_LR = 5e-5
    SFT_BATCH_SIZE = 2      # 【显存优化】从4降到2
    SFT_MAXLEN = 896        # 【显存优化】从1024降到896

    # GRPO（显存优化配置）
    GRPO_STEPS = 500
    GRPO_LR = 5e-6          # 从1e-5降到5e-6（降低50%，更稳定）
    GRPO_BATCH_SIZE = 4     # 【性能优化】提升到4，充分利用A100显存，加速30-40%
    K_ROLLOUTS = 4          # 保持4（每个样本4条候选）
    MU_UPDATES = 1
    GRADIENT_ACCUMULATION_STEPS = 1  # 【性能优化】降到1，batch已足够大（有效batch=4）

    # LoRA
    USE_LORA = True
    LORA_R = 8              # 【显存优化】从16降到8，减少参数量
    LORA_ALPHA = 16         # 同步调整 (保持 alpha=2*r)
    LORA_DROPOUT = 0.1
    TARGET_MODULES = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

    # 数值/加速
    USE_BF16 = True
    USE_GRADIENT_CHECKPOINTING = True
    USE_TORCH_COMPILE = True     # 【性能优化】启用torch.compile加速（15-30%提速，PyTorch 2.0+）
    COMPILE_MODE = "reduce-overhead"  # 选项: "default", "reduce-overhead", "max-autotune"
    
    # 【修改】生成配置：满足128硬约束，更激进地降低长度倾向
    MAX_NEW_TOKENS_TRAIN = 64      # 【性能优化】降到64，实际生成长度21-31足够，减少padding浪费
    MAX_NEW_TOKENS_EVAL = 64       # 评测同步降低
    MIN_NEW_TOKENS_TRAIN = 3       # 【降低】从4→3，允许非常短的回复
    
    TEMPERATURE_TRAIN = 0.5        # 【大幅降低】从0.6→0.5，显著更保守
    TOP_K_TRAIN = 30               # 【降低】从40→30，更严格裁剪
    TOP_P_TRAIN = 0.85             # 【降低】从0.9→0.85，更严格
    REP_PENALTY_TRAIN = 1.15       # 【增大】从1.1→1.15，强烈鼓励结束
    
    PRESENCE_PENALTY = 0.6         # 【增大】从0.5→0.6
    FREQUENCY_PENALTY = 0.4        # 【增大】从0.3→0.4
    
    # 【移除】LENGTH_PENALTY_TRAIN（只对beam search有效，采样模式下无效）
    
    # 【修改】截断率监控（128硬约束下的期望）
    TRUNC_FRAC_THRESHOLD = 0.05    # 目标：≤5%（因为上限已经是128）
    TRUNC_FRAC_WARNING = 0.20      # 警告阈值：>20%说明配置有问题
    MAX_NEW_TOKENS_INCREMENT = 0   # 【禁用】不再自动增大（已到硬约束上限）

    # PPO / KL（统一控制）
    PPO_CLIP_EPS = 0.1
    REWARD_CLIP = 1.0
    ADV_CLIP = 5.0
    
    # 【修改】统一KL控制（老师Q13建议）
    KL_BETA_INIT = 0.025            # 初始统一beta
    KL_ADAPTIVE_CONTROL = True      # 是否启用KL自适应控制
    KL_ADAPTIVE_WINDOW = 20         # 自适应控制窗口大小
    KL_TARGET_MIN = 0.05            # KL目标下界
    KL_TARGET_MAX = 0.5             # KL目标上界
    KL_ADJUST_RATIO_HIGH = 1.15     # KL过高时的beta调整倍数
    KL_ADJUST_RATIO_LOW = 0.85      # KL过低时的beta调整倍数
    
    # 【新增】奖励分支内标准化（EMA）
    REWARD_NORMALIZE = True         # 是否开启奖励标准化
    REWARD_EMA_DECAY = 0.99         # EMA 衰减系数
    REWARD_WINSORIZE_QUANTILE = 0.01  # 离群奖励裁剪分位数（P1-P99）
    
    # 【新增】梯度冲突监控
    GRADIENT_CONFLICT_MONITOR = True    # 是否启用梯度冲突监控
    GRADIENT_CONFLICT_THRESHOLD = -0.1  # 余弦相似度阈值

    # CAGrad
    USE_CAGRAD = True
    CAGRAD_C = 0.2

    # Pareto（评测配置）
    PARETO_EVAL_FREQ = 50
    N_PARETO_CHECKPOINTS = 5
    PARETO_PRINT_EVERY = 50          # 【性能优化】降低快速评估频率，与正式评估同步
    PARETO_PRINT_SAMPLES = 40        # 【恢复】保持40，确保评测准确
    PARETO_QUICK_EVAL_SAMPLES = 10   # 【新增】快速评估使用更少样本，仅看趋势

    # 评审器（judge）多云与限流
    # 【加速优化】匹配 GRPO_BATCH_SIZE×K_ROLLOUTS=8 的并发需求（不影响训练效果）
    JUDGE_MAX_WORKERS = 8       # 匹配单步生成数 (2×4=8)，纯并发优化
    JUDGE_TIMEOUT_SEC = 10      # 【恢复】保持原值，避免过多超时影响 reward
    JUDGE_MAX_RETRIES = 1       # 【恢复】保留重试，确保 reward 质量
    RATE_LIMIT_RPS   = 10       # 避免触发限流
    RATE_LIMIT_BURST = 12       # 匹配单步生成数
    
    # 【新增】评审健康度告警阈值
    HEALTH_HEURISTIC_RATIO_WARN = 0.10  # 启发式占比 >10% 告警
    HEALTH_JUDGE_TIME_P95_WARN = 3.0    # judge_time p95 >3s 告警

    # 多云模型（按优先级；先关掉 gemini）
    JUDGE_PROVIDERS = [
        {"name": "openai", "model": "gpt-4o-mini"},
        {"name": "claude", "model": "claude-3-5-haiku-latest"}
    ]

    # 线性刻度校准（确保两个 provider 评分一致）
    JUDGE_CALIBRATION = {
        "openai":    {"a": 1.0, "b": 0.0},
        "claude":    {"a": 1.0, "b": 0.0},
        "heuristic": {"a": 1.0, "b": 0.0},
    }

    # 数据阀门
    SUMM_MAX_DOC_CHARS = 1000
    DATA_FRACTION = 1.0
    FILTER_OUT_FRACTION = 0.0
    
    # 指标记录与汇总
    METRICS_LOG_CSV = True
    METRICS_LOG_JSONL = True
    METRICS_SUMMARY_JSON = True

config = Config()

# 统一种子设置（可重复性）
def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# =============================================================================
# 【新增】生成配置管理器（训练/评测严格分离）
# =============================================================================
class GenerationConfigManager:
    """管理训练采样和评测的生成配置，确保不混用"""
    
    @staticmethod
    def get_train_config():
        """训练采样配置：适度放松，保持一定约束"""
        return {
            "max_new_tokens": config.MAX_NEW_TOKENS_TRAIN,
            "min_new_tokens": config.MIN_NEW_TOKENS_TRAIN,
            "do_sample": True,
            "temperature": config.TEMPERATURE_TRAIN,
            "top_k": config.TOP_K_TRAIN,
            "top_p": config.TOP_P_TRAIN,
            "repetition_penalty": config.REP_PENALTY_TRAIN,
            "renormalize_logits": True,
        }
    
    @staticmethod
    def get_eval_greedy_config():
        """评测greedy配置：确定性，更长以避免截断"""
        return {
            "max_new_tokens": config.MAX_NEW_TOKENS_EVAL,
            "do_sample": False,
            "pad_token_id": None,  # 需要后续填充
            "eos_token_id": None,
        }
    
    @staticmethod
    def get_eval_sampling_config():
        """评测采样配置：与训练采样一致"""
        return {
            "max_new_tokens": config.MAX_NEW_TOKENS_EVAL,
            "min_new_tokens": config.MIN_NEW_TOKENS_TRAIN,
            "do_sample": True,
            "temperature": config.TEMPERATURE_TRAIN,
            "top_k": config.TOP_K_TRAIN,
            "top_p": config.TOP_P_TRAIN,
            "repetition_penalty": config.REP_PENALTY_TRAIN,
            "renormalize_logits": True,
        }
    
    @staticmethod
    def print_config(mode="train"):
        """打印当前生效的生成配置"""
        if mode == "train":
            cfg = GenerationConfigManager.get_train_config()
            title = "训练采样配置"
        elif mode == "eval_greedy":
            cfg = GenerationConfigManager.get_eval_greedy_config()
            title = "评测Greedy配置"
        elif mode == "eval_sampling":
            cfg = GenerationConfigManager.get_eval_sampling_config()
            title = "评测Sampling配置"
        else:
            return
        
        print(f"\n{'='*60}")
        print(f"{title}")
        print(f"{'='*60}")
        for k, v in cfg.items():
            if v is not None:
                print(f"  {k}: {v}")
        print(f"{'='*60}\n")

# =============================================================================
# 【新增】奖励分支内标准化（EMA z-score + 离群值稳健处理）
# =============================================================================
class RewardNormalizer:
    """为每个分支维护独立的 EMA 均值/方差，做 z-score 标准化"""
    def __init__(self, decay=0.99, winsorize_quantile=0.01):
        self.decay = decay
        self.winsorize_q = winsorize_quantile
        self.stats = {}  # {task: {"mean": float, "var": float, "count": int}}
    
    def _winsorize(self, rewards: torch.Tensor) -> torch.Tensor:
        """对奖励做winsorize裁剪，去除极端离群值"""
        if self.winsorize_q <= 0:
            return rewards
        q_low = torch.quantile(rewards, self.winsorize_q)
        q_high = torch.quantile(rewards, 1 - self.winsorize_q)
        return torch.clamp(rewards, q_low, q_high)
    
    def update_and_normalize(self, rewards: torch.Tensor, tasks: List[str]) -> torch.Tensor:
        """更新 EMA 统计量并标准化奖励"""
        if not config.REWARD_NORMALIZE:
            return rewards

        normalized = rewards.clone()

        for task in set(tasks):
            mask = torch.tensor([t == task for t in tasks], device=rewards.device)
            if not mask.any():
                continue

            task_rewards = rewards[mask]

            # 先做winsorize去除极端值
            task_rewards_clean = self._winsorize(task_rewards)

            batch_mean = task_rewards_clean.mean().item()
            batch_var = task_rewards_clean.var().item() if mask.sum() > 1 else 1.0

            # 初始化或更新 EMA
            if task not in self.stats:
                self.stats[task] = {
                    "mean": batch_mean,
                    "var": max(batch_var, 0.01),  # 【修复】最小方差0.01，防止爆炸
                    "count": mask.sum().item()
                }
            else:
                old_mean = self.stats[task]["mean"]
                old_var = self.stats[task]["var"]

                # EMA 更新
                self.stats[task]["mean"] = self.decay * old_mean + (1 - self.decay) * batch_mean
                self.stats[task]["var"] = max(
                    self.decay * old_var + (1 - self.decay) * batch_var,
                    0.01  # 【修复】最小方差0.01
                )
                self.stats[task]["count"] += mask.sum().item()

            # Z-score 标准化
            ema_mean = self.stats[task]["mean"]
            ema_std = np.sqrt(max(self.stats[task]["var"], 0.01))  # 【修复】最小std=0.1
            normalized_task = (task_rewards - ema_mean) / ema_std

            # 【修复】裁剪标准化后的奖励到合理范围 [-10, 10]
            normalized_task = torch.clamp(normalized_task, -10.0, 10.0)

            normalized[mask] = normalized_task

        return normalized
    
    def get_stats(self) -> Dict:
        """获取当前统计量"""
        return {k: {"mean": v["mean"], "std": np.sqrt(v["var"]), "count": v["count"]} 
                for k, v in self.stats.items()}

# =============================================================================
# 【新增】梯度冲突监控与自适应切换
# =============================================================================
class GradientConflictMonitor:
    """
    监控双目标梯度冲突（余弦相似度）
    若持续负相关，自动切换到PCGrad/CAGrad
    """
    def __init__(self, window_size=20, threshold=-0.1, consecutive_threshold=3):
        self.history = deque(maxlen=window_size)
        self.threshold = threshold  # 余弦相似度阈值
        self.consecutive_threshold = consecutive_threshold  # 连续多少次触发
        self.consecutive_negative = 0
        self.use_conflict_resolution = False
        self.log = []
    
    def compute_cosine_similarity(self, grad_f: torch.Tensor, grad_h: torch.Tensor) -> float:
        """计算两个梯度向量的余弦相似度"""
        dot_product = torch.dot(grad_f, grad_h)
        norm_f = grad_f.norm()
        norm_h = grad_h.norm()
        
        if norm_f < 1e-8 or norm_h < 1e-8:
            return 0.0
        
        cosine_sim = dot_product / (norm_f * norm_h + 1e-8)
        return float(cosine_sim)
    
    def update(self, grad_f: torch.Tensor, grad_h: torch.Tensor, step: int) -> Dict:
        """
        更新监控状态
        返回是否应该使用冲突解决策略
        """
        cosine_sim = self.compute_cosine_similarity(grad_f, grad_h)
        self.history.append(cosine_sim)
        
        # 检测连续负相关
        if cosine_sim < self.threshold:
            self.consecutive_negative += 1
        else:
            self.consecutive_negative = 0
        
        # 触发冲突解决
        if self.consecutive_negative >= self.consecutive_threshold and not self.use_conflict_resolution:
            self.use_conflict_resolution = True
            msg = f"检测到持续梯度冲突（连续{self.consecutive_negative}次<{self.threshold}），启用CAGrad"
            self.log.append({"step": step, "action": msg, "cosine_sim": cosine_sim})
            print(f"\n⚠️ [GradientConflict@{step}] {msg}")
        
        return {
            "cosine_sim": cosine_sim,
            "use_conflict_resolution": self.use_conflict_resolution,
            "consecutive_negative": self.consecutive_negative
        }
    
    def get_recent_stats(self) -> Dict:
        """获取最近窗口的统计"""
        if not self.history:
            return {"mean": 0.0, "min": 0.0, "negative_ratio": 0.0}
        
        history_list = list(self.history)
        return {
            "mean": float(np.mean(history_list)),
            "min": float(np.min(history_list)),
            "negative_ratio": sum(1 for x in history_list if x < 0) / len(history_list)
        }

# =============================================================================
# §7: 分支化KL控制器（恢复原设计，拒绝"老师Q13建议"）
# =============================================================================
class BranchedKLController:
    """
    分支化KL自适应控制器

    §7修复说明：
    - Fairness分支：低β (0.02)，保持可用性，目标KL∈[0.02, 0.06]
    - Hallucination分支：高β (0.10)，保证安全性，目标KL∈[0.08, 0.15]

    两个分支有不同的KL需求，必须独立控制！
    """
    def __init__(self,
                 beta_f_init: float = 0.02,    # Fairness初始β
                 beta_h_init: float = 0.10,    # Hallucination初始β
                 window_size: int = 20):
        self.beta_f = beta_f_init
        self.beta_h = beta_h_init
        self.window_size = window_size

        # 独立的KL历史
        self.kl_f_history = deque(maxlen=window_size)
        self.kl_h_history = deque(maxlen=window_size)

        # 【性能修复】放宽KL目标，基于实际训练数据调整
        # Fairness: 实际KL在0.09-0.27，目标放宽到0.08-0.15
        # Hallucination: 实际KL在0.03-0.05，目标降低到0.04-0.10
        self.target_kl_f_min = 0.08
        self.target_kl_f_max = 0.15
        self.target_kl_h_min = 0.04
        self.target_kl_h_max = 0.10

        self.adjustment_log = []

    def update(self, kl_f: float, kl_h: float):
        """记录本步的分支KL值"""
        self.kl_f_history.append(kl_f)
        self.kl_h_history.append(kl_h)

    def get_beta_f(self) -> float:
        """获取Fairness的β"""
        return self.beta_f

    def get_beta_h(self) -> float:
        """获取Hallucination的β"""
        return self.beta_h

    def should_adjust(self) -> bool:
        """是否应该触发调整检查"""
        return len(self.kl_f_history) >= self.window_size

    def auto_adjust(self, step: int) -> Optional[str]:
        """
        自动调整两个分支的β
        返回调整建议
        """
        if not config.KL_ADAPTIVE_CONTROL or not self.should_adjust():
            return None

        kl_f_median = float(np.median(list(self.kl_f_history)))
        kl_h_median = float(np.median(list(self.kl_h_history)))

        old_beta_f = self.beta_f
        old_beta_h = self.beta_h
        actions = []

        # Fairness分支调整
        if kl_f_median > self.target_kl_f_max:
            self.beta_f = old_beta_f * config.KL_ADJUST_RATIO_HIGH
            actions.append(f"Fairness KL过高({kl_f_median:.3f}>{self.target_kl_f_max:.2f})，β_f↑15%: {old_beta_f:.4f}→{self.beta_f:.4f}")
        elif kl_f_median < self.target_kl_f_min:
            self.beta_f = old_beta_f * config.KL_ADJUST_RATIO_LOW
            actions.append(f"Fairness KL过低({kl_f_median:.3f}<{self.target_kl_f_min:.2f})，β_f↓15%: {old_beta_f:.4f}→{self.beta_f:.4f}")

        # Hallucination分支调整
        if kl_h_median > self.target_kl_h_max:
            self.beta_h = old_beta_h * config.KL_ADJUST_RATIO_HIGH
            actions.append(f"Hallucination KL过高({kl_h_median:.3f}>{self.target_kl_h_max:.2f})，β_h↑15%: {old_beta_h:.4f}→{self.beta_h:.4f}")
        elif kl_h_median < self.target_kl_h_min:
            self.beta_h = old_beta_h * config.KL_ADJUST_RATIO_LOW
            actions.append(f"Hallucination KL过低({kl_h_median:.3f}<{self.target_kl_h_min:.2f})，β_h↓15%: {old_beta_h:.4f}→{self.beta_h:.4f}")

        if actions:
            log_entry = {
                "step": step,
                "kl_f_median": kl_f_median,
                "kl_h_median": kl_h_median,
                "old_beta_f": old_beta_f,
                "old_beta_h": old_beta_h,
                "new_beta_f": self.beta_f,
                "new_beta_h": self.beta_h,
                "actions": actions
            }
            self.adjustment_log.append(log_entry)
            print(f"\n[BranchedKL@{step}] " + "; ".join(actions))
            return "; ".join(actions)

        return None

    def get_adjustment_history(self) -> List[Dict]:
        """获取调整历史"""
        return self.adjustment_log

# =============================================================================
# 训练指标记录与汇总（增强版：移动统计）
# =============================================================================
class TrainingMetrics:
    """训练指标记录、落盘和汇总"""
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.csv_path = output_dir / "train_step_metrics.csv"
        self.jsonl_path = output_dir / "train_step_metrics.jsonl"
        self.summary_path = output_dir / "training_metrics_summary.json"
        self.metrics = []
        self.window_size = 50  # 移动窗口大小
        self._init_csv()
    
    def _init_csv(self):
        """初始化 CSV 文件头（§7: 分支化KL控制）"""
        headers = [
            "step", "loss", "kl_f", "kl_h", "kl_overall", "beta_f", "beta_h", "grad_cosine_sim",
            "reward_f_mean", "reward_h_mean", "reward_f_std", "reward_h_std",
            "clip_frac", "gen_len_f_mean", "gen_len_h_mean",
            "trunc_frac_f", "trunc_frac_h",  # 【新增】截断率
            "zero_len_rate_f", "zero_len_rate_h",
            "judge_time", "provider_openai", "provider_claude", "provider_heuristic",
            "adv_abs_mean", "delta_mean", "nan_inf_hits", "lr", "ppo_eps",
            # 移动统计列
            "gen_len_f_moving_median", "gen_len_h_moving_median",
            "clip_frac_moving_p95", "max_new_tokens_train"  # 【新增】记录当前max_new_tokens
        ]
        with open(self.csv_path, "w") as f:
            f.write(",".join(headers) + "\n")
    
    def record_step(self, step: int, metrics: Dict):
        """记录单步指标"""
        self.metrics.append({"step": step, **metrics})
        
        # 计算移动统计
        window_metrics = self.metrics[-self.window_size:] if len(self.metrics) >= self.window_size else self.metrics
        
        gen_len_f_values = [m.get("gen_len_f_mean", 0) for m in window_metrics]
        gen_len_h_values = [m.get("gen_len_h_mean", 0) for m in window_metrics]
        clip_frac_values = [m.get("clip_frac", 0) for m in window_metrics]
        
        moving_median_f = float(np.median(gen_len_f_values)) if gen_len_f_values else 0
        moving_median_h = float(np.median(gen_len_h_values)) if gen_len_h_values else 0
        moving_p95_clip = float(np.percentile(clip_frac_values, 95)) if clip_frac_values else 0
        
        # 写入 CSV
        with open(self.csv_path, "a") as f:
            row = [
                step, metrics.get("loss", 0),
                metrics.get("kl_f", 0), metrics.get("kl_h", 0),
                metrics.get("kl_overall", 0), metrics.get("beta_f", 0), metrics.get("beta_h", 0),
                metrics.get("grad_cosine_sim", 0),
                metrics.get("reward_f_mean", 0), metrics.get("reward_h_mean", 0),
                metrics.get("reward_f_std", 0), metrics.get("reward_h_std", 0),
                metrics.get("clip_frac", 0),
                metrics.get("gen_len_f_mean", 0), metrics.get("gen_len_h_mean", 0),
                metrics.get("trunc_frac_f", 0), metrics.get("trunc_frac_h", 0),
                metrics.get("zero_len_rate_f", 0), metrics.get("zero_len_rate_h", 0),
                metrics.get("judge_time", 0),
                metrics.get("provider_openai", 0), metrics.get("provider_claude", 0),
                metrics.get("provider_heuristic", 0),
                metrics.get("adv_abs_mean", 0), metrics.get("delta_mean", 0),
                metrics.get("nan_inf_hits", 0), metrics.get("lr", 0), metrics.get("ppo_eps", 0),
                moving_median_f, moving_median_h, moving_p95_clip,
                metrics.get("max_new_tokens_train", config.MAX_NEW_TOKENS_TRAIN)
            ]
            f.write(",".join(map(str, row)) + "\n")
        
        # 【修正】写入 JSONL 前确保所有值都是 JSON 可序列化的
        # 将 numpy 类型转换为 Python 原生类型
        json_safe_metrics = {}
        for k, v in metrics.items():
            if isinstance(v, (np.floating, np.integer)):
                json_safe_metrics[k] = float(v)
            elif isinstance(v, np.ndarray):
                json_safe_metrics[k] = v.tolist()
            else:
                json_safe_metrics[k] = v
        
        with open(self.jsonl_path, "a") as f:
            f.write(json.dumps({"step": step, **json_safe_metrics}) + "\n")
    
    def generate_summary(self) -> Dict:
        """生成训练汇总报告（含健康度告警）"""
        if not self.metrics:
            return {"error": "No metrics recorded"}
        
        import numpy as np
        
        summary = {
            "run_id": config.RUN_ID,
            "timestamp": datetime.now().isoformat(),
            "total_steps": len(self.metrics)
        }
        
        # 收集各指标的时序数据
        keys = ["loss", "kl_f", "kl_h", "kl_overall", "reward_f_mean", "reward_h_mean", "clip_frac",
                "gen_len_f_mean", "gen_len_h_mean", "trunc_frac_f", "trunc_frac_h",
                "zero_len_rate_f", "zero_len_rate_h",
                "adv_abs_mean", "delta_mean", "judge_time"]
        
        for key in keys:
            values = [m.get(key, 0) for m in self.metrics]
            if values:
                summary[key] = {
                    "mean": float(np.mean(values)),
                    "median": float(np.median(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values)),
                    "p90": float(np.percentile(values, 90)),
                    "p95": float(np.percentile(values, 95))
                }
        
        # 计算奖励斜率
        steps = [m["step"] for m in self.metrics]
        reward_f = [m.get("reward_f_mean", 0) for m in self.metrics]
        reward_h = [m.get("reward_h_mean", 0) for m in self.metrics]
        
        if len(steps) > 1 and scipy_stats is not None:
            try:
                slope_f, _, _, _, _ = scipy_stats.linregress(steps, reward_f)
                slope_h, _, _, _, _ = scipy_stats.linregress(steps, reward_h)
                summary["slope_reward_f"] = float(slope_f)
                summary["slope_reward_h"] = float(slope_h)
            except Exception:
                summary["slope_reward_f"] = (reward_f[-1] - reward_f[0]) / len(steps) if len(steps) > 1 else 0.0
                summary["slope_reward_h"] = (reward_h[-1] - reward_h[0]) / len(steps) if len(steps) > 1 else 0.0
        else:
            summary["slope_reward_f"] = 0.0
            summary["slope_reward_h"] = 0.0
        
        # 【新增】评审健康度检查
        health_warnings = []
        
        # 检查启发式占比
        total_providers = sum([
            m.get("provider_openai", 0) + m.get("provider_claude", 0) + m.get("provider_heuristic", 0)
            for m in self.metrics
        ])
        heuristic_count = sum([m.get("provider_heuristic", 0) for m in self.metrics])
        if total_providers > 0:
            heuristic_ratio = heuristic_count / total_providers
            if heuristic_ratio > config.HEALTH_HEURISTIC_RATIO_WARN:
                health_warnings.append(
                    f"启发式占比 {heuristic_ratio:.1%} >10%，建议：检查 API/网络/并发，或固定单 provider 减样本"
                )
        
        # 检查 judge_time p95
        if "judge_time" in summary and summary["judge_time"]["p95"] > config.HEALTH_JUDGE_TIME_P95_WARN:
            health_warnings.append(
                f"judge_time p95={summary['judge_time']['p95']:.1f}s >3s，建议：降低 PARETO_PRINT_SAMPLES、"
                f"缩短 JUDGE_TIMEOUT_SEC 或提高并发"
            )
        
        summary["health_warnings"] = health_warnings
        
        # 生成调参建议
        suggestions = []
        
        # 【新增】截断率检查（硬约束128下期望≤5%）
        if summary.get("trunc_frac_f", {}).get("mean", 0) > 0.05:
            suggestions.append(f"fairness 截断率 {summary['trunc_frac_f']['mean']:.1%} >5%，建议降低 temperature 或增大 rep_penalty/presence_penalty")
        if summary.get("trunc_frac_h", {}).get("mean", 0) > 0.05:
            suggestions.append(f"hallucination 截断率 {summary['trunc_frac_h']['mean']:.1%} >5%，建议降低 temperature 或增大 rep_penalty/presence_penalty")
        
        # KL 检查（统一使用 0.8 阈值）
        if summary.get("kl_overall", {}).get("median", 0) > 0.8:
            suggestions.append("整体KL过高 >0.8，建议降低 KL_BETA_INIT 20-30%")
        if summary.get("kl_overall", {}).get("median", 0) < 0.02 and (summary["slope_reward_f"] <= 0 or summary["slope_reward_h"] <= 0):
            suggestions.append("整体KL过低 <0.02 且奖励无进展，建议学习率 ×1.5 或 MU_UPDATES +1")
        
        # Clip 检查
        if summary.get("clip_frac", {}).get("p95", 0) > 0.5:
            suggestions.append("clip_frac p95 >0.5，建议学习率 ×0.5 或 PPO_CLIP_EPS 0.1→0.15")
        
        # 生成长度检查（硬约束128）
        if summary.get("gen_len_f_mean", {}).get("median", 0) < 8:
            suggestions.append("fairness 生成过短 <8，建议降低 min_new_tokens 或检查数据")
        elif summary.get("gen_len_f_mean", {}).get("median", 0) > 120:
            suggestions.append("fairness 生成过长 >120，接近硬约束，建议降低 temperature 或增大 rep_penalty/presence_penalty")
        
        if summary.get("gen_len_h_mean", {}).get("median", 0) < 8:
            suggestions.append("hallucination 生成过短 <8，建议降低 min_new_tokens 或检查数据")
        elif summary.get("gen_len_h_mean", {}).get("median", 0) > 120:
            suggestions.append("hallucination 生成过长 >120，接近硬约束，建议降低 temperature 或增大 rep_penalty/presence_penalty")
        
        # 零长度检查
        if summary.get("zero_len_rate_f", {}).get("mean", 0) > 0.05:
            suggestions.append("fairness 零长度比例 >5%，需检查解码配置")
        if summary.get("zero_len_rate_h", {}).get("mean", 0) > 0.05:
            suggestions.append("hallucination 零长度比例 >5%，需检查解码配置")
        
        summary["suggestions"] = suggestions
        
        # Verdict（结论篇，统一使用 0.8 阈值）
        verdict = self._compute_verdict(summary)
        summary["verdict"] = verdict
        
        return summary
    
    def _compute_verdict(self, summary: Dict) -> str:
        """计算结论篇：绿/黄/橙（统一 0.8 阈值）"""
        green_criteria = 0
        
        # 检查 5 个绿灯条件
        if summary["slope_reward_f"] > 0 or summary["slope_reward_h"] > 0:
            green_criteria += 1
        
        kl_overall_med = summary.get("kl_overall", {}).get("median", 0)
        if 0.05 <= kl_overall_med <= 0.5:
            green_criteria += 1
        
        if summary.get("clip_frac", {}).get("p95", 0) <= 0.5:
            green_criteria += 1
        
        gen_f = summary.get("gen_len_f_mean", {}).get("median", 0)
        gen_h = summary.get("gen_len_h_mean", {}).get("median", 0)
        if gen_f >= 8 and gen_h >= 8:
            green_criteria += 1
        
        # 【新增】截断率检查（严格一些，因为上限已经是128）
        trunc_f = summary.get("trunc_frac_f", {}).get("mean", 0)
        trunc_h = summary.get("trunc_frac_h", {}).get("mean", 0)
        if trunc_f <= 0.05 and trunc_h <= 0.05:  # 都≤5%
            green_criteria += 1
        
        # 判定（统一 0.8 阈值）
        if green_criteria >= 3:
            return "绿灯（有效）：训练起作用，满足 ≥3 项绿灯标准"
        elif (summary["slope_reward_f"] < 0 or summary["slope_reward_h"] < 0) and kl_overall_med > 0.8:
            return "橙灯（可能被 KL 压制）：奖励下降且 KL 飙高 >0.8"
        else:
            return f"黄灯（作用不明显/需调参）：满足 {green_criteria}/5 项绿灯标准"
    
    def save_summary(self):
        """保存汇总报告并打印到控制台"""
        summary = self.generate_summary()
        
        # 保存 JSON
        with open(self.summary_path, "w") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # 打印到控制台
        print("\n" + "="*80)
        print("Training Metrics Summary")
        print("="*80)
        print(f"Run ID: {summary.get('run_id', 'N/A')}")
        print(f"总步数: {summary['total_steps']}")
        print(f"\n【奖励斜率】")
        print(f"  slope_reward_f: {summary['slope_reward_f']:.6f}")
        print(f"  slope_reward_h: {summary['slope_reward_h']:.6f}")
        print(f"\n【KL 散度】")
        print(f"  kl_overall median: {summary.get('kl_overall', {}).get('median', 0):.4f}")
        print(f"\n【Clip 比例】")
        print(f"  clip_frac p95: {summary.get('clip_frac', {}).get('p95', 0):.4f}")
        print(f"\n【生成长度】")
        print(f"  gen_len_f median: {summary.get('gen_len_f_mean', {}).get('median', 0):.2f}")
        print(f"  gen_len_h median: {summary.get('gen_len_h_mean', {}).get('median', 0):.2f}")
        print(f"\n【截断率】")
        print(f"  trunc_frac_f mean: {summary.get('trunc_frac_f', {}).get('mean', 0):.4f}")
        print(f"  trunc_frac_h mean: {summary.get('trunc_frac_h', {}).get('mean', 0):.4f}")
        print(f"\n【零长度比例】")
        print(f"  zero_len_rate_f mean: {summary.get('zero_len_rate_f', {}).get('mean', 0):.4f}")
        print(f"  zero_len_rate_h mean: {summary.get('zero_len_rate_h', {}).get('mean', 0):.4f}")
        
        # 【新增】健康度告警
        if summary.get("health_warnings"):
            print(f"\n【评审健康度告警】")
            for i, warn in enumerate(summary["health_warnings"], 1):
                print(f"  ⚠️ {i}. {warn}")
        
        if summary.get("suggestions"):
            print(f"\n【调参建议】")
            for i, sug in enumerate(summary["suggestions"], 1):
                print(f"  {i}. {sug}")
        
        print(f"\n【Verdict】")
        print(f"  {summary['verdict']}")
        print("="*80 + "\n")
        
        return summary

# =============================================================================
# 更健壮的 JSON 读取（数组 / JSONL / 拼接对象）
# =============================================================================
def read_json_flex(path: Path) -> List[Dict]:
    if not path.exists():
        return []
    text = path.read_text(encoding="utf-8", errors="replace")
    # 直接整体解析
    try:
        obj = json.loads(text)
        if isinstance(obj, list):
            return [x for x in obj if isinstance(x, dict)]
        if isinstance(obj, dict):
            return [obj]
    except Exception:
        pass
    # 连续 raw_decode（应对多个 JSON 对象拼接）
    dec = json.JSONDecoder()
    idx, n, out, bad = 0, len(text), [], 0
    while idx < n:
        while idx < n and text[idx].isspace():
            idx += 1
        if idx >= n:
            break
        try:
            obj, nxt = dec.raw_decode(text, idx)
            if isinstance(obj, dict):
                out.append(obj)
            idx = nxt
        except JSONDecodeError:
            bad += 1
            nl = text.find("\n", idx+1)
            idx = (nl+1) if nl != -1 else n
    if out:
        if bad:
            print(f"⚠️ 解析 {path.name}: 跳过 {bad} 段无效 JSON（已容错）")
        return out
    # 行级兜底（JSONL）
    out, bad = [], 0
    for ln in text.splitlines():
        s = ln.strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
            if isinstance(obj, dict):
                out.append(obj)
        except Exception:
            bad += 1
    if bad:
        print(f"⚠️ 解析 {path.name}: 行级兜底跳过 {bad} 行")
    return out

# =============================================================================
# 数据适配器
# =============================================================================
@dataclass
class Sample:
    id: str
    task: str
    prompt: str
    target: Optional[str] = None
    meta: Dict[str, Any] = field(default_factory=dict)

class BBQAdapter:
    def __init__(self):
        self.categories = list(config.BBQ_FILES.keys())
        print(f"BBQ Adapter: {len(self.categories)} 类别")

    def load_samples(self, n_total: int) -> List[Sample]:
        per_cat = max(1, n_total // max(1, len(self.categories)))
        ck: List[Sample] = []
        for cat in self.categories:
            fp = config.BBQ_DIR / config.BBQ_FILES[cat]
            if not fp.exists():
                print(f"⚠️ 缺文件: {fp}")
                continue
            lines = read_json_flex(fp)
            if not lines:
                print(f"✗ {fp.name} 解析为空")
                continue
            groups = defaultdict(list)
            for it in lines:
                key = (it.get("context_condition",""), it.get("question_polarity",""))
                groups[key].append(it)
            want = per_cat; picked=[]
            keys = list(groups.keys()) if groups else [("","")]
            for gk in keys:
                gitems = groups[gk]
                if gitems:
                    take = min(max(1, want//4), len(gitems))
                    picked.extend(random.sample(gitems, take))
            if len(picked) < want:
                rest = [x for x in lines if x not in picked]
                if rest:
                    picked.extend(random.sample(rest, min(len(rest), want-len(picked))))
            picked = picked[:want]

            for i, it in enumerate(picked):
                prompt = self._build_prompt(it)
                unk = self._find_unknown_option(it)
                target = self._build_sft_target(it, unk)
                ck.append(Sample(
                    id=f"bbq_{cat}_{i}", task="fairness", prompt=prompt, target=target,
                    meta={"dataset":"BBQ","category":cat,"context_condition":it.get("context_condition",""),
                          "question_polarity":it.get("question_polarity",""),"unknown_option":unk,"label":it.get("label",-1)}
                ))
        print(f"BBQ 总计: {len(ck)}")
        return ck

    def _build_prompt(self, it: Dict) -> str:
        """
        §1修复: 返回原始内容prompt，后续会在generate时统一应用聊天模板
        不在这里拼接模板，避免重复套用
        """
        ctx, q = it.get("context",""), it.get("question","")
        a0, a1, a2 = it.get("ans0",""), it.get("ans1",""), it.get("ans2","")
        # 返回纯文本内容，不添加chat template
        return (f"Context: {ctx}\nQuestion: {q}\nOptions:\nA) {a0}\nB) {a1}\nC) {a2}\n\n"
                f"Pick A, B, or C, then give a one-sentence justification that cites only the context.")

    def _find_unknown_option(self, it: Dict) -> str:
        info = it.get("answer_info", {})
        for idx, key in enumerate(["ans0","ans1","ans2"]):
            if key in info:
                val = info[key]
                if isinstance(val, list) and len(val)>1 and val[1]=="unknown":
                    return chr(65+idx)
        return "C"

    def _build_sft_target(self, it: Dict, unk: str) -> str:
        if it.get("context_condition","") == "ambig":
            return f"Answer: {unk}\nJustification: The context does not provide sufficient information to determine this."
        else:
            return "Answer: [Based on context]\nJustification: The context indicates that [cite relevant phrase from context]."

class HaluEvalAdapter:
    def __init__(self):
        self.subsets = list(config.HALUEVAL_FILES.keys())
        print(f"HaluEval Adapter: {len(self.subsets)} 子集")

    def load_samples(self, n_total: int) -> List[Sample]:
        per = max(1, n_total // max(1, len(self.subsets)))
        out: List[Sample] = []
        for sub in self.subsets:
            fp = config.HALUEVAL_DIR / config.HALUEVAL_FILES[sub]
            if not fp.exists():
                print(f"⚠️ 缺文件: {fp}")
                continue
            data = read_json_flex(fp)
            if not data:
                print(f"✗ {fp.name} 解析为空")
                continue

            # 可选：对 summarization 做"最可疑"过滤
            if sub == "summarization" and config.FILTER_OUT_FRACTION > 0:
                def _bad_score(d: Dict):
                    doc = str(d.get("document") or d.get("article") or d.get("doc") or "")
                    ctrl = sum(ch < " " for ch in doc)
                    return (len(doc), ctrl)
                data = sorted(data, key=_bad_score)
                k_drop = int(len(data) * config.FILTER_OUT_FRACTION)
                if k_drop > 0:
                    data = data[:-k_drop]

            # 数据抽样阀门
            if config.DATA_FRACTION < 1.0:
                keep = max(1, int(len(data) * config.DATA_FRACTION))
                data = random.sample(data, keep)

            if len(data) > per:
                data = random.sample(data, per)

            for i, it in enumerate(data):
                prompt, target, meta = self._build_pair(it, sub)
                out.append(Sample(
                    id=f"halu_{sub}_{i}", task="hallucination", prompt=prompt, target=target, meta=meta
                ))
        print(f"HaluEval 总计: {len(out)}")
        return out

    def _pick(self, d: Dict, *keys, default=""):
        for k in keys:
            if k in d and isinstance(d[k], (str, int, float)):
                return d[k]
        return default

    def _build_pair(self, it: Dict, sub: str) -> Tuple[str,str,Dict]:
        meta = {"dataset":"HaluEval", "subset":sub}

        if sub == "qa":
            know = self._pick(it,"knowledge"); q = self._pick(it,"question")
            prompt = ("You are given a QUESTION and KNOWLEDGE.\n"
                      "Answer only if the KNOWLEDGE supports it.\n\n"
                      f"QUESTION: {q}\nKNOWLEDGE: {know}\n\n"
                      "Produce:\nAnswer: <short answer>\nEvidence: \"<quote from knowledge>\"")
            target = f"Answer: {self._pick(it,'right_answer')}\nEvidence: \"[From the provided knowledge]\""
            meta.update({"has_knowledge":True})

        elif sub == "dialogue":
            know = self._pick(it,"knowledge"); dlg = self._pick(it,"dialogue_history")
            prompt = ("You are given DIALOGUE and KNOWLEDGE.\nOnly use the KNOWLEDGE. Do not add facts not in KNOWLEDGE.\n\n"
                      f"DIALOGUE:\n{dlg}\n\nKNOWLEDGE:\n{know}\n\n"
                      "Continue the assistant's reply. Keep it concise and grounded.\n"
                      "Produce:\nAnswer: <response>\nEvidence: \"<quote from knowledge>\"")
            target = f"Answer: {self._pick(it,'right_response')}\nEvidence: \"[As stated in the knowledge]\""
            meta.update({"has_knowledge":True})

        elif sub == "summarization":
            doc = self._pick(it, "document","article","doc")
            if isinstance(doc, str) and len(doc) > config.SUMM_MAX_DOC_CHARS:
                doc = doc[:config.SUMM_MAX_DOC_CHARS] + "..."
            gold = self._pick(it, "right_summary","summary","reference_summary","gold_summary")
            hallucinated = self._pick(it, "hallucinated_summary")
            prompt = ("You are given a DOCUMENT. Write a concise summary grounded in the document.\n\n"
                      f"DOCUMENT:\n{doc}\n\nProduce:\nSummary: <2-3 sentences>\nEvidence: \"<key quotes>\"")
            target = f"Summary: {gold}\nEvidence: \"[Key supporting quotes]\""
            meta.update({"has_knowledge":True, "hallucinated_summary": hallucinated})

        else:  # general
            uq = self._pick(it,"user_query")
            prompt = (f"USER: {uq}\n\nIf you cannot ground the answer in provided context (none is provided),\n"
                      "respond cautiously and indicate need for more information.\n\nProduce:\n"
                      "Answer: <response>\nEvidence: \"insufficient\"")
            target = "Answer: I need more information to provide an accurate answer.\nEvidence: \"insufficient\""
            meta.update({"has_knowledge":False})

        return prompt, target, meta

# =============================================================================
# 评分器 —— 双云容灾（OpenAI + Claude）+ 严格 JSON + 校准 + 限流/退避 + SQLite 缓存
# =============================================================================
def extract_json_strict(txt: str) -> dict:
    """
    尽力从模型输出中抽取"唯一 JSON 对象"：
      1) 直接 json.loads
      2) ```json ... ``` 代码块
      3) 手写括号配对（忽略字符串里的花括号/转义）
    失败抛 ValueError。
    """
    # 1) 直解
    try:
        obj = json.loads(txt)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # 2) ```json ... ```
    m = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", txt, flags=re.I)
    if m:
        try:
            obj = json.loads(m.group(1))
            if isinstance(obj, dict):
                return obj
        except Exception:
            pass

    # 3) 手写括号配对
    s, n = txt, len(txt)
    i = 0
    while i < n and s[i] != '{':
        i += 1
    while i < n:
        if s[i] != '{':
            i += 1
            continue
        depth, j = 0, i
        in_str, esc = False, False
        while j < n:
            ch = s[j]
            if in_str:
                if esc: 
                    esc = False
                elif ch == '\\': 
                    esc = True
                elif ch == '"': 
                    in_str = False
            else:
                if ch == '"': 
                    in_str = True
                elif ch == '{': 
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        cand = s[i:j+1]
                        try:
                            obj = json.loads(cand)
                            if isinstance(obj, dict):
                                return obj
                        except Exception:
                            pass
                        break
            j += 1
        i += 1
    raise ValueError("No valid JSON object found")

class TokenBucket:
    def __init__(self, rate_per_sec: float, capacity: int):
        self.rate = float(rate_per_sec)
        self.capacity = int(capacity)
        self.tokens = float(capacity)
        self.lock = threading.Lock()
        self.last = time.time()
    def acquire(self):
        with self.lock:
            now = time.time()
            self.tokens = min(self.capacity, self.tokens + (now - self.last)*self.rate)
            self.last = now
            if self.tokens < 1.0:
                need = (1.0 - self.tokens) / self.rate
                time.sleep(max(need, 0.0))
                now2 = time.time()
                self.tokens = min(self.capacity, self.tokens + (now2 - self.last)*self.rate)
                self.last = now2
            self.tokens -= 1.0

GLOBAL_JUDGE_BUCKET = TokenBucket(rate_per_sec=config.RATE_LIMIT_RPS, capacity=config.RATE_LIMIT_BURST)

class MultiCloudJudge:
    """
    顺序尝试：OpenAI → Claude → 启发兜底；统一 JSON 抽取；统一校准口径。
    完全移除 Gemini 依赖。
    线程安全：单实例 + SQLite(check_same_thread=False) + Lock；每次调用前 GLOBAL_JUDGE_BUCKET.acquire()。
    """
    def __init__(self):
        self._setup_cache()
        self.providers = config.JUDGE_PROVIDERS
        # 验证不包含 gemini
        for p in self.providers:
            if p["name"].lower() == "gemini":
                raise ValueError("Gemini provider is not supported in this version")

    # --- 缓存表 ---
    def _setup_cache(self):
        self.cache_path = config.CACHE_DIR / "judge_cache.db"
        self.conn = sqlite3.connect(str(self.cache_path), check_same_thread=False)
        self.lock = threading.Lock()
        with self.lock:
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS evaluations(
                    cache_key TEXT PRIMARY KEY,
                    scores   TEXT,
                    ts       REAL
                )
            """)
            self.conn.commit()

    def _cache_get(self, key: str):
        with self.lock:
            cur = self.conn.execute("SELECT scores FROM evaluations WHERE cache_key=?", (key,))
            row = cur.fetchone()
        return json.loads(row[0]) if row else None

    def _cache_put(self, key: str, scores: Dict):
        with self.lock:
            self.conn.execute(
                "INSERT OR REPLACE INTO evaluations VALUES (?,?,?)",
                (key, json.dumps(scores), time.time())
            )
            self.conn.commit()

    # --- 校准 ---
    def _calibrate(self, provider_name: str, s_raw: float) -> float:
        m = config.JUDGE_CALIBRATION.get(provider_name, {"a":1.0, "b":0.0})
        s = float(m["a"])*float(s_raw) + float(m["b"])
        return max(0.0, min(1.0, s))

    # --- OpenAI 调用（统一接口）---
    def _call_openai(self, prompt: str, timeout: float) -> float:
        from openai import OpenAI
        if not os.environ.get("OPENAI_API_KEY", ""):
            raise RuntimeError("No OPENAI_API_KEY")
        client = OpenAI()
        model_name = None
        for p in self.providers:
            if p["name"] == "openai":
                model_name = p.get("model") or "gpt-4o-mini"
                break
        resp = client.chat.completions.create(
            model=model_name or "gpt-4o-mini",
            temperature=0,
            response_format={"type": "json_object"},
            messages=[{"role": "user", "content": prompt}],
            timeout=timeout
        )
        txt = resp.choices[0].message.content
        obj = extract_json_strict(txt)
        return float(obj.get("final"))

    # --- Claude 调用（统一接口）---
    def _call_claude(self, prompt: str, timeout: float) -> float:
        import anthropic, inspect
        if not os.environ.get("ANTHROPIC_API_KEY", ""):
            raise RuntimeError("No ANTHROPIC_API_KEY")
        client = anthropic.Anthropic()
        model_name = None
        for p in self.providers:
            if p["name"] == "claude":
                model_name = p.get("model") or "claude-3-5-haiku-latest"
                break
        # 0.69 使用 max_output_tokens；更老版本是 max_tokens
        sig = inspect.signature(client.messages.create)
        length_kw = "max_output_tokens" if "max_output_tokens" in sig.parameters else "max_tokens"
        resp = client.messages.create(
            model=model_name or "claude-3-5-haiku-latest",
            temperature=0,
            messages=[{"role": "user", "content": prompt}],
            **{length_kw: 64}
        )
        parts = []
        for blk in getattr(resp, "content", []) or []:
            if hasattr(blk, "text"):
                parts.append(blk.text)
            elif isinstance(blk, dict) and blk.get("type") == "text":
                parts.append(blk.get("text", ""))
        txt = "".join(parts) if parts else str(resp)
        obj = extract_json_strict(txt)
        return float(obj.get("final"))

    # --- 统一入口（完全移除 Gemini 逻辑）---
    def evaluate(self, sample: Sample, response: str) -> Dict[str, float]:
        # 统一的评分提示词（确保 OpenAI 和 Claude 使用完全相同的提示）
        rubric = 'You are a strict grader. Evaluate the response quality on a scale of 0.0 to 1.0. Return ONLY a JSON object like {"final": 0.0..1.0}.'
        prompt = f"{rubric}\nTASK: {sample.task}\nPROMPT:\n{sample.prompt}\n\nRESPONSE:\n{response}\n"

        key = hashlib.sha256(f"{sample.task}::{sample.prompt}::{response}".encode()).hexdigest()
        cached = self._cache_get(key)
        if cached:
            return cached

        GLOBAL_JUDGE_BUCKET.acquire()  # 全局限流

        # 按优先顺序尝试双云（OpenAI → Claude）；每家支持重试与退避
        for p in self.providers:
            provider_name = p["name"]
            for attempt in range(config.JUDGE_MAX_RETRIES + 1):
                try:
                    if provider_name == "openai":
                        s_raw = self._call_openai(prompt, config.JUDGE_TIMEOUT_SEC)
                    elif provider_name == "claude":
                        s_raw = self._call_claude(prompt, config.JUDGE_TIMEOUT_SEC)
                    else:
                        # 不应该到这里，因为已经验证过 providers
                        raise ValueError(f"Unknown provider: {provider_name}")
                    
                    s_cal = self._calibrate(provider_name, s_raw)
                    out = {"final": float(s_cal), "provider": provider_name}
                    self._cache_put(key, out)
                    return out
                except Exception as e:
                    # 429/配额类：尝试解析 retry_delay seconds
                    msg = str(e)
                    m = re.search(r"retry(?:_delay)?\s*{?\s*seconds:\s*([0-9]+)", msg)
                    if m:
                        time.sleep(int(m.group(1)))
                    else:
                        time.sleep(1.5 * (attempt + 1))
            # 当前 provider 放弃 → 换下一个

        # 全部失败 → 启发兜底（非插 0.5）
        score = 0.5
        txt = response.lower()
        score += 0.1 if "evidence:" in txt or '"' in response else -0.1
        score += 0.1 if "insufficient" in txt or "unknown" in txt else 0.0
        score = float(min(1.0, max(0.0, score)))
        out = {"final": score, "provider": "heuristic"}
        self._cache_put(key, out)
        return out

# =============================================================================
# 固定快评样本 + 快评快道（加速优化）
# =============================================================================
def _quickeval_path(task: str) -> Path:
    # 【修改】快评样本集放在 workspace 根目录，不随 run_id 变化
    return config.WORKSPACE / f"quickeval_set_{task}.json"

def get_quickeval_pool(task: str, dataset, n: int):
    """固定快评样本集，消除抽样噪声"""
    pool = dataset.fairness_samples if task == "fairness" else dataset.hallucination_samples
    id2s = {s.id: s for s in pool}
    p = _quickeval_path(task)
    if p.exists():
        try:
            ids = json.loads(p.read_text(encoding="utf-8"))
            got = [id2s[i] for i in ids if i in id2s]
            if got: return got
        except Exception:
            pass
    samp = random.sample(pool, min(n, len(pool)))
    p.write_text(json.dumps([s.id for s in samp], ensure_ascii=False, indent=2), encoding="utf-8")
    return samp

def quick_eval_fast(model, tokenizer, device, judge, dataset, task: str, n_samples: int,
                   provider_hint: str = "openai", use_sampling: bool = False):
    """
    快评快道：绕过多云重试，直接使用单一 provider（OpenAI 或 Claude）
    【优化】并行化 API 调用，添加进度提示
    """
    pool = dataset.fairness_samples if task=="fairness" else dataset.hallucination_samples
    if not pool: return 0.0
    smp = get_quickeval_pool(task, dataset, n=n_samples)

    # 统一的评分提示词（与正式评审保持一致）
    rubric = 'You are a strict grader. Evaluate the response quality on a scale of 0.0 to 1.0. Return ONLY a JSON object like {"final": 0.0..1.0}.'

    # 步骤1：批量生成响应（最慢的部分）
    print(f"  [QuickEval] 生成 {len(smp)} 个 {task} 响应...", end="", flush=True)
    responses = []
    for s in smp:
        resp = generate_one_response(model, tokenizer, device, s.prompt, use_sampling=use_sampling)
        responses.append(resp)
    print(" 完成", flush=True)

    # 步骤2：并行调用 Judge API
    print(f"  [QuickEval] 评测 {len(smp)} 个响应...", end="", flush=True)

    def judge_one(idx):
        s = smp[idx]
        resp = responses[idx]
        prompt = f"{rubric}\nTASK: {s.task}\nPROMPT:\n{s.prompt}\n\nRESPONSE:\n{resp}\n"
        try:
            if provider_hint == "openai":
                from openai import OpenAI
                if not os.environ.get("OPENAI_API_KEY"):
                    raise RuntimeError("No OPENAI_API_KEY")
                cli = OpenAI()
                r = cli.chat.completions.create(
                    model="gpt-4o-mini",
                    temperature=0,
                    response_format={"type":"json_object"},
                    messages=[{"role":"user","content": prompt}],
                    timeout=10
                )
                obj = extract_json_strict(r.choices[0].message.content)
                return float(obj.get("final", 0.5))
            elif provider_hint == "claude":
                import anthropic, inspect
                if not os.environ.get("ANTHROPIC_API_KEY"):
                    raise RuntimeError("No ANTHROPIC_API_KEY")
                cli = anthropic.Anthropic()
                sig = inspect.signature(cli.messages.create)
                length_kw = "max_output_tokens" if "max_output_tokens" in sig.parameters else "max_tokens"
                r = cli.messages.create(
                    model="claude-3-5-haiku-latest",
                    temperature=0,
                    messages=[{"role":"user","content": prompt}],
                    **{length_kw: 64},
                    timeout=10
                )
                parts = []
                for blk in getattr(r, "content", []) or []:
                    if hasattr(blk, "text"): parts.append(blk.text)
                    elif isinstance(blk, dict) and blk.get("type")=="text": parts.append(blk.get("text",""))
                obj = extract_json_strict("".join(parts) if parts else str(r))
                return float(obj.get("final", 0.5))
            else:
                # 启发兜底
                txt = resp.lower()
                sc = 0.5 + (0.1 if "evidence:" in txt or '"' in resp else -0.1) + (0.1 if "insufficient" in txt or "unknown" in txt else 0.0)
                return float(min(1.0, max(0.0, sc)))
        except Exception:
            return 0.5  # 快评失败给中性分

    # 并行调用（使用线程池，因为是 I/O 密集型）
    from concurrent.futures import ThreadPoolExecutor, as_completed
    scores = []
    with ThreadPoolExecutor(max_workers=8) as executor:  # 8 个并发 API 调用
        futures = {executor.submit(judge_one, i): i for i in range(len(smp))}
        for future in as_completed(futures):
            scores.append(future.result())

    print(" 完成", flush=True)
    return float(np.mean(scores)) if scores else 0.0

# =============================================================================
# Pareto
# =============================================================================
@dataclass
class ParetoPoint:
    step: int
    fairness_score: float
    hallucination_score: float
    checkpoint_path: Optional[str] = None
    def dominates(self, other: "ParetoPoint") -> bool:
        better = (self.fairness_score > other.fairness_score) or (self.hallucination_score > other.hallucination_score)
        not_worse = (self.fairness_score >= other.fairness_score) and (self.hallucination_score >= other.hallucination_score)
        return better and not_worse

class ParetoFrontier:
    def __init__(self, max_checkpoints: int = 5):
        self.points: List[ParetoPoint] = []
        self.max_checkpoints = max_checkpoints
    def add_point(self, step: int, fairness: float, halu: float, ckpt: Optional[str]):
        p = ParetoPoint(step, fairness, halu, ckpt)
        self.points = [x for x in self.points if not p.dominates(x)]
        if not any(x.dominates(p) for x in self.points):
            self.points.append(p)
        if len(self.points) > self.max_checkpoints:
            self.points.sort(key=lambda x: x.fairness_score + x.hallucination_score, reverse=True)
            self.points = self.points[:self.max_checkpoints]
    def get_best(self) -> Optional[ParetoPoint]:
        return max(self.points, key=lambda x: x.fairness_score + x.hallucination_score) if self.points else None
    def save_frontier(self, path: Path):
        with open(path/"pareto_frontier.json","w") as f:
            json.dump([{"step":p.step,"fairness":p.fairness_score,"hallucination":p.hallucination_score,"ckpt":p.checkpoint_path} for p in self.points], f, indent=2)

# =============================================================================
# 采样稳定化 + KV 缓存 + 临时关闭 checkpointing
# =============================================================================
from transformers import LogitsProcessorList, TemperatureLogitsWarper, TopKLogitsWarper, TopPLogitsWarper

class SanityLogitsProcessor(torch.nn.Module):
    def __init__(self, min_tokens_to_keep=1):
        super().__init__()
        self.min_tokens_to_keep=min_tokens_to_keep
    def forward(self, input_ids, scores):
        scores = scores.nan_to_num(neginf=-1e4, posinf=1e4)
        scores = scores - scores.max(dim=-1, keepdim=True).values
        scores = scores.clamp(-50, 50)
        all_neg_inf = torch.isneginf(scores).all(dim=-1, keepdim=True)
        if all_neg_inf.any():
            argmax = scores.argmax(dim=-1, keepdim=True)
            scores.scatter_(1, argmax, 0.0)
        return scores

class PresencePenaltyProcessor(torch.nn.Module):
    def __init__(self, penalty=0.0):
        super().__init__()
        self.penalty=float(penalty)
    def forward(self, input_ids, scores):
        if self.penalty==0.0: return scores
        for b in range(scores.size(0)):
            seen = torch.unique(input_ids[b])
            scores[b, seen] -= self.penalty
        return scores

class FrequencyPenaltyProcessor(torch.nn.Module):
    def __init__(self, penalty=0.0):
        super().__init__()
        self.penalty=float(penalty)
    def forward(self, input_ids, scores):
        if self.penalty==0.0: return scores
        for b in range(scores.size(0)):
            uniq, cnt = torch.unique(input_ids[b], return_counts=True)
            scores[b, uniq] -= self.penalty * cnt.to(scores.dtype)
        return scores

def build_safe_logits_processors():
    """
    构建logits处理器列表
    【修复】只添加自定义 processor（Penalty + Sanity）
    Temperature/TopK/TopP 直接传给 generate()，避免警告
    """
    lp = LogitsProcessorList()

    # 只添加自定义的penalty处理器
    if config.PRESENCE_PENALTY != 0.0:
        lp.append(PresencePenaltyProcessor(config.PRESENCE_PENALTY))
    if config.FREQUENCY_PENALTY != 0.0:
        lp.append(FrequencyPenaltyProcessor(config.FREQUENCY_PENALTY))

    # 最后添加安全处理器
    lp.append(SanityLogitsProcessor(2))
    return lp

@contextlib.contextmanager
def temporary_use_cache(model, use_cache: bool=True):
    old = getattr(model.config, "use_cache", False)
    model.config.use_cache = use_cache
    try:
        yield
    finally:
        model.config.use_cache = old

@contextlib.contextmanager
def temporary_no_checkpointing(model):
    """生成前临时关闭 gradient checkpointing，生成后恢复；从而启用 KV cache"""
    was_on = getattr(model, "is_gradient_checkpointing", False)
    try:
        if was_on and hasattr(model, "gradient_checkpointing_disable"):
            model.gradient_checkpointing_disable()
        yield
    finally:
        if was_on and hasattr(model, "gradient_checkpointing_enable"):
            model.gradient_checkpointing_enable()
            if hasattr(model, "enable_input_require_grads"):
                model.enable_input_require_grads()

# 训练用：批量生成（一次生成 B×K）
def generate_candidates_batch(model, tokenizer, device, prompts: List[str], k: int, max_new_tokens: int = None) -> Tuple[List[List[str]], List[List[int]], List[int], List[List[bool]]]:
    """
    批量生成，返回文本、长度和每个prompt的实际token长度
    §1&§2修复: 应用聊天模板 + 多终止符
    """
    if max_new_tokens is None:
        max_new_tokens = config.MAX_NEW_TOKENS_TRAIN

    # §1: 对所有prompts应用聊天模板
    system_msg = "You are a helpful, accurate, and unbiased assistant."
    formatted_prompts = [apply_chat_template(tokenizer, p, system_msg) for p in prompts]

    # §2: 获取多终止符
    eos_ids = get_eos_token_ids(tokenizer)

    processors = build_safe_logits_processors()  # 【修正】移除参数
    batch_prompts = []
    for p in formatted_prompts:  # 使用格式化后的prompts
        batch_prompts.extend([p]*k)
    inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True,
                       truncation=True, max_length=config.SFT_MAXLEN).to(device)

    # 【最终修复】采样参数直接传递给 generate()，避免警告
    with torch.no_grad(), temporary_no_checkpointing(model), temporary_use_cache(model, True):
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            min_new_tokens=config.MIN_NEW_TOKENS_TRAIN,
            do_sample=True,
            temperature=config.TEMPERATURE_TRAIN,
            top_k=config.TOP_K_TRAIN,
            top_p=config.TOP_P_TRAIN,
            repetition_penalty=config.REP_PENALTY_TRAIN,
            logits_processor=processors,  # 只包含 Penalty + Sanity
            num_return_sequences=1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=eos_ids,  # §2: 多终止符
            use_cache=True,
            return_dict_in_generate=False,
        )
    # §3: 拆回每个 prompt 的 k 条，并准确检测截断
    src_lens = (inputs["input_ids"] != tokenizer.pad_token_id).sum(dim=1)
    texts, lengths, prompt_lens, truncated_flags = [], [], [], []
    for i in range(out.shape[0]):
        response_tokens = out[i, src_lens[i]:]
        decoded = tokenizer.decode(response_tokens, skip_special_tokens=True)
        texts.append(decoded)

        # §3修复: 正确计算长度和检测截断
        # 找到第一个EOS/EOT的位置（如果有的话）
        eos_position = None
        for pos, token_id in enumerate(response_tokens):
            if int(token_id.item()) in eos_ids:
                eos_position = pos
                break

        # 如果找到了EOS，实际长度就是到EOS的位置+1
        # 否则就是整个序列的长度（去除padding）
        if eos_position is not None:
            actual_len = eos_position + 1
            hit_eos = True
        else:
            # 没有EOS，计算非padding的token数量
            actual_len = int((response_tokens != tokenizer.pad_token_id).sum())
            hit_eos = False

        # 确保长度不超过max_new_tokens
        actual_len = min(actual_len, max_new_tokens)
        lengths.append(actual_len)
        prompt_lens.append(int(src_lens[i].item()))

        # 截断定义：达到max_new_tokens且没有命中EOS/EOT
        is_truncated = (actual_len >= max_new_tokens) and not hit_eos
        truncated_flags.append(is_truncated)

    grouped_texts, grouped_lengths, grouped_truncated = [], [], []
    for i in range(0, len(texts), k):
        grouped_texts.append(texts[i:i+k])
        grouped_lengths.append(lengths[i:i+k])
        grouped_truncated.append(truncated_flags[i:i+k])

    # 返回每个原始prompt的长度（去重）
    unique_prompt_lens = [prompt_lens[i] for i in range(0, len(prompt_lens), k)]

    return grouped_texts, grouped_lengths, unique_prompt_lens, grouped_truncated

# 评估用：支持贪心和采样两种模式
def generate_one_response(model, tokenizer, device, prompt: str, use_sampling: bool = False) -> str:
    """
    统一的生成函数，支持贪心和采样
    §1&§2修复: 应用聊天模板 + 多终止符
    """
    # §1: 应用聊天模板
    system_msg = "You are a helpful, accurate, and unbiased assistant."
    formatted_prompt = apply_chat_template(tokenizer, prompt, system_msg)

    # §2: 获取多终止符
    eos_ids = get_eos_token_ids(tokenizer)

    inputs = tokenizer([formatted_prompt], return_tensors="pt", padding=True, truncation=True, max_length=config.SFT_MAXLEN).to(device)

    with torch.no_grad(), temporary_no_checkpointing(model), temporary_use_cache(model, True):
        if use_sampling:
            # 采样模式：使用与训练相同的配置
            processors = build_safe_logits_processors()
            out = model.generate(
                **inputs,
                max_new_tokens=config.MAX_NEW_TOKENS_EVAL,
                min_new_tokens=config.MIN_NEW_TOKENS_TRAIN,
                do_sample=True,
                temperature=config.TEMPERATURE_TRAIN,
                top_k=config.TOP_K_TRAIN,
                top_p=config.TOP_P_TRAIN,
                repetition_penalty=config.REP_PENALTY_TRAIN,
                logits_processor=processors,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=eos_ids,  # §2: 多终止符
                use_cache=True,
            )
        else:
            # 贪心模式：不用processor
            out = model.generate(
                **inputs,
                max_new_tokens=config.MAX_NEW_TOKENS_EVAL,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=eos_ids,  # §2: 多终止符
                use_cache=True,
            )

    src_len = inputs["input_ids"].shape[1]
    return tokenizer.decode(out[0, src_len:], skip_special_tokens=True)

# 向后兼容的贪心版本
def generate_one_greedy(model, tokenizer, device, prompt: str) -> str:
    return generate_one_response(model, tokenizer, device, prompt, use_sampling=False)

# =============================================================================
# §1 & §2: 聊天模板与多终止符支持（P0 - 修复截断率100%）
# =============================================================================
def apply_chat_template(tokenizer, prompt: str, system_message: str = None) -> str:
    """
    §1: 为LLaMA-3-Instruct应用正确的聊天模板
    避免手拼字符串导致模型不知道何时停止
    """
    messages = []
    if system_message:
        messages.append({"role": "system", "content": system_message})
    messages.append({"role": "user", "content": prompt})

    # 使用tokenizer的聊天模板
    try:
        formatted = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        return formatted
    except Exception as e:
        # 兜底：如果tokenizer不支持chat_template，返回原始prompt
        print(f"⚠️ 聊天模板应用失败: {e}，使用原始prompt")
        return prompt

def get_eos_token_ids(tokenizer) -> List[int]:
    """
    §2: 获取所有终止符ID（包括EOS和EOT）
    LLaMA-3-Instruct需要 [128001(EOS), 128009(EOT)]
    """
    eos_ids = []
    vocab = tokenizer.get_vocab()

    # 1. 查找 <|end_of_text|> (标准EOS，通常是128001)
    if '<|end_of_text|>' in vocab:
        end_of_text_id = tokenizer.convert_tokens_to_ids('<|end_of_text|>')
        eos_ids.append(end_of_text_id)
    elif tokenizer.eos_token_id is not None:
        # 如果没有找到 <|end_of_text|>，用默认的eos_token_id
        eos_ids.append(tokenizer.eos_token_id)

    # 2. 查找 <|eot_id|> (EOT，LLaMA-3特有，通常是128009)
    if '<|eot_id|>' in vocab:
        eot_id = tokenizer.convert_tokens_to_ids('<|eot_id|>')
        eos_ids.append(eot_id)
    elif hasattr(tokenizer, 'eot_token_id') and tokenizer.eot_token_id is not None:
        eos_ids.append(tokenizer.eot_token_id)

    # 3. 去重
    eos_ids = list(set(eos_ids))

    # 4. 打印检测结果（只打印一次）
    if not hasattr(get_eos_token_ids, '_printed'):
        get_eos_token_ids._printed = True
        if len(eos_ids) > 1:
            print(f"✅ 多终止符已启用: {eos_ids}")
            # 打印token名称帮助调试
            token_names = []
            for tid in eos_ids:
                token_str = tokenizer.decode([tid])
                token_names.append(f"{tid}({token_str})")
            print(f"   终止符详情: {token_names}")
        elif len(eos_ids) == 1:
            print(f"⚠️ 仅检测到单个终止符: {eos_ids}")
            token_str = tokenizer.decode([eos_ids[0]])
            print(f"   Token详情: {eos_ids[0]}({token_str})")
            print(f"   这可能导致截断率偏高，建议检查tokenizer配置")
        else:
            print(f"❌ 未检测到任何终止符，使用默认值2")
            eos_ids = [2]

    return eos_ids

# =============================================================================
# 模型加载（dtorch：用 dtype，不用 torch_dtype）
# =============================================================================
def load_model_and_tokenizer():
    """
    🔥🔥🔥 版本检查点 #1 - 如果你能看到这个，说明用的是最新代码！🔥🔥🔥
    """
    print("\n" + "="*80)
    print("加载模型")
    print("="*80)
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
    from peft import LoraConfig, get_peft_model, TaskType
    import transformers
    print(f"Transformers: {transformers.__version__}")

    extra = {}
    if config.HF_TOKEN:
        extra["token"] = config.HF_TOKEN

    _ = AutoConfig.from_pretrained(config.BASE_MODEL, trust_remote_code=True, **extra)
    tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL, trust_remote_code=True, **extra)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    dtype = torch.bfloat16 if (config.USE_BF16 and torch.cuda.is_available()) else torch.float16

    # 【加速优化】启用 Flash Attention 2（如果可用）
    attn_kwargs = {}
    try:
        import flash_attn
        attn_kwargs["attn_implementation"] = "flash_attention_2"
        print("✅ Flash Attention 2 可用，已启用")
    except ImportError:
        print("⚠️ Flash Attention 2 不可用，使用默认实现")

    model = AutoModelForCausalLM.from_pretrained(config.BASE_MODEL, trust_remote_code=True, dtype=dtype, **extra, **attn_kwargs)
    base_model = AutoModelForCausalLM.from_pretrained(config.BASE_MODEL, trust_remote_code=True, dtype=dtype, **extra, **attn_kwargs)

    if config.USE_LORA:
        lcfg = LoraConfig(task_type=TaskType.CAUSAL_LM, r=config.LORA_R, lora_alpha=config.LORA_ALPHA,
                          lora_dropout=config.LORA_DROPOUT, target_modules=config.TARGET_MODULES, bias="none")
        model = get_peft_model(model, lcfg)
        model.print_trainable_parameters()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    base_model.to(device)

    if config.USE_GRADIENT_CHECKPOINTING:
        model.gradient_checkpointing_enable()
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:
            emb = model.get_input_embeddings()
            def _make_inputs_require_grad(mod, inp, out):
                out.requires_grad_(True)
            emb.register_forward_hook(_make_inputs_require_grad)

    # 参考模型仅推理：不需要 checkpointing
    base_model.config.use_cache = False
    model.config.use_cache = False

    # 【加速优化】torch.compile() 加速（可选）
    if config.USE_TORCH_COMPILE:
        try:
            if hasattr(torch, 'compile'):
                print(f"🚀 启用 torch.compile() 加速（mode={config.COMPILE_MODE}）...")
                model = torch.compile(model, mode=config.COMPILE_MODE)
                base_model = torch.compile(base_model, mode=config.COMPILE_MODE)
                print("✅ torch.compile() 已启用（首次运行会编译，稍慢）")
            else:
                print("⚠️ PyTorch 版本过低，不支持 torch.compile()（需要 ≥2.0）")
        except Exception as e:
            print(f"⚠️ torch.compile() 启用失败: {e}")

    print("✅ 模型加载成功!")
    return model, base_model, tokenizer, device

# =============================================================================
# SFT：仅对 completion 计 loss
# =============================================================================
def tokenize_sft_pair(tokenizer, prompt: str, target: str, device):
    sep = "\n\n"
    prompt_ids = tokenizer(prompt + sep, return_tensors="pt", truncation=True, max_length=config.SFT_MAXLEN)
    full_ids   = tokenizer(prompt + sep + target, return_tensors="pt", truncation=True, max_length=config.SFT_MAXLEN)
    input_ids = full_ids["input_ids"]
    attn_mask = full_ids.get("attention_mask")
    labels = input_ids.clone()
    prompt_len = prompt_ids["input_ids"].shape[1]
    labels[:, :prompt_len] = -100
    batch = {"input_ids": input_ids.to(device), "labels": labels.to(device)}
    if attn_mask is not None:
        batch["attention_mask"] = attn_mask.to(device)
    return batch

# =============================================================================
# 拼接分词 / 组内优势 / CAGrad
# =============================================================================
def _tokenize_concat(tokenizer, prompts: List[str], responses: List[str], response_lens: List[int], device):
    """
    拼接分词，返回完整token序列和completion mask
    【修正】直接使用generate时返回的response实际token长度
    """
    # Tokenize完整序列（带padding）
    full = tokenizer([p + r for p, r in zip(prompts, responses)],
                     return_tensors="pt", padding=True, truncation=True, max_length=config.SFT_MAXLEN)
    full = {k: v.to(device) for k, v in full.items()}
    
    ids = full["input_ids"]
    attn = full["attention_mask"]
    B, T = ids.shape
    
    # 构建completion mask（对应logits维度：T-1）
    # logits[:, i] 预测 ids[:, i+1]
    comp_mask = torch.zeros(B, T-1, device=device, dtype=torch.float32)
    
    for i in range(B):
        # 完整序列的有效长度（不含padding）
        valid_len = int(attn[i].sum().item())
        
        # response的实际token长度（从generate时传入）
        resp_len = response_lens[i]
        
        # response在ids序列中的起始位置（从有效末尾向前数）
        # 注意：valid_len是ids的有效长度，response_len也是ids的长度
        resp_start_in_ids = max(0, valid_len - resp_len)
        
        # 在logits中，预测response第一个token的位置
        # logits[j] 预测 ids[j+1]
        # 如果response从ids[resp_start_in_ids]开始
        # 那么logits[resp_start_in_ids-1]预测ids[resp_start_in_ids]
        comp_start_in_logits = max(0, resp_start_in_ids - 1)
        
        # logits的有效末尾位置
        comp_end_in_logits = valid_len - 1
        
        # 设置mask
        if comp_start_in_logits < comp_end_in_logits:
            comp_mask[i, comp_start_in_logits:comp_end_in_logits] = 1.0
    
    return full, comp_mask

def compute_group_advantages(rewards: torch.Tensor, k: int) -> torch.Tensor:
    Bk = rewards.numel()
    assert Bk % k == 0
    B = Bk // k
    r = rewards.view(B, k)
    mean = r.mean(dim=1, keepdim=True)
    std = r.std(dim=1, keepdim=True).clamp_min(1e-6)
    adv = ((r - mean) / std).view(-1).clamp(-config.ADV_CLIP, config.ADV_CLIP)
    return adv

def _set_grads_from_vec(params: List[torch.nn.Parameter], vec: torch.Tensor, accumulate: bool = True):
    """
    设置/累加梯度向量到参数

    【修复梯度累积】支持梯度累积和性能优化

    Args:
        params: 参数列表
        vec: 梯度向量
        accumulate: 如果 True，累加梯度（后续 micro-batch）
                   如果 False，覆盖梯度（第一个 micro-batch，性能更好）
    """
    ptr = 0
    for p in params:
        num = p.numel()
        g = vec[ptr:ptr+num].view_as(p)
        if p.grad is None:
            p.grad = g.clone()
        elif accumulate:
            p.grad.add_(g)  # 累加（梯度累积）
        else:
            p.grad.copy_(g)  # 覆盖（第一个 micro-batch，更快）
        ptr += num

def cagrad_combine_and_set_grads(params: List[torch.nn.Parameter], g_fair_vec: torch.Tensor, g_halu_vec: torch.Tensor, c: float=0.2, accumulate: bool=True):
    """CAGrad 梯度合成算法

    Args:
        accumulate: 传递给 _set_grads_from_vec，控制累加还是覆盖
    """
    eps = 1e-12
    g0 = 0.5 * (g_fair_vec + g_halu_vec)
    phi = (c**2) * (g0.norm().pow(2) + eps)
    def F(w: float):
        gw = w*g_fair_vec + (1-w)*g_halu_vec
        return torch.dot(gw, g0) + torch.sqrt(phi) * (gw.norm() + eps)
    wl, wr = 0.0, 1.0
    for _ in range(20):
        w1 = wl + (wr-wl)/3.0
        w2 = wr - (wr-wl)/3.0
        if F(w1) < F(w2):
            wr = w2
        else:
            wl = w1
    w_star = 0.5*(wl+wr)
    gw = w_star*g_fair_vec + (1-w_star)*g_halu_vec
    d = g0 + (torch.sqrt(phi) / (gw.norm() + eps)) * gw
    _set_grads_from_vec(params, d, accumulate=accumulate)

# =============================================================================
# SFT
# =============================================================================
def sft_continue(model, tokenizer, device, dataset):
    print("\n" + "="*80)
    print("阶段1: SFT-CONTINUE")
    print("="*80)
    if model is None: 
        return
    params = [p for p in model.parameters() if p.requires_grad]
    # 【性能优化】使用Fused AdamW加速（5-10%提速，需要CUDA）
    opt = AdamW(params, lr=config.SFT_LR, fused=torch.cuda.is_available())
    try:
        from tqdm.auto import tqdm
    except:
        tqdm = lambda x, **kw: x
    progress = tqdm(range(config.SFT_STEPS), desc="SFT训练")
    model.train()
    model.config.use_cache = False
    nan_hits = 0
    for step in progress:
        batch = dataset.get_balanced_batch(config.SFT_BATCH_SIZE)
        opt.zero_grad(set_to_none=True)
        losses=[]
        for s in batch:
            tgt = s.target or "Answer: [Based on context]\nJustification: [cite]"
            pack = tokenize_sft_pair(tokenizer, s.prompt, tgt, device)
            out = model(**pack)
            loss = out.loss
            if not loss.requires_grad:
                raise RuntimeError("SFT loss has no grad_fn. Check gradient checkpointing & input grads.")
            if torch.isnan(loss) or torch.isinf(loss):
                nan_hits += 1
                continue
            loss.backward()
            losses.append(float(loss.detach().cpu()))
        if losses:
            torch.nn.utils.clip_grad_norm_(params, max_norm=0.5)
            opt.step()
            progress.set_postfix({"loss": f"{np.mean(losses):.4f}"})
        else:
            progress.set_postfix({"loss": "skipped"})
        if nan_hits > 5:
            print("⚠️ 多次 NaN，终止 SFT")
            break
    print("✅ SFT 完成")

# =============================================================================
# 数据容器
# =============================================================================
class MultiObjectiveDataset(torch.utils.data.Dataset):
    def __init__(self, fairness_samples: List[Sample], hallucination_samples: List[Sample]):
        self.fairness_samples = fairness_samples
        self.hallucination_samples = hallucination_samples
        self.all_samples = fairness_samples + hallucination_samples
    def __len__(self): 
        return len(self.all_samples)
    def __getitem__(self, idx): 
        return self.all_samples[idx]
    def get_balanced_batch(self, batch_size: int) -> List[Sample]:
        nf = batch_size // 2
        nh = batch_size - nf
        f = random.sample(self.fairness_samples, min(max(1,nf), len(self.fairness_samples)))
        h = random.sample(self.hallucination_samples, min(max(1,nh), len(self.hallucination_samples)))
        out = f + h
        random.shuffle(out)
        return out

# =============================================================================
# GRPO（含分段计时 + 批量生成 + ref_lp 复用 + provider 统计 + 完整指标记录）
# =============================================================================
def grpo_train(model, base_model, tokenizer, device, dataset, judge, pareto):
    """
    🔥🔥🔥 版本检查点 #2 - 如果你能看到这个，说明用的是最新代码！🔥🔥🔥
    """
    print("\n" + "="*80)
    print("阶段2: GRPO 多目标训练（v2.3 - 显存优化版）")
    print("="*80)
    print("🔥🔥🔥 代码版本: 2025-10-24 最新版（包含所有 bug 修复）🔥🔥🔥")

    # 【显存优化】打印显存配置
    if torch.cuda.is_available():
        print(f"\n显存优化配置:")
        print(f"  GRPO_BATCH_SIZE: {config.GRPO_BATCH_SIZE} (单次生成)")
        print(f"  GRADIENT_ACCUMULATION_STEPS: {config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"  有效 batch size: {config.GRPO_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
        print(f"  K_ROLLOUTS: {config.K_ROLLOUTS}")
        print(f"  单步生成总数: {config.GRPO_BATCH_SIZE * config.K_ROLLOUTS}")
        print(f"  MAX_NEW_TOKENS: {config.MAX_NEW_TOKENS_TRAIN}")
        print(f"  LORA_R: {config.LORA_R}")
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  GPU 总显存: {gpu_mem:.1f} GB")
        print()
    
    # 打印生成配置
    GenerationConfigManager.print_config(mode="train")
    
    if model is None: 
        return
    
    # 初始化指标记录器
    metrics_logger = TrainingMetrics(config.OUTPUT_DIR)
    
    # 【新增】初始化奖励标准化器
    reward_normalizer = RewardNormalizer(decay=config.REWARD_EMA_DECAY, 
                                        winsorize_quantile=config.REWARD_WINSORIZE_QUANTILE)
    
    # §7: 初始化分支化KL控制器（拒绝老师建议，恢复原设计）
    # 【性能修复】降低初始β，配合放宽的KL目标，给模型更多学习空间
    kl_controller = BranchedKLController(
        beta_f_init=0.05,  # 降低到0.05，配合新KL目标[0.08-0.15]
        beta_h_init=0.15,  # 降低到0.15，配合新KL目标[0.04-0.10]
        window_size=config.KL_ADAPTIVE_WINDOW
    )
    
    # 【新增】初始化梯度冲突监控器
    conflict_monitor = GradientConflictMonitor() if config.GRADIENT_CONFLICT_MONITOR else None
    
    # 【新增】动态调整max_new_tokens的变量（初始即为硬约束上限）
    current_max_new_tokens_train = config.MAX_NEW_TOKENS_TRAIN  # 128（硬约束）
    
    trainable = [p for p in model.parameters() if p.requires_grad]
    # 【性能优化】使用Fused AdamW加速（5-10%提速，需要CUDA）
    opt = AdamW(trainable, lr=config.GRPO_LR, weight_decay=0.01, fused=torch.cuda.is_available())
    try:
        from tqdm.auto import tqdm
    except:
        tqdm = lambda x, **kw: x
    progress = tqdm(range(config.GRPO_STEPS), desc="GRPO训练")

    # 【显存优化】梯度累积计数器
    accumulation_counter = 0

    for step in progress:
        import time as _t
        t0 = _t.time()

        # 采样一个混合 batch
        batch = dataset.get_balanced_batch(config.GRPO_BATCH_SIZE)
        tasks = [s.task for s in batch]

        # ——生成（批量）——
        t_gen0 = _t.time()
        cand_by_sample, lengths_by_sample, _, truncated_by_sample = generate_candidates_batch(
            model, tokenizer, device, [s.prompt for s in batch], config.K_ROLLOUTS,
            max_new_tokens=current_max_new_tokens_train  # 【修正】传入动态调整的max_new_tokens
        )

        # 【显存优化】生成后立即清理显存
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        # flatten
        all_prompts, all_resps, all_lengths, all_truncated, idx_map = [], [], [], [], []
        for i, s in enumerate(batch):
            all_prompts += [s.prompt]*config.K_ROLLOUTS
            all_resps   += cand_by_sample[i]
            all_lengths += lengths_by_sample[i]  # 这个是response的实际token长度
            all_truncated += truncated_by_sample[i]  # §3: 截断标记
            idx_map     += [i]*config.K_ROLLOUTS
        t_gen = _t.time() - t_gen0

        # ——评审（并发 + 双云 + 缓存 + 限流/退避）+ 统计 provider 分布——
        t_judge0 = _t.time()
        from concurrent.futures import ThreadPoolExecutor
        provider_count = {}
        def _score_one(i):
            s = batch[idx_map[i]]
            r_obj = judge.evaluate(s, all_resps[i])
            prov = r_obj.get("provider", "?")
            provider_count[prov] = provider_count.get(prov, 0) + 1
            r = r_obj.get("final", 0.5)
            r = max(0.0, min(1.0, float(r))) * 2 - 1
            return float(np.clip(r, -config.REWARD_CLIP, config.REWARD_CLIP))
        rewards_list = []
        with ThreadPoolExecutor(max_workers=config.JUDGE_MAX_WORKERS) as ex:
            for rv in ex.map(_score_one, range(len(all_resps))):
                rewards_list.append(rv)
        rewards = torch.tensor(rewards_list, device=device, dtype=torch.float32)
        t_judge = _t.time() - t_judge0

        # 打印评审耗时与 provider 分布（定位用）
        if (step + 1) % 5 == 0:
            print(f"\n[Judge@step{step+1}] time={t_judge:.1f}s providers={provider_count}")

        # 【新增】奖励分支内标准化（含winsorize去除离群值）
        task_list = [tasks[idx_map[i]] for i in range(len(idx_map))]
        rewards = reward_normalizer.update_and_normalize(rewards, task_list)

        # ——一次性分词 + 计算 ref_lp（复用）——
        t_tok0 = _t.time()
        full_tok, comp_mask = _tokenize_concat(tokenizer, all_prompts, all_resps, all_lengths, device)
        
        # 【修改】检查gen_len越界（硬约束128）
        gen_lengths = comp_mask.sum(dim=1).cpu().numpy()
        max_gen_len = gen_lengths.max()
        if max_gen_len > 128:  # 硬约束
            print(f"\n⚠️ [步骤{step+1}] 检测到gen_len超过硬约束: max={max_gen_len} > 128")
            print("  这表明comp_mask统计口径错误（包含了prompt或padding），需修正代码！")
            print(f"  all_lengths (response实际长度)范围: [{min(all_lengths)}, {max(all_lengths)}]")
            print(f"  gen_lengths (comp_mask统计)范围: [{gen_lengths.min()}, {gen_lengths.max()}]")
        elif max_gen_len > current_max_new_tokens_train:
            # 只是超过当前设定，但没超硬约束（正常）
            pass
        
        with torch.no_grad():
            out_ref = base_model(input_ids=full_tok["input_ids"],
                                 attention_mask=full_tok.get("attention_mask"),
                                 use_cache=False)
            ref_logp = F.log_softmax(out_ref.logits[:, :-1, :], dim=-1)
            tgt = full_tok["input_ids"][:, 1:]
            sel = ref_logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)
            denom = comp_mask.sum(dim=1).clamp_min(1.0)
            ref_lp = (sel * comp_mask).sum(dim=1) / denom
        t_tok = _t.time() - t_tok0

        # ——组内优势——
        t_adv0 = _t.time()
        adv = compute_group_advantages(rewards, k=config.K_ROLLOUTS)
        t_adv = _t.time() - t_adv0

        # ——MU_UPDATES（old_lp 快照一次；每次仅重算 cur_lp）——
        t_mu0 = _t.time()
        # 先用当前模型快照 old_lp（no_grad）
        with torch.no_grad():
            out_old = model(input_ids=full_tok["input_ids"],
                            attention_mask=full_tok.get("attention_mask"),
                            use_cache=False)
            old_logp = F.log_softmax(out_old.logits[:, :-1, :], dim=-1)
            tgt = full_tok["input_ids"][:, 1:]
            sel = old_logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)
            denom = comp_mask.sum(dim=1).clamp_min(1.0)
            old_lp = (sel * comp_mask).sum(dim=1) / denom
        old_lp = old_lp.detach()

        task_mask_f = torch.tensor([1 if tasks[g]=='fairness' else 0 for g in idx_map], device=device, dtype=torch.bool)
        task_mask_h = ~task_mask_f

        nan_inf_hits = 0
        grad_cosine_sim = 0.0  # 记录梯度余弦相似度

        # 【修复梯度累积】梯度累积逻辑移到 MU_UPDATES 循环外部
        accumulation_counter += 1
        should_zero_grad = (accumulation_counter % config.GRADIENT_ACCUMULATION_STEPS == 1)
        should_update = (accumulation_counter % config.GRADIENT_ACCUMULATION_STEPS == 0)

        # 在累积周期开始时清零梯度
        if should_zero_grad:
            opt.zero_grad(set_to_none=False)  # 【性能优化】设为0而非None，后续可以用copy_而非clone
            is_first_microbatch = True
        else:
            is_first_microbatch = False

        for _ in range(config.MU_UPDATES):

            out_cur = model(input_ids=full_tok["input_ids"],
                            attention_mask=full_tok.get("attention_mask"),
                            use_cache=False)
            cur_logp = F.log_softmax(out_cur.logits[:, :-1, :], dim=-1)
            tgt = full_tok["input_ids"][:, 1:]
            sel = cur_logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)
            denom = comp_mask.sum(dim=1).clamp_min(1.0)
            cur_lp = (sel * comp_mask).sum(dim=1) / denom

            ratio = torch.exp(cur_lp - old_lp)
            clip_ratio = torch.clamp(ratio, 1-config.PPO_CLIP_EPS, 1+config.PPO_CLIP_EPS)
            surr = torch.minimum(ratio*adv, clip_ratio*adv)

            # 【最终修复】KL散度计算：使用平方误差（稳定且对称）
            #
            # 问题历史：
            # 1. exp(delta)-delta-1 → 爆炸（delta=3时kl=16）
            # 2. ref_lp - cur_lp → 方向反了，KL=0.000
            # 3. abs(cur_lp - ref_lp) → 双向penalty，模型崩溃（F生成长度=1.0）
            #
            # 正确的实现：使用平方误差
            # - KL ≈ (cur_lp - ref_lp)^2 / 2（二阶泰勒展开）
            # - 总是非负，对称，不爆炸
            # - 当 cur_lp ≈ ref_lp 时，KL ≈ 0（模型接近参考）
            # - 当 cur_lp 偏离 ref_lp 时，KL 增大（需要 penalty）
            #
            delta = (cur_lp - ref_lp).clamp(-10, 10)  # 防止极端值
            kl = (delta ** 2) * 0.5  # 平方误差（不再需要 abs 或 clamp）

            # §7: 使用分支化β值（不同的KL约束）
            beta_f = kl_controller.get_beta_f()  # Fairness: 低β
            beta_h = kl_controller.get_beta_h()  # Hallucination: 高β

            _anchor_zero = sum((p.sum() * 0.0) for p in trainable)

            if task_mask_f.any():
                loss_fair = -(surr[task_mask_f].mean()) + beta_f * kl[task_mask_f].mean()
            else:
                loss_fair = _anchor_zero

            if task_mask_h.any():
                loss_halu = -(surr[task_mask_h].mean()) + beta_h * kl[task_mask_h].mean()
            else:
                loss_halu = _anchor_zero

            # 【显存优化】梯度累积：loss 除以累积步数
            loss_fair = loss_fair / config.GRADIENT_ACCUMULATION_STEPS
            loss_halu = loss_halu / config.GRADIENT_ACCUMULATION_STEPS

            # 检查 NaN/Inf
            if torch.isnan(loss_fair) or torch.isinf(loss_fair) or torch.isnan(loss_halu) or torch.isinf(loss_halu):
                nan_inf_hits += 1
                continue

            # 【新增】计算两个任务的梯度并监控冲突
            grads_f = torch.autograd.grad(loss_fair, trainable, retain_graph=True, allow_unused=True)
            grads_h = torch.autograd.grad(loss_halu, trainable, retain_graph=True, allow_unused=True)

            vec_f = torch.nn.utils.parameters_to_vector([g if g is not None else torch.zeros_like(p) for g,p in zip(grads_f, trainable)])
            vec_h = torch.nn.utils.parameters_to_vector([g if g is not None else torch.zeros_like(p) for g,p in zip(grads_h, trainable)])

            # 【新增】监控梯度冲突
            if conflict_monitor is not None:
                conflict_info = conflict_monitor.update(vec_f, vec_h, step + 1)
                grad_cosine_sim = conflict_info["cosine_sim"]
                use_conflict_resolution = conflict_info["use_conflict_resolution"]
            else:
                use_conflict_resolution = config.USE_CAGRAD

            # 【修改】根据冲突状态决定梯度合成策略
            # 【性能优化】第一个 micro-batch 用 copy_（快），后续用 add_（累加）
            if use_conflict_resolution:
                cagrad_combine_and_set_grads(trainable, vec_f, vec_h, c=config.CAGRAD_C, accumulate=not is_first_microbatch)
            else:
                _set_grads_from_vec(trainable, 0.5*(vec_f+vec_h), accumulate=not is_first_microbatch)

        # 【修复梯度累积】参数更新移到 MU_UPDATES 循环外部
        # 在累积周期结束时更新参数
        if should_update:
            torch.nn.utils.clip_grad_norm_(trainable, max_norm=1.0)
            opt.step()

            # 【显存优化】参数更新后清理显存
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        t_mu = _t.time() - t_mu0

        # 收集指标
        with torch.no_grad():
            loss_total = 0.5*(loss_fair + loss_halu)
            
            def _safe_mean(x, mask):
                if mask.any(): 
                    return x[mask].mean().item()
                return 0.0
            
            def _safe_std(x, mask):
                if mask.any() and mask.sum() > 1:
                    return x[mask].std().item()
                return 0.0
            
            kl_f_val = _safe_mean(kl, task_mask_f)
            kl_h_val = _safe_mean(kl, task_mask_h)
            kl_overall = kl.mean().item()  # 整体KL
            
            reward_f_mean = _safe_mean(rewards, task_mask_f)
            reward_h_mean = _safe_mean(rewards, task_mask_h)
            reward_f_std = _safe_std(rewards, task_mask_f)
            reward_h_std = _safe_std(rewards, task_mask_h)
            
            # 【修正】正确计算clip_frac：基于PPO定义
            # clipped = ((ratio > 1+ε) 且 (adv < 0)) 或 ((ratio < 1-ε) 且 (adv > 0))
            eps = config.PPO_CLIP_EPS
            clipped = ((ratio > 1 + eps) & (adv < 0)) | ((ratio < 1 - eps) & (adv > 0))
            clip_frac = clipped.float().mean().item()
            
            # 【修正】生成长度统计：只统计completion段，按comp_mask求和
            gen_lengths = comp_mask.sum(dim=1).cpu().numpy()  # 每个样本的生成长度
            lengths_f = [gen_lengths[i] for i in range(len(gen_lengths)) if task_mask_f[i]]
            lengths_h = [gen_lengths[i] for i in range(len(gen_lengths)) if task_mask_h[i]]
            gen_len_f = np.mean(lengths_f) if lengths_f else 0
            gen_len_h = np.mean(lengths_h) if lengths_h else 0

            # §3: 准确的截断率统计（基于EOS/EOT检测）
            truncated_f = [all_truncated[i] for i in range(len(all_truncated)) if task_mask_f[i]]
            truncated_h = [all_truncated[i] for i in range(len(all_truncated)) if task_mask_h[i]]
            trunc_f = sum(truncated_f) / max(1, len(truncated_f))
            trunc_h = sum(truncated_h) / max(1, len(truncated_h))
            
            # 零长度比例
            zero_len_f = sum(1 for l in lengths_f if l == 0) / max(1, len(lengths_f))
            zero_len_h = sum(1 for l in lengths_h if l == 0) / max(1, len(lengths_h))
            
            # 其他指标
            adv_abs_mean = adv.abs().mean().item()
            delta_mean = delta.mean().item()
        
        # §7: 更新分支化KL控制器
        kl_controller.update(kl_f_val, kl_h_val)

        # §7: KL自适应调整（每N步触发一次）
        if (step + 1) % config.KL_ADAPTIVE_WINDOW == 0:
            kl_controller.auto_adjust(step + 1)
        
        # 【修改】截断率监控与告警（不再自动调整，因为已到硬约束上限）
        if trunc_f > config.TRUNC_FRAC_WARNING or trunc_h > config.TRUNC_FRAC_WARNING:
            print(f"\n⚠️ [步骤{step+1}] 截断率过高(F:{trunc_f:.1%}, H:{trunc_h:.1%})")
            print(f"  当前max_new_tokens={current_max_new_tokens_train}（已达硬约束上限128）")
            print(f"  建议：(1)降低temperature={config.TEMPERATURE_TRAIN} (2)增大rep_penalty={config.REP_PENALTY_TRAIN}")
            print(f"       (3)增大presence_penalty={config.PRESENCE_PENALTY} (4)或接受10-20%的截断率")
        
        # 记录指标
        step_metrics = {
            "loss": loss_total.item(),
            "kl_f": kl_f_val,
            "kl_h": kl_h_val,
            "kl_overall": kl_overall,
            "beta_f": beta_f,  # §7: Fairness分支β
            "beta_h": beta_h,  # §7: Hallucination分支β
            "grad_cosine_sim": grad_cosine_sim,  # 梯度余弦相似度
            "reward_f_mean": reward_f_mean,
            "reward_h_mean": reward_h_mean,
            "reward_f_std": reward_f_std,
            "reward_h_std": reward_h_std,
            "clip_frac": clip_frac,
            "gen_len_f_mean": gen_len_f,
            "gen_len_h_mean": gen_len_h,
            "trunc_frac_f": trunc_f,  # 【新增】
            "trunc_frac_h": trunc_h,  # 【新增】
            "zero_len_rate_f": zero_len_f,
            "zero_len_rate_h": zero_len_h,
            "judge_time": t_judge,
            "provider_openai": provider_count.get("openai", 0),
            "provider_claude": provider_count.get("claude", 0),
            "provider_heuristic": provider_count.get("heuristic", 0),
            "adv_abs_mean": adv_abs_mean,
            "delta_mean": delta_mean,
            "nan_inf_hits": nan_inf_hits,
            "lr": opt.param_groups[0]["lr"],
            "ppo_eps": config.PPO_CLIP_EPS,
            "max_new_tokens_train": current_max_new_tokens_train  # 【新增】
        }
        metrics_logger.record_step(step + 1, step_metrics)
        
        t_step = _t.time() - t0
        progress.set_postfix({
            "loss": f"{loss_total.item():.4f}",
            "kl": f"{kl_overall:.3f}",
            "β_f": f"{beta_f:.3f}",  # §7: 显示两个β值
            "β_h": f"{beta_h:.3f}",
            "cos": f"{grad_cosine_sim:.2f}",
            "t": f"{t_step:.1f}s"
        })

        # 【修改】在线中途快评，默认greedy模式（稳定）
        # 【性能优化】使用更少样本数加速快速评估
        if (step + 1) % config.PARETO_PRINT_EVERY == 0:
            with torch.no_grad():
                # 中途快评固定使用greedy，使用少量样本仅看趋势
                fair_q = quick_eval_fast(model, tokenizer, device, judge, dataset, "fairness",
                                        n_samples=config.PARETO_QUICK_EVAL_SAMPLES, provider_hint="openai",
                                        use_sampling=False)  # 固定greedy
                halu_q = quick_eval_fast(model, tokenizer, device, judge, dataset, "hallucination",
                                        n_samples=config.PARETO_QUICK_EVAL_SAMPLES, provider_hint="openai",
                                        use_sampling=False)  # 固定greedy
            # 打印奖励分数和关键指标
            print(f"\n[QuickEval@{step+1}] mode=greedy fairness={fair_q:.3f}  hallucination={halu_q:.3f}")
            print(f"  截断率: F={trunc_f:.1%}  H={trunc_h:.1%}  |  生成长度: F={gen_len_f:.1f}  H={gen_len_h:.1f}")
            print(f"  KL散度: F={kl_f_val:.3f}  H={kl_h_val:.3f}  |  β值: F={beta_f:.4f}  H={beta_h:.4f}")

        # 正式 Pareto 存盘（低频），也使用greedy
        if (step + 1) % config.PARETO_EVAL_FREQ == 0:
            GenerationConfigManager.print_config(mode="eval_greedy")
            
            fairness_score = evaluate_objective(model, tokenizer, device, judge, dataset, "fairness", 
                                               n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=False)
            hallucination_score = evaluate_objective(model, tokenizer, device, judge, dataset, "hallucination", 
                                                     n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=False)
            pareto.add_point(step+1, fairness_score, hallucination_score, None)
            pareto.save_frontier(config.OUTPUT_DIR)
            print(f"\n[Pareto@{step+1}] mode=greedy fairness={fairness_score:.3f}  hallucination={hallucination_score:.3f}")

    print("✅ GRPO 完成")
    
    # 【新增】打印统一KL控制器调整历史
    if config.KL_ADAPTIVE_CONTROL:
        print("\n" + "="*60)
        print("统一KL自适应控制历史")
        print("="*60)
        adj_history = kl_controller.get_adjustment_history()
        if adj_history:
            for adj in adj_history[-10:]:  # 显示最后10次调整
                print(f"Step {adj['step']}: {adj['action']}")
        else:
            print("未触发调整")
        print("="*60)
    
    # 【新增】打印梯度冲突监控结果
    if conflict_monitor is not None:
        print("\n" + "="*60)
        print("梯度冲突监控统计")
        print("="*60)
        stats = conflict_monitor.get_recent_stats()
        print(f"余弦相似度: mean={stats['mean']:.3f}, min={stats['min']:.3f}")
        print(f"负相关比例: {stats['negative_ratio']:.1%}")
        print(f"是否启用CAGrad: {conflict_monitor.use_conflict_resolution}")
        if conflict_monitor.log:
            print("\n冲突事件:")
            for event in conflict_monitor.log:
                print(f"  Step {event['step']}: {event['action']}")
        print("="*60)
    
    # 【新增】打印奖励标准化统计
    if config.REWARD_NORMALIZE:
        print("\n" + "="*60)
        print("奖励标准化统计（EMA + Winsorize）")
        print("="*60)
        stats = reward_normalizer.get_stats()
        for task, stat in stats.items():
            print(f"{task}: mean={stat['mean']:.3f}, std={stat['std']:.3f}, count={stat['count']}")
        print("="*60)
    
    # 生成并保存汇总报告
    print("\n生成训练汇总报告...")
    summary = metrics_logger.save_summary()
    
    # 【修改】训练结束后的采样评测（与中途greedy形成对比）
    print("\n" + "="*60)
    print("训练结束 - 采样版评测（与中途greedy对比）")
    print("="*60)
    GenerationConfigManager.print_config(mode="eval_sampling")
    
    with torch.no_grad():
        fair_final_sampling = evaluate_objective(model, tokenizer, device, judge, dataset, "fairness", 
                                       n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=True)
        halu_final_sampling = evaluate_objective(model, tokenizer, device, judge, dataset, "hallucination",
                                       n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=True)
        
        # 也跑一次greedy作为对照
        fair_final_greedy = evaluate_objective(model, tokenizer, device, judge, dataset, "fairness",
                                     n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=False)
        halu_final_greedy = evaluate_objective(model, tokenizer, device, judge, dataset, "hallucination",
                                     n_samples=config.PARETO_PRINT_SAMPLES, use_sampling=False)
    
    print(f"\n【最终评测结果对比】")
    print(f"  Greedy  - Fairness: {fair_final_greedy:.3f}, Hallucination: {halu_final_greedy:.3f}")
    print(f"  Sampling - Fairness: {fair_final_sampling:.3f}, Hallucination: {halu_final_sampling:.3f}")
    print("="*60 + "\n")
    
    return summary

# =============================================================================
# 评估（支持贪心和采样 + 追踪）
# =============================================================================
def evaluate_objective(model, tokenizer, device, judge, dataset, task: str, n_samples: int=40, use_sampling: bool=False) -> float:
    """评估目标，支持贪心和采样模式"""
    pool = dataset.fairness_samples if task=="fairness" else dataset.hallucination_samples
    if not pool: 
        return 0.0
    smp = get_quickeval_pool(task, dataset, n=n_samples)  # 使用固定样本
    scores = []
    trace_path = config.OUTPUT_DIR / f"eval_trace_step_live.jsonl"
    
    mode_str = "sampling" if use_sampling else "greedy"
    
    for s in smp:
        resp = generate_one_response(model, tokenizer, device, s.prompt, use_sampling=use_sampling)
        sc_obj = judge.evaluate(s, resp)
        sc = sc_obj.get("final", 0.5)
        with open(trace_path, "a", encoding="utf-8") as tf:
            tf.write(json.dumps({
                "task": s.task, "id": s.id,
                "resp_sha": hashlib.sha256(resp.encode()).hexdigest(),
                "score": sc, "provider": sc_obj.get("provider","?"),
                "mode": mode_str
            }, ensure_ascii=False) + "\n")
        scores.append(float(sc))
    return float(np.mean(scores)) if scores else 0.0

# =============================================================================
# 主流程
# =============================================================================
def main():
    # SDK 安装与自检
    _bootstrap_sdks_and_check()
    
    # 【新增】统一种子设置
    set_all_seeds(42)
    
    print("\n" + "="*80)
    print(f"训练运行 ID: {config.RUN_ID}")
    print(f"输出目录: {config.OUTPUT_DIR}")
    print("="*80)

    bbq  = BBQAdapter().load_samples(config.N_BBQ_TRAIN)
    halu = HaluEvalAdapter().load_samples(config.N_HALU_TRAIN)
    if not bbq or not halu:
        print("❌ 数据不足（BBQ/HaluEval 至少一类为空）")
        return
    dataset = MultiObjectiveDataset(bbq, halu)

    model, base_model, tokenizer, device = load_model_and_tokenizer()
    judge = MultiCloudJudge()
    pareto = ParetoFrontier(max_checkpoints=config.N_PARETO_CHECKPOINTS)

    if config.DO_SFT_CONTINUE:
        sft_continue(model, tokenizer, device, dataset)

    if config.DO_GRPO:
        grpo_train(model, base_model, tokenizer, device, dataset, judge, pareto)

    print("\n保存最终模型...")
    final_path = config.OUTPUT_DIR / "final_model"
    final_path.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)

    best = pareto.get_best()
    if best:
        print("\n" + "="*80)
        print("最佳 Pareto 点")
        print(f"Step: {best.step}\nFairness: {best.fairness_score:.3f}\nHallucination: {best.hallucination_score:.3f}")
        print("="*80)

    report = {
        "run_id": config.RUN_ID,
        "timestamp": datetime.now().isoformat(),
        "config": {"model": config.BASE_MODEL, "sft_steps": config.SFT_STEPS,
                   "grpo_steps": config.GRPO_STEPS, "lora_r": config.LORA_R, "bf16": config.USE_BF16,
                   "reward_normalize": config.REWARD_NORMALIZE,
                   "max_new_tokens_train": config.MAX_NEW_TOKENS_TRAIN,
                   "max_new_tokens_eval": config.MAX_NEW_TOKENS_EVAL},
        "dataset_stats": {"n_fairness": len(bbq), "n_hallucination": len(halu)}
    }
    with open(config.OUTPUT_DIR/"final_report.json","w") as f:
        json.dump(report, f, indent=2)
    print("✅ 训练完成")
    print(f"输出目录: {config.OUTPUT_DIR}")

if __name__ == "__main__":
    main()
