# ğŸ”¬ Fairnessä¿¡å·ä¸º0 - å…¨é¢è¯Šæ–­æŠ¥å‘Š

**é—®é¢˜**: Step 5æ˜¾ç¤º `F std=0.000, rel=0.000` - Fairnessä¿¡å·å®Œå…¨æ­»äº¡

**å½±å“**: 50%çš„groupäº§ç”Ÿé›¶æ¢¯åº¦ï¼ŒFairnessä»»åŠ¡æ— æ³•å­¦ä¹ 

---

## ğŸ“‹ 7å¤§å¯èƒ½åŸå› åŠè¯Šæ–­æ–¹æ¡ˆ

### âœ… **åŸå› 1: æ¨¡æ¿å´©æºƒï¼ˆå·²ç¡®è®¤ï¼‰**

**ç°è±¡**: æ‰€æœ‰4ä¸ªå€™é€‰å®Œå…¨ç›¸åŒ
```
Answer: B
Justification: The context does not provide sufficient information to determine this.
```

**è¯æ®**:
- Step 5æ‰€æœ‰å€™é€‰19 tokensï¼Œå®Œå…¨ä¸€è‡´
- ç†µå€¼0.012-0.293ï¼ˆä¸¥é‡åä½ï¼‰

**å·²åº”ç”¨ä¿®å¤**:
- ENTROPY_COEF: 1.5 â†’ 2.5
- MIN_NEW_TOKENS: 15 â†’ 30
- TEMPERATURE: 0.9 â†’ 1.0

**çŠ¶æ€**: âœ… å·²ä¿®å¤ï¼ˆå¾…éªŒè¯ï¼‰

---

### â“ **åŸå› 2: Batchå†…åªé‡‡æ ·ambigæ ·æœ¬**

**å‡è®¾**: å¦‚æœbatchå†…åªæœ‰ambigæ ·æœ¬ï¼Œä¸”éƒ½ç”¨æ¨¡æ¿å›ç­”"insufficient information"ï¼Œåˆ™å¿…ç„¶std=0

**éœ€è¦æ£€æŸ¥**:
1. `get_balanced_batch()` æ˜¯å¦æ­£ç¡®æ··åˆambig/disambigï¼Ÿ
2. BBQAdapteré‡‡æ ·æ¯”ä¾‹ï¼š80% disambig / 20% ambigæ˜¯å¦ç”Ÿæ•ˆï¼Ÿ
3. å•ä¸ªbatch (BATCH_SIZE=2) ä¸­Fairnessæ ·æœ¬çš„context_conditionåˆ†å¸ƒ

**å½“å‰é…ç½®**:
- `GRPO_BATCH_SIZE = 2` ï¼ˆæ¯æ­¥2ä¸ªæ ·æœ¬ï¼‰
- BBQAdapter: 80% disambig / 20% ambigï¼ˆå…¨å±€æ¯”ä¾‹ï¼‰

**æ½œåœ¨é—®é¢˜**:
- å¦‚æœbatch=2ï¼Œfairnessåªæœ‰1ä¸ªæ ·æœ¬ï¼ˆå¦ä¸€ä¸ªæ˜¯hallucinationï¼‰
- é‚£1ä¸ªfairnessæ ·æœ¬å¦‚æœæ˜¯ambig â†’ 4ä¸ªå€™é€‰ç”¨æ¨¡æ¿ â†’ std=0

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ batch compositionè¯Šæ–­
if step < 20:
    fairness_samples = [s for s in batch if s.task == "fairness"]
    for i, s in enumerate(fairness_samples):
        ctx_cond = s.meta.get("context_condition", "unknown")
        print(f"  Fairness sample #{i}: context_condition={ctx_cond}")
```

**å¯èƒ½ä¿®å¤**:
1. **å¢åŠ BATCH_SIZE**: 2 â†’ 4ï¼ˆç¡®ä¿è‡³å°‘2ä¸ªfairnessæ ·æœ¬ï¼‰
2. **å¼ºåˆ¶disambigé‡‡æ ·**: å¯¹äºè®­ç»ƒå‰æœŸï¼ˆstep<50ï¼‰ï¼Œå¼ºåˆ¶100% disambig
3. **åˆ†ç¦»ambig/disambig batches**: ä¸æ··åˆï¼Œäº¤æ›¿è®­ç»ƒ

---

### â“ **åŸå› 3: LLM Judgeå¯¹ambigæ¨¡æ¿æ‰“åˆ†è¿‡äºä¸€è‡´**

**å‡è®¾**: å¯¹äºambigæ ·æœ¬ï¼Œæ‰€æœ‰"insufficient information"å›ç­”éƒ½å¾—åˆ°ç›¸åŒçš„é«˜åˆ†ï¼ˆå¦‚0.8-0.9ï¼‰

**ç†è®ºä¾æ®**:
- llm_judge_prompts_v2.py Line 214:
  ```python
  if context_condition == "ambig":
      correct_answer = unknown_option
      task_type = "AMBIGUOUS"
      reasoning_focus = "è¯†åˆ«ä¿¡æ¯ä¸è¶³"
  ```
- LLM Judgeå¯èƒ½å¯¹"æ ‡å‡†ç­”æ¡ˆ"éƒ½ç»™ç›¸åŒåˆ†æ•°

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨é›¶æ¢¯åº¦ç»„è¯Šæ–­ä¸­æ·»åŠ LLM JudgeåŸå§‹åˆ†æ•°æ‰“å°
if step < 20 and zero_gradient_group_idx is not None:
    i = zero_gradient_group_idx
    sample = batch[i]
    if sample.task == "fairness" and sample.meta.get("context_condition") == "ambig":
        print(f"\n[LLM Judgeè¯Šæ–­] Ambigæ ·æœ¬ï¼Œ4ä¸ªå€™é€‰çš„åŸå§‹è¯„åˆ†:")
        for j in range(K):
            idx = i * K + j
            response = all_resps[idx]
            # é‡æ–°è¯„ä¼°è·å–è¯¦ç»†åˆ†æ•°
            result = judge.evaluate(sample, response)
            print(f"  Candidate {j+1}:")
            print(f"    Final score: {result.get('final', 'N/A')}")
            print(f"    Provider: {result.get('provider', 'N/A')}")
            print(f"    Response (å‰80å­—ç¬¦): {response[:80]}")
```

**å¯èƒ½ä¿®å¤**:
1. **ä¿®æ”¹ambigè¯„åˆ†é€»è¾‘**: å³ä½¿ç­”æ¡ˆæ­£ç¡®(unknown)ï¼Œä¹Ÿæ ¹æ®justificationè´¨é‡äº§ç”Ÿå·®å¼‚
2. **å¢åŠ reasoningæƒé‡**: è®©æ¨ç†è´¨é‡æˆä¸ºä¸»è¦åŒºåˆ†ç‚¹
3. **æ·»åŠ diversity bonus**: å¯¹ä¸åŒè¡¨è¿°æ–¹å¼ç»™äºˆé¢å¤–åˆ†æ•°

---

### â“ **åŸå› 4: Reward Scaleç²¾åº¦ä¸¢å¤±**

**å‡è®¾**: `FAIRNESS_REWARD_SCALE = 0.7` å¯èƒ½å¯¼è‡´å¾®å°å·®å¼‚è¢«æŠ¹å¹³

**åœºæ™¯**:
```python
# åŸå§‹LLM Judgeåˆ†æ•°ï¼ˆå‡è®¾ï¼‰
candidate_1: 0.85
candidate_2: 0.86
candidate_3: 0.85
candidate_4: 0.86

# åº”ç”¨scale=0.7å
candidate_1: 0.595
candidate_2: 0.602
candidate_3: 0.595
candidate_4: 0.602

# å¦‚æœåç»­normalizationä½¿ç”¨float32ç²¾åº¦æˆªæ–­ï¼Œå¯èƒ½å˜æˆ
candidate_1-4: 0.60ï¼ˆå®Œå…¨ç›¸åŒï¼‰
```

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨reward scaleåç«‹å³æ‰“å°
if step < 20:
    fairness_indices = [i for i, t in enumerate(task_list) if t == "fairness"]
    if fairness_indices:
        f_rewards_before_scale = rewards_before_scale[fairness_indices]  # éœ€è¦ä¿å­˜scaleå‰çš„å€¼
        f_rewards_after_scale = rewards[fairness_indices]
        print(f"[Reward Scaleè¯Šæ–­@step{step+1}]")
        print(f"  Before scale (0.7): {f_rewards_before_scale.cpu().numpy()}")
        print(f"  After scale: {f_rewards_after_scale.cpu().numpy()}")
        print(f"  Std before: {f_rewards_before_scale.std():.6f}")
        print(f"  Std after: {f_rewards_after_scale.std():.6f}")
```

**å¯èƒ½ä¿®å¤**:
1. **æé«˜FAIRNESS_REWARD_SCALE**: 0.7 â†’ 1.0ï¼ˆä¸hallucinationå¹³ç­‰ï¼‰
2. **ä½¿ç”¨float64**: æé«˜æ•°å€¼ç²¾åº¦
3. **ç§»é™¤scale**: è®©normalizationè‡ªåŠ¨å¤„ç†å¹³è¡¡

---

### â“ **åŸå› 5: Reward NormalizationæŠ¹å¹³å·®å¼‚**

**å‡è®¾**: EMA z-scoreæ ‡å‡†åŒ–å¯èƒ½åœ¨æ–¹å·®è¿‡å°æ—¶äº§ç”Ÿæ•°å€¼ä¸ç¨³å®š

**ä»£ç å®¡æŸ¥** (trainer.py:436-485):
```python
def update_and_normalize(self, rewards, tasks):
    # ...
    batch_var = task_rewards_clean.var().item() if mask.sum() > 1 else 1.0

    # æœ€å°æ–¹å·®0.01
    self.stats[task]["var"] = max(
        self.decay * old_var + (1 - self.decay) * batch_var,
        0.01
    )

    # Z-score
    ema_std = np.sqrt(max(self.stats[task]["var"], 0.01))  # æœ€å°std=0.1
    normalized_task = (task_rewards - ema_mean) / ema_std
```

**æ½œåœ¨é—®é¢˜**:
1. å¦‚æœ`batch_var â‰ˆ 0`ï¼ˆæ‰€æœ‰å€™é€‰å¥–åŠ±ç›¸åŒï¼‰ï¼ŒEMAä»ä¼šä¿ç•™å†å²æ–¹å·®
2. ä½†å†å²æ–¹å·®å¯èƒ½ä¹Ÿå¾ˆå°ï¼ˆå¦‚æœä¸€ç›´éƒ½æ˜¯æ¨¡æ¿ï¼‰
3. æœ€å°æ–¹å·®0.01å¯èƒ½ä¸è¶³ä»¥é˜²æ­¢æ•°å€¼ä¸ç¨³å®š

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨normalizationåæ‰“å°è¯¦ç»†ç»Ÿè®¡
if step < 20:
    fairness_indices = [i for i, t in enumerate(task_list) if t == "fairness"]
    if fairness_indices:
        f_rewards_before_norm = rewards_before_norm[fairness_indices]
        f_rewards_after_norm = rewards[fairness_indices]

        print(f"[Reward Normalizationè¯Šæ–­@step{step+1}]")
        print(f"  Before norm: mean={f_rewards_before_norm.mean():.4f}, std={f_rewards_before_norm.std():.6f}")
        print(f"  After norm: mean={f_rewards_after_norm.mean():.4f}, std={f_rewards_after_norm.std():.6f}")
        print(f"  EMA stats: mean={reward_normalizer.stats.get('fairness', {}).get('mean', 'N/A'):.4f}, "
              f"std={np.sqrt(reward_normalizer.stats.get('fairness', {}).get('var', 0)):.4f}")
        print(f"  Values before norm: {f_rewards_before_norm.cpu().numpy()}")
        print(f"  Values after norm: {f_rewards_after_norm.cpu().numpy()}")
```

**å¯èƒ½ä¿®å¤**:
1. **æé«˜æœ€å°æ–¹å·®**: 0.01 â†’ 0.1ï¼ˆstdä»0.1â†’0.316ï¼‰
2. **ç¦ç”¨normalization**: `REWARD_NORMALIZE = False`ï¼ˆè‡³å°‘åœ¨åˆæœŸï¼‰
3. **ä¿®æ”¹normalizationç­–ç•¥**: ä½¿ç”¨min-max scalingè€Œéz-score

---

### â“ **åŸå› 6: 4ä¸ªå€™é€‰æ¥æºéªŒè¯**

**å‡è®¾**: è™½ç„¶ç†è®ºä¸Šæ¯ä¸ªæ ·æœ¬ç”ŸæˆK=4ä¸ªå€™é€‰ï¼Œä½†å¯èƒ½æœ‰bugå¯¼è‡´4ä¸ªå€™é€‰å®é™…æ¥è‡ªä¸åŒæ ·æœ¬

**éœ€è¦éªŒè¯**: `compute_group_advantages`çš„groupingé€»è¾‘

**ä»£ç å®¡æŸ¥** (trainer.py:3713-3756):
```python
def compute_group_advantages(rewards: torch.Tensor, k: int):
    B = Bk // k
    r = rewards.view(B, k)  # [B, K] - å‡è®¾æ¯Kä¸ªè¿ç»­rewardå±äºåŒä¸€ç»„
```

**æ½œåœ¨é—®é¢˜**:
- ä¾èµ–`rewards`çš„é¡ºåºä¸`idx_map`çš„é¡ºåºä¸€è‡´
- å¦‚æœä¸­é—´æœ‰ä»»ä½•ä¹±åºï¼Œgroupingä¼šé”™è¯¯

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨é›¶æ¢¯åº¦ç»„è¯Šæ–­ä¸­éªŒè¯grouping
if step < 20 and zero_gradient_group_idx is not None:
    i = zero_gradient_group_idx
    print(f"\n[GroupingéªŒè¯] ç»„{i}çš„idx_map:")
    for j in range(K):
        idx = i * K + j
        mapped_sample_idx = idx_map[idx]
        print(f"  Candidate {j+1}: idx_map[{idx}] = {mapped_sample_idx} (should be {i})")
        if mapped_sample_idx != i:
            print(f"  âŒ ERROR: Groupingé”™è¯¯ï¼å€™é€‰{j+1}å®é™…å±äºsample {mapped_sample_idx}")
```

**å¯èƒ½ä¿®å¤**:
- å¦‚æœå‘ç°groupingé”™è¯¯ï¼Œéœ€è¦ä¿®å¤`idx_map`æ„å»ºé€»è¾‘ï¼ˆtrainer.py:3986-3993ï¼‰

---

### â“ **åŸå› 7: Advantageè®¡ç®—çš„æ•°å€¼é—®é¢˜**

**å‡è®¾**: å³ä½¿rewardæœ‰å¾®å°å·®å¼‚ï¼Œadvantageè®¡ç®—ä¸­çš„é™¤æ³•å¯èƒ½å¼•å…¥æ•°å€¼é—®é¢˜

**ä»£ç å®¡æŸ¥** (trainer.py:3743-3751):
```python
if group_std < 0.01:
    group_adv = torch.zeros_like(group_rewards)  # é›¶æ¢¯åº¦
else:
    group_mean = group_rewards.mean()
    group_adv = (group_rewards - group_mean) / group_std.clamp_min(1e-6)
```

**æ½œåœ¨é—®é¢˜**:
- é˜ˆå€¼`0.01`å¯èƒ½å¤ªä½ï¼ˆå¯¹åº”std=1%ï¼‰
- å®é™…ä¸­rewardå·®å¼‚å¯èƒ½æ˜¯0.002-0.005ï¼Œè¢«åˆ¤å®šä¸º"é›¶æ–¹å·®"

**è¯Šæ–­æ–¹æ¡ˆ**:
```python
# åœ¨compute_group_advantagesä¸­æ·»åŠ è¯¦ç»†æ—¥å¿—
if step < 20:
    for i in range(B):
        group_rewards = r[i]
        group_std = group_rewards.std()
        group_mean = group_rewards.mean()

        if group_std < 0.01:
            print(f"[Advantageè¯Šæ–­] ç»„{i}: std={group_std:.6f} < 0.01ï¼Œè®¾ç½®adv=0")
            print(f"  Rewards: {group_rewards.cpu().numpy()}")
        elif group_std < 0.05:  # ä¹Ÿæ‰“å°æ¥è¿‘é˜ˆå€¼çš„æƒ…å†µ
            print(f"[Advantageè¯Šæ–­] ç»„{i}: std={group_std:.6f} (æ¥è¿‘é˜ˆå€¼)")
            print(f"  Rewards: {group_rewards.cpu().numpy()}")
```

**å¯èƒ½ä¿®å¤**:
1. **é™ä½é˜ˆå€¼**: 0.01 â†’ 0.001ï¼ˆå…è®¸æ›´å°çš„æ–¹å·®ï¼‰
2. **ä½¿ç”¨ç›¸å¯¹é˜ˆå€¼**: `std / mean < 0.01`ï¼ˆç›¸å¯¹å˜åŒ–ï¼‰
3. **ä¿ç•™å¾®å°æ¢¯åº¦**: å³ä½¿std<0.01ï¼Œä¹Ÿç”¨åŸå§‹rewardä½œä¸ºadvantageï¼ˆä¸ä¹‹å‰é”™è¯¯æ–¹æ¡ˆä¸åŒï¼Œè¿™æ¬¡è¦æ­£ç¡®å½’ä¸€åŒ–ï¼‰

---

## ğŸ¯ ç»¼åˆä¿®å¤æ–¹æ¡ˆ

åŸºäºä»¥ä¸Šåˆ†æï¼Œå»ºè®®**åˆ†é˜¶æ®µä¿®å¤**ï¼š

### Phase 1: è¯Šæ–­å¢å¼ºï¼ˆç«‹å³å®æ–½ï¼‰

åœ¨`grpo_train()`ä¸­æ·»åŠ æ‰€æœ‰7ä¸ªè¯Šæ–­æ£€æŸ¥ï¼š

1. Batch compositionç›‘æ§
2. LLM JudgeåŸå§‹åˆ†æ•°æ‰“å°
3. Reward Scaleå‰åå¯¹æ¯”
4. Reward Normalizationè¯¦ç»†ç»Ÿè®¡
5. GroupingéªŒè¯
6. Advantageè®¡ç®—è¯¦ç»†æ—¥å¿—
7. æ¯æ­¥æ‰“å°fairnessæ ·æœ¬çš„context_condition

**ç›®æ ‡**: æ‰¾å‡ºçœŸæ­£çš„root cause

### Phase 2: å‚æ•°è°ƒæ•´ï¼ˆåŸºäºè¯Šæ–­ç»“æœï¼‰

| å‚æ•° | å½“å‰å€¼ | å»ºè®®å€¼ï¼ˆæ–¹æ¡ˆAï¼‰ | å»ºè®®å€¼ï¼ˆæ–¹æ¡ˆBï¼‰ |
|------|--------|----------------|----------------|
| `GRPO_BATCH_SIZE` | 2 | 4 | 6 |
| `FAIRNESS_REWARD_SCALE` | 0.7 | 1.0 | 1.0 |
| `REWARD_NORMALIZE` | True | True | False |
| `æœ€å°æ–¹å·®(RewardNormalizer)` | 0.01 | 0.1 | - |
| `é›¶æ¢¯åº¦é˜ˆå€¼(advantage)` | 0.01 | 0.001 | 0.005 |
| `BBQ disambigæ¯”ä¾‹` | 80% | 90% | 100% (å‰50æ­¥) |

**æ–¹æ¡ˆA**: ä¿å®ˆä¿®å¤ï¼ˆå‡è®¾normalizationæœ‰é—®é¢˜ï¼‰
**æ–¹æ¡ˆB**: æ¿€è¿›ä¿®å¤ï¼ˆç¦ç”¨normalizationï¼Œä¾èµ–raw rewardï¼‰

### Phase 3: æ•°æ®ç­–ç•¥è°ƒæ•´

1. **å‰50æ­¥**: ä»…ä½¿ç”¨disambigæ ·æœ¬ï¼ˆé¿å…ambigæ¨¡æ¿å½±å“ï¼‰
2. **50-100æ­¥**: 90% disambig / 10% ambig
3. **100+æ­¥**: æ¢å¤80/20æ¯”ä¾‹

**å®ç°**:
```python
# åœ¨BBQAdapter.load_samples()ä¸­
current_step = global_step  # éœ€è¦ä¼ å…¥
if current_step < 50:
    target_disambig_ratio = 1.0
    target_ambig_ratio = 0.0
elif current_step < 100:
    target_disambig_ratio = 0.9
    target_ambig_ratio = 0.1
else:
    target_disambig_ratio = 0.8
    target_ambig_ratio = 0.2
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

åº”ç”¨ä¿®å¤åï¼Œé¢„æœŸçœ‹åˆ°ï¼š

1. **Batch composition**: è‡³å°‘50%çš„batchåŒ…å«disambig fairnessæ ·æœ¬
2. **LLM Judgeåˆ†æ•°**: å³ä½¿æ˜¯ambigæ ·æœ¬ï¼Œ4ä¸ªå€™é€‰çš„åˆ†æ•°ä¹Ÿæœ‰å·®å¼‚ï¼ˆå¦‚0.75, 0.80, 0.78, 0.82ï¼‰
3. **Reward std**: Fairnessç»„å†…stdä»0.000æå‡åˆ°>0.01
4. **Advantage**: éé›¶advantageæ¯”ä¾‹ä»50%æå‡åˆ°>80%
5. **é›¶æ¢¯åº¦ç»„**: ä»50%é™åˆ°<30%

---

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³**: æ·»åŠ æ‰€æœ‰7ä¸ªè¯Šæ–­æ£€æŸ¥
2. **è¿è¡Œ1-2æ­¥**: æ”¶é›†è¯¦ç»†æ—¥å¿—
3. **åˆ†ææ—¥å¿—**: ç¡®å®šä¸»è¦root cause
4. **åº”ç”¨targeted fix**: åªä¿®å¤çœŸæ­£çš„é—®é¢˜
5. **éªŒè¯**: ç¡®è®¤Fairnessä¿¡å·æ¢å¤

---

**Created**: 2025-11-17
**Status**: è¯Šæ–­æ–¹æ¡ˆå¾…å®æ–½
