#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SFT Target é•¿åº¦æ£€æŸ¥è„šæœ¬

è¯Šæ–­ç›®æ ‡ï¼šæ£€æŸ¥ SFT è®­ç»ƒæ•°æ®çš„ target é•¿åº¦æ˜¯å¦ä¸ MIN_NEW_TOKENS=5 åŒ¹é…
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ target å¹³å‡é•¿åº¦è¿œå¤§äº 5 tokensï¼Œä¼šå¯¼è‡´ï¼š
  1. æ¨¡å‹åœ¨ SFT å­¦ä¹ ç”Ÿæˆæ›´é•¿çš„å†…å®¹
  2. GRPO æ—¶æ¨¡å‹æƒ³ç”Ÿæˆé•¿å†…å®¹ï¼Œä½†è¢« MIN_NEW_TOKENS=5 çº¦æŸ
  3. EOS Suppressor å¼ºåˆ¶ç¦æ­¢è¿‡æ—©ç»“æŸ
  4. æ¨¡å‹ä¸çŸ¥é“è¯´ä»€ä¹ˆ â†’ ç”Ÿæˆ"æœ€ç¡®å®š"çš„token â†’ Entropyå´©æºƒ

ç”¨æ³•ï¼š
  python scripts/inspect_sft_targets.py
"""

import sys
import os
from pathlib import Path
import json
import random
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# å¯¼å…¥é…ç½®
from grpo.trainer import config, BBQAdapter, HaluEvalAdapter, Sample

def tokenize_rough(text: str) -> int:
    """
    ç²—ç•¥ä¼°ç®—tokenæ•°é‡ï¼ˆå®é™…ä¼šç”¨çœŸå®tokenizerï¼‰
    ç»éªŒå…¬å¼ï¼šè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦/token
    è¿™é‡Œç”¨ä¿å®ˆä¼°ç®—ï¼š3.5å­—ç¬¦/token
    """
    return len(text) // 4  # ç²—ç•¥ä¼°ç®—

def analyze_targets(samples: List[Sample], name: str):
    """åˆ†ætargeté•¿åº¦åˆ†å¸ƒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {name} Target é•¿åº¦åˆ†æ")
    print(f"{'='*80}")

    if not samples:
        print("âŒ æ— æ ·æœ¬æ•°æ®")
        return

    # ç»Ÿè®¡
    char_lengths = []
    token_lengths_rough = []

    for s in samples:
        if s.target:
            char_len = len(s.target)
            token_len = tokenize_rough(s.target)
            char_lengths.append(char_len)
            token_lengths_rough.append(token_len)

    if not char_lengths:
        print("âŒ æ‰€æœ‰æ ·æœ¬çš„ target éƒ½ä¸ºç©º")
        return

    # ç»Ÿè®¡æŒ‡æ ‡
    char_mean = sum(char_lengths) / len(char_lengths)
    char_min = min(char_lengths)
    char_max = max(char_lengths)
    char_median = sorted(char_lengths)[len(char_lengths)//2]

    token_mean = sum(token_lengths_rough) / len(token_lengths_rough)
    token_min = min(token_lengths_rough)
    token_max = max(token_lengths_rough)
    token_median = sorted(token_lengths_rough)[len(token_lengths_rough)//2]

    print(f"\næ ·æœ¬æ•°: {len(samples)}")
    print(f"\nå­—ç¬¦é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡: {char_mean:.1f} å­—ç¬¦")
    print(f"  ä¸­ä½æ•°: {char_median} å­—ç¬¦")
    print(f"  èŒƒå›´: {char_min} - {char_max} å­—ç¬¦")

    print(f"\nToken é•¿åº¦ä¼°ç®— (ç²—ç•¥):")
    print(f"  å¹³å‡: {token_mean:.1f} tokens")
    print(f"  ä¸­ä½æ•°: {token_median} tokens")
    print(f"  èŒƒå›´: {token_min} - {token_max} tokens")

    # ğŸ”¥ å…³é”®è¯Šæ–­
    print(f"\nğŸ”¥ å…³é”®è¯Šæ–­:")
    print(f"  å½“å‰é…ç½®: MIN_NEW_TOKENS_TRAIN = {config.MIN_NEW_TOKENS_TRAIN}")

    if token_mean > config.MIN_NEW_TOKENS_TRAIN * 2:
        print(f"  âš ï¸ è­¦å‘Š: Target å¹³å‡é•¿åº¦ ({token_mean:.1f}) æ˜¯ MIN_NEW_TOKENS ({config.MIN_NEW_TOKENS_TRAIN}) çš„ {token_mean/config.MIN_NEW_TOKENS_TRAIN:.1f}x")
        print(f"  â†’ SFT è®­ç»ƒæ¨¡å‹ç”Ÿæˆ {token_mean:.1f} tokensï¼Œä½† GRPO æ—¶åªå…è®¸æœ€å°‘ {config.MIN_NEW_TOKENS_TRAIN} tokens")
        print(f"  â†’ è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æƒ³ç”Ÿæˆæ›´é•¿å†…å®¹ä½†è¢«å¼ºåˆ¶æˆªæ–­ â†’ EOS Suppressor è§¦å‘ â†’ Entropy å´©æºƒ")
        print(f"  å»ºè®®: MIN_NEW_TOKENS_TRAIN åº”è‡³å°‘è®¾ä¸º {int(token_mean * 0.7)}-{int(token_mean)}")
    elif token_mean > config.MIN_NEW_TOKENS_TRAIN:
        print(f"  âœ… Target å¹³å‡é•¿åº¦ ({token_mean:.1f}) ç•¥é«˜äº MIN_NEW_TOKENS ({config.MIN_NEW_TOKENS_TRAIN})")
        print(f"  å»ºè®®: è€ƒè™‘æå‡ MIN_NEW_TOKENS_TRAIN åˆ° {int(token_mean * 0.8)}-{int(token_mean)} ä»¥æ›´å¥½åŒ¹é… SFT è®­ç»ƒ")
    else:
        print(f"  âœ… Target å¹³å‡é•¿åº¦ ({token_mean:.1f}) ä¸ MIN_NEW_TOKENS ({config.MIN_NEW_TOKENS_TRAIN}) åŸºæœ¬åŒ¹é…")

    # åˆ†å¸ƒç»Ÿè®¡
    bins = [0, 5, 10, 20, 50, 100, 200, float('inf')]
    bin_labels = ["0-5", "5-10", "10-20", "20-50", "50-100", "100-200", "200+"]
    bin_counts = [0] * len(bin_labels)

    for tl in token_lengths_rough:
        for i, (low, high) in enumerate(zip(bins[:-1], bins[1:])):
            if low <= tl < high:
                bin_counts[i] += 1
                break

    print(f"\nToken é•¿åº¦åˆ†å¸ƒ:")
    for label, count in zip(bin_labels, bin_counts):
        pct = count / len(token_lengths_rough) * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"  {label:>10} tokens: {count:>4} ({pct:>5.1f}%) {bar}")

    # å±•ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬å±•ç¤º (éšæœºæŠ½å–3ä¸ª):")
    sample_indices = random.sample(range(len(samples)), min(3, len(samples)))
    for idx in sample_indices:
        s = samples[idx]
        target_preview = s.target[:100] + "..." if len(s.target) > 100 else s.target
        print(f"\næ ·æœ¬ #{idx} ({s.task}):")
        print(f"  ID: {s.id}")
        print(f"  Target ({len(s.target)} å­—ç¬¦, ~{tokenize_rough(s.target)} tokens):")
        print(f"    {target_preview}")

def main():
    print("="*80)
    print("ğŸ” SFT Target é•¿åº¦æ£€æŸ¥è„šæœ¬")
    print("="*80)
    print(f"\nå½“å‰é…ç½®:")
    print(f"  BBQ_DIR: {config.BBQ_DIR}")
    print(f"  HALUEVAL_DIR: {config.HALUEVAL_DIR}")
    print(f"  N_BBQ_TRAIN: {config.N_BBQ_TRAIN}")
    print(f"  N_HALU_TRAIN: {config.N_HALU_TRAIN}")
    print(f"  MIN_NEW_TOKENS_TRAIN: {config.MIN_NEW_TOKENS_TRAIN}")
    print(f"  MAX_NEW_TOKENS_TRAIN: {config.MAX_NEW_TOKENS_TRAIN}")

    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not config.BBQ_DIR.exists():
        print(f"\nâŒ BBQ ç›®å½•ä¸å­˜åœ¨: {config.BBQ_DIR}")
        print("   è¯·ç¡®ä¿æ•°æ®ç›®å½•æ­£ç¡®")
        return

    if not config.HALUEVAL_DIR.exists():
        print(f"\nâŒ HaluEval ç›®å½•ä¸å­˜åœ¨: {config.HALUEVAL_DIR}")
        print("   è¯·ç¡®ä¿æ•°æ®ç›®å½•æ­£ç¡®")
        return

    # åŠ è½½æ•°æ®
    print(f"\n{'='*80}")
    print("ğŸ“¦ åŠ è½½æ•°æ®...")
    print(f"{'='*80}")

    bbq = BBQAdapter()
    bbq_samples = bbq.load_samples(config.N_BBQ_TRAIN)

    halu = HaluEvalAdapter()
    halu_samples = halu.load_samples(config.N_HALU_TRAIN)

    # åˆ†æ
    analyze_targets(bbq_samples, "BBQ (Fairness)")
    analyze_targets(halu_samples, "HaluEval (Hallucination)")

    # æ€»ä½“åˆ†æ
    all_samples = bbq_samples + halu_samples
    analyze_targets(all_samples, "æ€»ä½“ (BBQ + HaluEval)")

    # æœ€ç»ˆå»ºè®®
    print(f"\n{'='*80}")
    print("ğŸ’¡ æœ€ç»ˆå»ºè®®")
    print(f"{'='*80}")

    all_token_lengths = [tokenize_rough(s.target) for s in all_samples if s.target]
    if all_token_lengths:
        avg_len = sum(all_token_lengths) / len(all_token_lengths)

        print(f"\nå½“å‰é…ç½®:")
        print(f"  MIN_NEW_TOKENS_TRAIN = {config.MIN_NEW_TOKENS_TRAIN}")
        print(f"  SFT Target å¹³å‡é•¿åº¦ â‰ˆ {avg_len:.1f} tokens")

        if avg_len > config.MIN_NEW_TOKENS_TRAIN * 2:
            recommended_min = int(avg_len * 0.7)
            print(f"\nğŸ”´ ä¸¥é‡ä¸åŒ¹é…ï¼å»ºè®®ä¿®æ”¹:")
            print(f"  MIN_NEW_TOKENS_TRAIN = 5 â†’ {recommended_min}")
            print(f"\nåŸå› :")
            print(f"  - SFT è®­ç»ƒæ¨¡å‹ç”Ÿæˆ {avg_len:.1f} tokens çš„å†…å®¹")
            print(f"  - GRPO æ—¶ MIN_NEW_TOKENS=5 è¿‡çŸ­ï¼Œå¯¼è‡´æ¨¡å‹æƒ³ç”Ÿæˆé•¿å†…å®¹ä½†è¢«é™åˆ¶")
            print(f"  - EOS Suppressor å¼ºåˆ¶ç¦æ­¢å‰5ä¸ªtokençš„EOS â†’ æ¨¡å‹è¢«è¿«ç»­å†™")
            print(f"  - æ¨¡å‹ä¸çŸ¥é“è¯´ä»€ä¹ˆ â†’ ç”Ÿæˆæœ€ç¡®å®šçš„token â†’ Entropy å´©æºƒåˆ° 0.005")
            print(f"\nä¿®å¤æ­¥éª¤:")
            print(f"  1. ä¿®æ”¹ trainer.py ç¬¬ {214} è¡Œå·¦å³:")
            print(f"     MIN_NEW_TOKENS_TRAIN = 5 â†’ {recommended_min}")
            print(f"  2. é‡æ–°å¼€å§‹ GRPO è®­ç»ƒï¼ˆSFT ä¸éœ€è¦é‡è®­ï¼‰")
        elif avg_len > config.MIN_NEW_TOKENS_TRAIN * 1.5:
            recommended_min = int(avg_len * 0.8)
            print(f"\nğŸŸ¡ ä¸­ç­‰ä¸åŒ¹é…ï¼Œå»ºè®®ä¼˜åŒ–:")
            print(f"  MIN_NEW_TOKENS_TRAIN = 5 â†’ {recommended_min}")
        else:
            print(f"\nâœ… Target é•¿åº¦ä¸ MIN_NEW_TOKENS åŸºæœ¬åŒ¹é…")
            print(f"   Entropy å´©æºƒå¯èƒ½ç”±å…¶ä»–åŸå› å¼•èµ·")

if __name__ == "__main__":
    main()
