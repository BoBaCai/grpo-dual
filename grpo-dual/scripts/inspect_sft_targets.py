#!/usr/bin/env python3
"""
SFT Target é•¿åº¦æ£€æŸ¥è„šæœ¬ - Jupyter notebookå‹å¥½ç‰ˆ
å¯ä»¥ç›´æ¥å¤åˆ¶æ•´ä¸ªè„šæœ¬åˆ°notebook cellæ‰§è¡Œ

è¯Šæ–­ç›®æ ‡ï¼šæ£€æŸ¥ SFT è®­ç»ƒæ•°æ®çš„ target é•¿åº¦æ˜¯å¦ä¸ MIN_NEW_TOKENS=5 åŒ¹é…
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ target å¹³å‡é•¿åº¦è¿œå¤§äº 5 tokensï¼Œä¼šå¯¼è‡´ï¼š
  1. æ¨¡å‹åœ¨ SFT å­¦ä¹ ç”Ÿæˆæ›´é•¿çš„å†…å®¹
  2. GRPO æ—¶æ¨¡å‹æƒ³ç”Ÿæˆé•¿å†…å®¹ï¼Œä½†è¢« MIN_NEW_TOKENS=5 çº¦æŸ
  3. EOS Suppressor å¼ºåˆ¶ç¦æ­¢è¿‡æ—©ç»“æŸ
  4. æ¨¡å‹ä¸çŸ¥é“è¯´ä»€ä¹ˆ â†’ ç”Ÿæˆ"æœ€ç¡®å®š"çš„token â†’ Entropyå´©æºƒ

ç”¨æ³•ï¼š
  # æ–¹å¼1ï¼šå‘½ä»¤è¡Œè¿è¡Œ
  python scripts/inspect_sft_targets.py

  # æ–¹å¼2ï¼šJupyter notebook
  %run scripts/inspect_sft_targets.py

  # æ–¹å¼3ï¼šç›´æ¥å¤åˆ¶æ•´ä¸ªè„šæœ¬åˆ°notebook cellè¿è¡Œ
"""

import json
import random
from pathlib import Path
from collections import defaultdict

# ============================================================================
# é…ç½®è·¯å¾„ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
# ============================================================================
print("="*80)
print("ğŸ” SFT Target é•¿åº¦æ£€æŸ¥è„šæœ¬")
print("="*80)

# å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
BBQ_PATHS = [
    Path("/workspace/data/bbq"),
    Path("data/bbq"),
    Path("../data/bbq"),
    Path("grpo-dual/data/bbq"),
]

HALUEVAL_PATHS = [
    Path("/workspace/data/halueval"),
    Path("data/halueval"),
    Path("../data/halueval"),
    Path("grpo-dual/data/halueval"),
]

BBQ_DIR = None
HALUEVAL_DIR = None

for path in BBQ_PATHS:
    if path.exists():
        BBQ_DIR = path
        break

for path in HALUEVAL_PATHS:
    if path.exists():
        HALUEVAL_DIR = path
        break

print(f"\næ•°æ®è·¯å¾„æ£€æµ‹:")
print(f"  BBQ: {BBQ_DIR if BBQ_DIR else 'âŒ æœªæ‰¾åˆ°'}")
print(f"  HaluEval: {HALUEVAL_DIR if HALUEVAL_DIR else 'âŒ æœªæ‰¾åˆ°'}")

if not BBQ_DIR or not HALUEVAL_DIR:
    print(f"\nâš ï¸  æ•°æ®é›†è·¯å¾„æœªæ‰¾åˆ°ï¼Œè¯·æ‰‹åŠ¨è®¾ç½®ï¼š")
    print(f"  BBQ_DIR = Path('/your/path/to/bbq')")
    print(f"  HALUEVAL_DIR = Path('/your/path/to/halueval')")
    # ä¸é€€å‡ºï¼Œç»§ç»­å°è¯•

# ============================================================================
# é…ç½®å‚æ•°ï¼ˆä¸ trainer.py ä¿æŒä¸€è‡´ï¼‰
# ============================================================================
MIN_NEW_TOKENS_TRAIN = 5  # å½“å‰é…ç½®
MAX_NEW_TOKENS_TRAIN = 128
N_BBQ_TRAIN = 1100
N_HALU_TRAIN = 400
SUMM_MAX_DOC_CHARS = 1000

BBQ_FILES = {
    "Age": "Age.jsonl",
    "Disability_status": "Disability_status.jsonl",
    "Gender_identity": "Gender_identity.jsonl",
    "Nationality": "Nationality.jsonl",
    "Physical_appearance": "Physical_appearance.jsonl",
    "Race_ethnicity": "Race_ethnicity.jsonl",
    "Race_x_gender": "Race_x_gender.jsonl",
    "Race_x_SES": "Race_x_SES.jsonl",
    "Religion": "Religion.jsonl",
    "SES": "SES.jsonl",
    "Sexual_orientation": "Sexual_orientation.jsonl",
}

HALUEVAL_FILES = {
    "qa": "qa_data.json",
    "dialogue": "dialogue_data.json",
    "summarization": "summarization_data.json",
    "general": "general_data.json",
}

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def tokenize_rough(text: str) -> int:
    """ç²—ç•¥ä¼°ç®—tokenæ•°é‡ï¼ˆè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼‰"""
    return max(1, len(text) // 4)

def read_jsonl(fp: Path, max_lines: int = None) -> list:
    """è¯»å–JSONLæ–‡ä»¶"""
    data = []
    try:
        with open(fp, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_lines and i >= max_lines:
                    break
                if line.strip():
                    data.append(json.loads(line))
    except Exception as e:
        print(f"âŒ è¯»å– {fp.name} å¤±è´¥: {e}")
    return data

# ============================================================================
# BBQ Target æ„å»ºï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰
# ============================================================================
def build_bbq_target(item: dict) -> str:
    """æ„å»ºBBQçš„SFT target"""
    context_condition = item.get("context_condition", "")

    if context_condition == "ambig":
        # Ambiguous: é€‰Unknown
        unk = find_unknown_option(item)
        return f"Answer: {unk}\nJustification: The context does not provide sufficient information to determine this."
    else:
        # Disambiguous: é€‰æ­£ç¡®ç­”æ¡ˆå¹¶å¼•ç”¨context
        label = item.get("label", -1)
        if 0 <= label <= 2:
            answer_key = f"ans{label}"
            correct_answer = chr(65 + label)  # A, B, C
            answer_text = item.get(answer_key, "")

            # æå–contextç‰‡æ®µï¼ˆ120å­—ç¬¦ï¼‰
            context = item.get("context", "")
            context_snippet = context[:120] + "..." if len(context) > 120 else context

            return f"Answer: {correct_answer}\nJustification: Based on the context: \"{context_snippet}\", the answer is {answer_text}."
        else:
            # æ²¡æœ‰labelï¼Œå›é€€åˆ°ambiguous
            unk = find_unknown_option(item)
            return f"Answer: {unk}\nJustification: The context does not provide sufficient information to determine this."

def find_unknown_option(item: dict) -> str:
    """æŸ¥æ‰¾Unknowné€‰é¡¹"""
    info = item.get("answer_info", {})
    for idx, key in enumerate(["ans0", "ans1", "ans2"]):
        if key in info:
            val = info[key]
            if isinstance(val, list) and "unknown" in [str(x).lower() for x in val]:
                return chr(65 + idx)
    return "C"

# ============================================================================
# HaluEval Target æ„å»ºï¼ˆä¸ trainer.py ä¸€è‡´ï¼‰
# ============================================================================
def build_halueval_target(item: dict, subset: str) -> str:
    """æ„å»ºHaluEvalçš„SFT target"""

    if subset == "qa":
        answer = item.get("right_answer", "")
        know = item.get("knowledge", "")
        # QA: 150å­—ç¬¦snippet
        know_snippet = know[:150] + "..." if len(know) > 150 else know
        return f"Answer: {answer}\nEvidence: \"{know_snippet}\""

    elif subset == "dialogue":
        response = item.get("right_response", "")
        know = item.get("knowledge", "")
        # Dialogue: 150å­—ç¬¦snippet
        know_snippet = know[:150] + "..." if len(know) > 150 else know
        return f"Answer: {response}\nEvidence: \"{know_snippet}\""

    elif subset == "summarization":
        gold = item.get("right_summary", "") or item.get("summary", "")
        doc = item.get("document", "") or item.get("article", "")
        # æˆªæ–­document
        if len(doc) > SUMM_MAX_DOC_CHARS:
            doc = doc[:SUMM_MAX_DOC_CHARS] + "..."
        # 200å­—ç¬¦evidence
        doc_snippet = doc[:200] + "..." if len(doc) > 200 else doc
        return f"Summary: {gold}\nEvidence: \"{doc_snippet}\""

    else:  # general
        chatgpt_resp = item.get("chatgpt_response", "")
        hallucination = item.get("hallucination", "no")

        if hallucination == "no":
            # æ— hallucination
            resp_truncated = chatgpt_resp[:200] + "..." if len(chatgpt_resp) > 200 else chatgpt_resp
            return f"Answer: {resp_truncated}\nEvidence: \"Based on general knowledge\""
        else:
            # æœ‰hallucination
            return "Answer: I need more information to provide an accurate answer.\nEvidence: \"insufficient\""

# ============================================================================
# åˆ†æå‡½æ•°
# ============================================================================
def analyze_targets(targets: list, name: str):
    """åˆ†ætargeté•¿åº¦åˆ†å¸ƒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {name} Target é•¿åº¦åˆ†æ")
    print(f"{'='*80}")

    if not targets:
        print("âŒ æ— æ ·æœ¬æ•°æ®")
        return None

    char_lengths = [len(t) for t in targets]
    token_lengths = [tokenize_rough(t) for t in targets]

    char_mean = sum(char_lengths) / len(char_lengths)
    char_median = sorted(char_lengths)[len(char_lengths)//2]
    char_min, char_max = min(char_lengths), max(char_lengths)

    token_mean = sum(token_lengths) / len(token_lengths)
    token_median = sorted(token_lengths)[len(token_lengths)//2]
    token_min, token_max = min(token_lengths), max(token_lengths)

    print(f"\næ ·æœ¬æ•°: {len(targets)}")
    print(f"\nå­—ç¬¦é•¿åº¦:")
    print(f"  å¹³å‡: {char_mean:.1f}, ä¸­ä½æ•°: {char_median}, èŒƒå›´: [{char_min}, {char_max}]")
    print(f"\nToken ä¼°ç®—:")
    print(f"  å¹³å‡: {token_mean:.1f}, ä¸­ä½æ•°: {token_median}, èŒƒå›´: [{token_min}, {token_max}]")

    # ğŸ”¥ å…³é”®è¯Šæ–­
    print(f"\nğŸ”¥ å…³é”®è¯Šæ–­:")
    print(f"  å½“å‰é…ç½®: MIN_NEW_TOKENS_TRAIN = {MIN_NEW_TOKENS_TRAIN}")
    ratio = token_mean / MIN_NEW_TOKENS_TRAIN

    if ratio > 3:
        print(f"  ğŸ”´ ä¸¥é‡ä¸åŒ¹é…: Targetå¹³å‡ {token_mean:.1f} tokens æ˜¯ MIN_NEW_TOKENS {MIN_NEW_TOKENS_TRAIN} çš„ {ratio:.1f}x")
        print(f"  â†’ SFTè®­ç»ƒæ¨¡å‹ç”Ÿæˆ~{token_mean:.1f}è¯ï¼ŒGRPOæ—¶è¢«MIN={MIN_NEW_TOKENS_TRAIN}çº¦æŸ")
        print(f"  â†’ EOS Suppressorå¼ºåˆ¶ç¦æ­¢å‰{MIN_NEW_TOKENS_TRAIN}ä¸ªtoken â†’ æ¨¡å‹è¢«è¿«ç»­å†™")
        print(f"  â†’ ä¸çŸ¥é“è¯´ä»€ä¹ˆ â†’ ç”Ÿæˆæœ€ç¡®å®štoken â†’ Entropyå´©æºƒ")
        recommended = int(token_mean * 0.7)
        print(f"  âœ… å»ºè®®: MIN_NEW_TOKENS_TRAIN = {MIN_NEW_TOKENS_TRAIN} â†’ {recommended}")
    elif ratio > 1.5:
        recommended = int(token_mean * 0.8)
        print(f"  ğŸŸ¡ ä¸­ç­‰ä¸åŒ¹é…: Targetå¹³å‡ {token_mean:.1f} > MIN {MIN_NEW_TOKENS_TRAIN}")
        print(f"  âœ… å»ºè®®: MIN_NEW_TOKENS_TRAIN â†’ {recommended}")
    else:
        print(f"  âœ… åŸºæœ¬åŒ¹é… (ratio={ratio:.1f})")

    # Tokenåˆ†å¸ƒ
    bins = [0, 5, 10, 20, 50, 100, 200, 500]
    labels = ["0-5", "5-10", "10-20", "20-50", "50-100", "100-200", "200+"]
    counts = [0] * len(labels)

    for tl in token_lengths:
        for i in range(len(bins)-1):
            if bins[i] <= tl < bins[i+1]:
                counts[i] += 1
                break
        else:
            counts[-1] += 1

    print(f"\nTokenåˆ†å¸ƒ:")
    for label, count in zip(labels, counts):
        pct = count / len(token_lengths) * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"  {label:>10}: {count:>4} ({pct:>5.1f}%) {bar}")

    # æ ·æœ¬å±•ç¤º
    print(f"\nğŸ“ éšæœºæ ·æœ¬ (3ä¸ª):")
    for i in random.sample(range(len(targets)), min(3, len(targets))):
        t = targets[i]
        preview = t[:80] + "..." if len(t) > 80 else t
        print(f"\n  #{i} ({len(t)}å­—ç¬¦, ~{tokenize_rough(t)} tokens):")
        print(f"    {preview}")

    return {"mean": token_mean, "median": token_median, "min": token_min, "max": token_max}

# ============================================================================
# ä¸»é€»è¾‘ï¼šåŠ è½½BBQæ•°æ®
# ============================================================================
print(f"\n{'='*80}")
print("ğŸ“¦ åŠ è½½ BBQ æ•°æ®...")
print(f"{'='*80}")

bbq_targets = []

if BBQ_DIR and BBQ_DIR.exists():
    per_cat = N_BBQ_TRAIN // len(BBQ_FILES)

    for cat, filename in BBQ_FILES.items():
        fp = BBQ_DIR / filename
        if not fp.exists():
            print(f"âš ï¸  {filename} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        data = read_jsonl(fp)
        if not data:
            continue

        # è‡ªé€‚åº”é‡‡æ ·ï¼ˆä¸trainer.pyä¸€è‡´ï¼‰
        ambig = [x for x in data if x.get("context_condition") == "ambig"]
        disambig = [x for x in data if x.get("context_condition") != "ambig"]

        if disambig:
            n_disambig = int(per_cat * 0.6)  # 60% disambig
            n_ambig = per_cat - n_disambig

            picked = random.sample(disambig, min(n_disambig, len(disambig)))
            if ambig:
                picked += random.sample(ambig, min(n_ambig, len(ambig)))
        else:
            picked = random.sample(data, min(per_cat, len(data)))

        for item in picked:
            target = build_bbq_target(item)
            bbq_targets.append(target)

        print(f"  {cat}: {len(picked)} æ ·æœ¬")

    print(f"\nBBQæ€»è®¡: {len(bbq_targets)} æ ·æœ¬")
else:
    print("âŒ BBQæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨")

# ============================================================================
# ä¸»é€»è¾‘ï¼šåŠ è½½HaluEvalæ•°æ®
# ============================================================================
print(f"\n{'='*80}")
print("ğŸ“¦ åŠ è½½ HaluEval æ•°æ®...")
print(f"{'='*80}")

halu_targets = []

if HALUEVAL_DIR and HALUEVAL_DIR.exists():
    per_subset = N_HALU_TRAIN // len(HALUEVAL_FILES)

    for subset, filename in HALUEVAL_FILES.items():
        fp = HALUEVAL_DIR / filename
        if not fp.exists():
            print(f"âš ï¸  {filename} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        data = read_jsonl(fp)
        if not data:
            continue

        sampled = random.sample(data, min(per_subset, len(data)))

        for item in sampled:
            target = build_halueval_target(item, subset)
            halu_targets.append(target)

        print(f"  {subset}: {len(sampled)} æ ·æœ¬")

    print(f"\nHaluEvalæ€»è®¡: {len(halu_targets)} æ ·æœ¬")
else:
    print("âŒ HaluEvalæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨")

# ============================================================================
# åˆ†æç»“æœ
# ============================================================================
bbq_stats = analyze_targets(bbq_targets, "BBQ (Fairness)")
halu_stats = analyze_targets(halu_targets, "HaluEval (Hallucination)")

all_targets = bbq_targets + halu_targets
all_stats = analyze_targets(all_targets, "æ€»ä½“ (BBQ + HaluEval)")

# ============================================================================
# æœ€ç»ˆå»ºè®®
# ============================================================================
print(f"\n{'='*80}")
print("ğŸ’¡ æœ€ç»ˆå»ºè®®")
print(f"{'='*80}")

if all_stats:
    avg_len = all_stats["mean"]
    ratio = avg_len / MIN_NEW_TOKENS_TRAIN

    print(f"\nå½“å‰é…ç½®:")
    print(f"  MIN_NEW_TOKENS_TRAIN = {MIN_NEW_TOKENS_TRAIN}")
    print(f"  SFT Target å¹³å‡é•¿åº¦ â‰ˆ {avg_len:.1f} tokens")
    print(f"  æ¯”ä¾‹: {ratio:.1f}x")

    if ratio > 3:
        recommended = int(avg_len * 0.7)
        print(f"\nğŸ”´ ä¸¥é‡ä¸åŒ¹é…ï¼è¿™å¾ˆå¯èƒ½æ˜¯Entropyå´©æºƒçš„æ ¹æœ¬åŸå› ï¼")
        print(f"\nä¿®å¤æ­¥éª¤:")
        print(f"  1. æ‰¾åˆ° trainer.py ä¸­çš„ MIN_NEW_TOKENS_TRAIN é…ç½®")
        print(f"  2. ä¿®æ”¹: MIN_NEW_TOKENS_TRAIN = 5 â†’ {recommended}")
        print(f"  3. å¯é€‰ï¼šåŒæ—¶æå‡ ENTROPY_COEF = 0.2 â†’ 0.5")
        print(f"  4. é‡æ–°å¼€å§‹GRPOè®­ç»ƒï¼ˆSFT checkpointå¯ä»¥ä¿ç•™ï¼‰")
        print(f"\né¢„æœŸæ•ˆæœ:")
        print(f"  - EOS Suppressor è§¦å‘ç‡: 100% â†’ 30-50%")
        print(f"  - Entropy: 0.005 â†’ 0.5-1.5 (çŸ­æœŸ)")
        print(f"  - Max prob: 99.9% â†’ 60-90%")
    elif ratio > 1.5:
        recommended = int(avg_len * 0.8)
        print(f"\nğŸŸ¡ ä¸­ç­‰ä¸åŒ¹é…ï¼Œå»ºè®®ä¼˜åŒ–:")
        print(f"  MIN_NEW_TOKENS_TRAIN = {MIN_NEW_TOKENS_TRAIN} â†’ {recommended}")
    else:
        print(f"\nâœ… Targeté•¿åº¦ä¸MIN_NEW_TOKENSåŸºæœ¬åŒ¹é…")
        print(f"\nEntropyå´©æºƒå¯èƒ½ç”±å…¶ä»–åŸå› å¼•èµ·ï¼Œå°è¯•:")
        print(f"  1. æå‡ ENTROPY_COEF: 0.2 â†’ 1.0")
        print(f"  2. æå‡ TEMPERATURE: 0.9 â†’ 1.2")
        print(f"  3. é™ä½ REP_PENALTY: 1.18 â†’ 1.05")
else:
    print("\nâŒ æ— æ³•åˆ†æï¼ˆæ•°æ®é›†æœªåŠ è½½ï¼‰")
    print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„é…ç½®")

print(f"\n{'='*80}")
print("âœ… åˆ†æå®Œæˆï¼")
print(f"{'='*80}")
