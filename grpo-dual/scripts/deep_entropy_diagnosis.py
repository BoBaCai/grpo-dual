#!/usr/bin/env python3
"""
æ·±åº¦ Entropy å´©æºƒè¯Šæ–­è„šæœ¬

æ£€æŸ¥ 10+ ä¸ªå¯èƒ½å¯¼è‡´ Entropy å´©æºƒçš„æ ¹æœ¬åŸå› ï¼š
1. SFT è¿‡åº¦æ‹Ÿåˆ
2. Base model æœ¬èº« Entropy ä½
3. Reward ä¿¡å·é€€åŒ–
4. Advantage è®¡ç®— bug
5. LoRA æ¢¯åº¦æ¶ˆå¤±
6. Temperature é…ç½®è¢«è¦†ç›–
7. KL penalty è¿‡å¼º
8. Logits æ ¹æºé—®é¢˜
9. Repetition penalty å‰¯ä½œç”¨
10. æ•°æ®æ³„éœ²

ç”¨æ³•ï¼š
  python scripts/deep_entropy_diagnosis.py
"""

import json
import torch
import numpy as np
from pathlib import Path
from collections import defaultdict

print("="*80)
print("ğŸ”¬ æ·±åº¦ Entropy å´©æºƒè¯Šæ–­")
print("="*80)

# ============================================================================
# è¯Šæ–­ 1: SFT è¿‡åº¦æ‹Ÿåˆæ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 1: SFT è¿‡åº¦æ‹Ÿåˆæ£€æŸ¥")
print("="*80)

print("""
æ£€æŸ¥ç‚¹:
1. SFT æœ€ç»ˆ loss æ˜¯å¦è¿‡ä½ï¼ˆ< 0.1ï¼‰ï¼Ÿ
2. SFT è®­ç»ƒæ­¥æ•°æ˜¯å¦è¿‡å¤šï¼Ÿ
3. è®­ç»ƒæ•°æ®æ˜¯å¦å¤ªå°‘å¯¼è‡´é‡å¤è¿‡å¤šï¼Ÿ

è¯·æŸ¥çœ‹ SFT è®­ç»ƒæ—¥å¿—ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
- SFT æœ€ç»ˆ loss: ______
- SFT è®­ç»ƒæ­¥æ•°: ______
- è®­ç»ƒæ ·æœ¬æ•°: BBQ 1100 + HaluEval 400 = 1500

âš ï¸ è­¦å‘Šæ ‡å‡†:
  - Loss < 0.1: å¯èƒ½è¿‡æ‹Ÿåˆ
  - Loss < 0.05: ä¸¥é‡è¿‡æ‹Ÿåˆï¼ˆæ¨¡å‹"èƒŒè¯µ"è®­ç»ƒæ•°æ®ï¼‰
  - Steps/Samples ratio > 0.2: è¿‡åº¦è®­ç»ƒï¼ˆæ¯ä¸ªæ ·æœ¬å¹³å‡è§ >0.2 æ¬¡ï¼‰

ğŸ’¡ å¦‚æœè¿‡æ‹Ÿåˆï¼š
  - æ–¹æ¡ˆA: å‡å°‘ SFT_STEPS (200 â†’ 100)
  - æ–¹æ¡ˆB: å¢åŠ æ•°æ®é‡ (1500 â†’ 3000+)
  - æ–¹æ¡ˆC: å¢åŠ  dropout (0.1 â†’ 0.2)
  - æ–¹æ¡ˆD: è·³è¿‡ SFTï¼Œç›´æ¥ä» base model å¼€å§‹ GRPO
""")

# ============================================================================
# è¯Šæ–­ 2: Base Model Entropy æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 2: Base Model (Reference) Entropy æ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒå‡è®¾ï¼šå¦‚æœ base_model æœ¬èº«ç”Ÿæˆå°±æåº¦ç¡®å®šï¼Œé‚£ä¹ˆ GRPO çš„ KL penalty ä¼š
å¼ºåˆ¶ policy model é è¿‘ base_modelï¼Œå¯¼è‡´ policy model ä¹Ÿæåº¦ç¡®å®šã€‚

æ£€æŸ¥æ–¹æ³•ï¼šç”¨ base_model å•ç‹¬ç”Ÿæˆï¼Œçœ‹ Entropy

è¯·åœ¨ Python ä¸­è¿è¡Œä»¥ä¸‹ä»£ç ï¼ˆéœ€è¦å…ˆåŠ è½½æ¨¡å‹ï¼‰ï¼š

```python
# åŠ è½½ base modelï¼ˆä¸è¦åŠ è½½ LoRA adapterï¼‰
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "meta-llama/Llama-3.2-1B-Instruct"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)
base_model.eval()

# ç”Ÿæˆæ—¶è®°å½• logits
prompt = "Context: John has 15 years of experience. Question: Who is more experienced? A) John B) Mary C) Unknown"
inputs = tokenizer([prompt], return_tensors="pt")

with torch.no_grad():
    outputs = base_model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.9,
        return_dict_in_generate=True,
        output_scores=True
    )

# è®¡ç®—æ¯ä¸€æ­¥çš„ entropy
import torch.nn.functional as F
entropies = []
for scores in outputs.scores[:10]:  # å‰10ä¸ªtoken
    probs = F.softmax(scores[0] / 0.9, dim=-1)  # temperature=0.9
    entropy = -(probs * torch.log(probs + 1e-10)).sum()
    entropies.append(entropy.item())
    max_prob = probs.max().item()
    print(f"Token step: entropy={entropy:.3f}, max_prob={max_prob:.4f}")

print(f"\\nBase model average entropy: {np.mean(entropies):.3f}")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - Base model avg entropy < 0.5: ğŸ”´ Base model æœ¬èº«å°±æœ‰é—®é¢˜ï¼
  - Base model avg entropy 0.5-1.5: ğŸŸ¡ åä½ä½†å¯æ¥å—
  - Base model avg entropy > 1.5: âœ… Base model æ­£å¸¸

ğŸ’¡ å¦‚æœ base model entropy ä½:
  - å¯èƒ½éœ€è¦æ¢ä¸€ä¸ª base model
  - æˆ–è€…é™ä½ KL penalty (beta é™ä½ 50%)
  - æˆ–è€…å®Œå…¨ç§»é™¤ KL penaltyï¼ˆçº¯ reward ä¼˜åŒ–ï¼‰
""")

# ============================================================================
# è¯Šæ–­ 3: Reward ä¿¡å·é€€åŒ–æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 3: Reward ä¿¡å·é€€åŒ–æ£€æŸ¥")
print("="*80)

print("""
æ£€æŸ¥ç‚¹:
1. Reward æ˜¯å¦æ€»æ˜¯å›ºå®šå‡ ä¸ªå€¼ï¼ˆ0.420, 0.700ï¼‰ï¼Ÿ
2. Reward std æ˜¯å¦æ¥è¿‘ 0ï¼Ÿ
3. Fairness å’Œ Hallucination çš„ reward æ˜¯å¦éƒ½ä¸€æ ·ï¼Ÿ

è¯·ä»è®­ç»ƒæ—¥å¿—ä¸­æå– 10-20 æ­¥çš„ Reward æ•°æ®ï¼Œç„¶åè¿è¡Œï¼š

```python
# ä»æ—¥å¿—ä¸­æå–çš„ Reward æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
fairness_rewards = [0.420, 0.420, -0.210, 0.420, ...]  # æ›¿æ¢ä¸ºå®é™…æ•°æ®
hallucination_rewards = [-0.500, -0.600, -0.400, ...]

import numpy as np
print(f"Fairness Reward:")
print(f"  Mean: {np.mean(fairness_rewards):.3f}")
print(f"  Std: {np.std(fairness_rewards):.3f}")
print(f"  Unique values: {len(set(fairness_rewards))}")

print(f"Hallucination Reward:")
print(f"  Mean: {np.mean(hallucination_rewards):.3f}")
print(f"  Std: {np.std(hallucination_rewards):.3f}")
print(f"  Unique values: {len(set(hallucination_rewards))}")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - Reward std < 0.1: ğŸ”´ ä¸¥é‡é€€åŒ–ï¼Œæ— åŒºåˆ†åº¦
  - Unique values < 3: ğŸ”´ Reward è¿‡äºç¦»æ•£ï¼Œæ²¡æœ‰è¿ç»­ä¿¡å·
  - æŸä¸ªä»»åŠ¡çš„ reward å…¨ä¸€æ ·: ğŸ”´ è¯¥ä»»åŠ¡çš„ judge æœ‰é—®é¢˜

ğŸ’¡ å¦‚æœ reward é€€åŒ–:
  - æ£€æŸ¥ judge è¯„ä¼°é€»è¾‘ï¼ˆæ˜¯å¦æ€»æ˜¯è¿”å›å›ºå®šå€¼ï¼‰
  - æ£€æŸ¥ reward normalizationï¼ˆæ˜¯å¦è¿‡åº¦æ ‡å‡†åŒ–ï¼‰
  - å°è¯•ç§»é™¤ reward normalizationï¼Œç”¨åŸå§‹ reward
""")

# ============================================================================
# è¯Šæ–­ 4: GRPO Advantage è®¡ç®—æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 4: GRPO Advantage è®¡ç®—æ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœæ‰€æœ‰æ ·æœ¬çš„ advantage éƒ½ä¸€æ ·ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ†å¥½åæ ·æœ¬ã€‚

è¯·åœ¨ trainer.py çš„ GRPO è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ è¯Šæ–­ä»£ç ï¼š

```python
# åœ¨è®¡ç®— advantage ä¹‹åï¼ˆtrainer.py çº¦ 2900 è¡Œé™„è¿‘ï¼‰
# æ‰¾åˆ°è¿™æ®µä»£ç ï¼š
#   adv = reward - reward.mean()
# åœ¨åé¢æ·»åŠ ï¼š

print(f"\\n[Advantage è¯Šæ–­ @step{step}]")
print(f"  Reward: mean={reward.mean():.3f}, std={reward.std():.3f}")
print(f"  Advantage: mean={adv.mean():.3f}, std={adv.std():.3f}")
print(f"  Advantage range: [{adv.min():.3f}, {adv.max():.3f}]")
print(f"  Non-zero advantages: {(adv.abs() > 0.01).sum()}/{len(adv)}")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - Advantage std < 0.05: ğŸ”´ æ‰€æœ‰æ ·æœ¬å¾—åˆ†å‡ ä¹ä¸€æ ·
  - Advantage std < 0.2: ğŸŸ¡ åŒºåˆ†åº¦è¾ƒä½
  - Non-zero advantages < 50%: ğŸ”´ å¤§éƒ¨åˆ†æ ·æœ¬æ²¡æœ‰æ¢¯åº¦ä¿¡å·

ğŸ’¡ å¦‚æœ advantage é€€åŒ–:
  - æ£€æŸ¥ reward è®¡ç®—æ˜¯å¦æ­£ç¡®
  - æ£€æŸ¥æ˜¯å¦åœ¨ advantage è®¡ç®—å‰è¿‡åº¦æ ‡å‡†åŒ–
  - å°è¯•å¢åŠ  K_ROLLOUTSï¼ˆ4 â†’ 8ï¼‰ï¼Œæé«˜æ ·æœ¬å¤šæ ·æ€§
""")

# ============================================================================
# è¯Šæ–­ 5: LoRA æ¢¯åº¦æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 5: LoRA æ¢¯åº¦æ¶ˆå¤±æ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ LoRA æ¢¯åº¦ä¸º 0 æˆ–æå°ï¼Œæ¨¡å‹æ ¹æœ¬æ²¡åœ¨å­¦ä¹ ã€‚

è¯·åœ¨ trainer.py çš„ä¼˜åŒ–å™¨æ­¥éª¤åæ·»åŠ ï¼š

```python
# åœ¨ optimizer.step() ä¹‹åï¼ˆçº¦ 3200 è¡Œé™„è¿‘ï¼‰
if step % 5 == 0:  # æ¯5æ­¥æ£€æŸ¥ä¸€æ¬¡
    total_norm = 0
    lora_norm = 0
    for name, param in model.named_parameters():
        if param.grad is not None and param.requires_grad:
            param_norm = param.grad.data.norm(2).item()
            total_norm += param_norm ** 2
            if 'lora' in name.lower():
                lora_norm += param_norm ** 2

    total_norm = total_norm ** 0.5
    lora_norm = lora_norm ** 0.5

    print(f"\\n[Gradient è¯Šæ–­ @step{step}]")
    print(f"  Total grad norm: {total_norm:.6f}")
    print(f"  LoRA grad norm: {lora_norm:.6f}")

    # æ£€æŸ¥ LoRA æƒé‡æ˜¯å¦åœ¨å˜åŒ–
    if hasattr(model, 'base_model'):
        for name, param in model.base_model.named_parameters():
            if 'lora_A' in name and 'q_proj' in name:  # æ£€æŸ¥ä¸€ä¸ªä»£è¡¨æ€§çš„ LoRA å±‚
                print(f"  Sample LoRA weight mean: {param.data.mean():.6f}")
                print(f"  Sample LoRA weight std: {param.data.std():.6f}")
                break
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - Total grad norm < 1e-6: ğŸ”´ æ¢¯åº¦æ¶ˆå¤±
  - LoRA grad norm < 1e-7: ğŸ”´ LoRA æ²¡åœ¨å­¦ä¹ 
  - LoRA weight std < 1e-4: ğŸ”´ LoRA æƒé‡å‡ ä¹ä¸å˜

ğŸ’¡ å¦‚æœæ¢¯åº¦æ¶ˆå¤±:
  - æ£€æŸ¥ gradient_checkpointing æ˜¯å¦å¯¼è‡´æ¢¯åº¦æ–­è£‚
  - æ£€æŸ¥ loss.backward() æ˜¯å¦æ­£ç¡®è°ƒç”¨
  - æ£€æŸ¥ LoRA çš„ scaling factor (lora_alpha / lora_r)
  - å°è¯•æé«˜å­¦ä¹ ç‡ï¼ˆ3e-6 â†’ 1e-5ï¼‰
""")

# ============================================================================
# è¯Šæ–­ 6: Temperature é…ç½®æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 6: Temperature å®é™…ç”Ÿæ•ˆæ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šé…ç½®äº† temperature=0.9ï¼Œä½†å¯èƒ½åœ¨æŸä¸ªåœ°æ–¹è¢«è¦†ç›–æˆ–æ²¡ç”Ÿæ•ˆã€‚

æ£€æŸ¥æ–¹æ³•ï¼šåœ¨ generate() è°ƒç”¨æ—¶æ‰“å°å®é™…å‚æ•°

è¯·åœ¨ trainer.py çš„ generate_k_rollouts å‡½æ•°ä¸­æ·»åŠ ï¼ˆçº¦ 2030 è¡Œï¼‰ï¼š

```python
# åœ¨ model.generate() è°ƒç”¨ä¹‹å‰
print(f"\\n[Generate Config Check @step{step}]")
print(f"  temperature: {config.TEMPERATURE_TRAIN}")
print(f"  top_k: {config.TOP_K_TRAIN}")
print(f"  top_p: {config.TOP_P_TRAIN}")
print(f"  do_sample: True")

# ç„¶åæ£€æŸ¥ generate çš„å®é™…è°ƒç”¨
out = model.generate(
    **inputs,
    max_new_tokens=max_new_tokens,
    min_new_tokens=config.MIN_NEW_TOKENS_TRAIN,
    do_sample=True,
    temperature=config.TEMPERATURE_TRAIN,  # ç¡®è®¤è¿™é‡Œç”¨çš„æ˜¯é…ç½®å€¼
    ...
)
```

é¢å¤–æ£€æŸ¥ï¼šåœ¨ DebugLogitsProcessor ä¸­éªŒè¯ temperature æ˜¯å¦ç”Ÿæ•ˆ

```python
# åœ¨ DebugLogitsProcessor.forward() ä¸­ï¼ˆçº¦ 1890 è¡Œï¼‰
# æ·»åŠ æ£€æŸ¥
probs_no_temp = F.softmax(scores[0], dim=-1)  # ä¸åº”ç”¨ temperature
probs_with_temp = F.softmax(scores[0] / self.temperature, dim=-1)

max_no_temp = probs_no_temp.max().item()
max_with_temp = probs_with_temp.max().item()

print(f"  Max prob (no temp): {max_no_temp:.4f}")
print(f"  Max prob (with temp={self.temperature}): {max_with_temp:.4f}")
print(f"  Difference: {max_no_temp - max_with_temp:.4f}")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - max_prob (no temp) â‰ˆ max_prob (with temp): ğŸ”´ Temperature æ²¡ç”Ÿæ•ˆ
  - Difference < 0.05: ğŸ”´ Temperature ä½œç”¨å¤ªå¼±
  - æ—¥å¿—æ˜¾ç¤ºçš„ temperature å’Œé…ç½®ä¸ä¸€è‡´: ğŸ”´ è¢«è¦†ç›–

ğŸ’¡ å¦‚æœ temperature æ²¡ç”Ÿæ•ˆ:
  - æ£€æŸ¥æ˜¯å¦è¢« model.generation_config è¦†ç›–
  - æ£€æŸ¥æ˜¯å¦è¢« logits_processor ä¿®æ”¹
  - ç›´æ¥åœ¨ logits_processor ä¸­æ‰‹åŠ¨åº”ç”¨ temperature
""")

# ============================================================================
# è¯Šæ–­ 7: KL Penalty è¿‡å¼ºæ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 7: KL Penalty è¿‡å¼ºæ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ beta è¿‡å¤§ï¼ŒKL term ä¸»å¯¼ lossï¼Œæ¨¡å‹ä¸æ•¢åç¦» base modelã€‚

ä»è®­ç»ƒæ—¥å¿—ä¸­æå– KL æ•°æ®ï¼š

```python
# ä»æ—¥å¿—æå–ï¼ˆç¤ºä¾‹ï¼‰
kl_values = [0.02, 0.01, 0.03, ...]  # å®é™…çš„ KL å€¼
beta_values = [0.15, 0.15, 0.17, ...]  # å®é™…çš„ beta å€¼

import numpy as np
print(f"KL Statistics:")
print(f"  Mean: {np.mean(kl_values):.4f}")
print(f"  Std: {np.std(kl_values):.4f}")
print(f"  Range: [{np.min(kl_values):.4f}, {np.max(kl_values):.4f}]")

print(f"\\nBeta Statistics:")
print(f"  Mean: {np.mean(beta_values):.4f}")
print(f"  Range: [{np.min(beta_values):.4f}, {np.max(beta_values):.4f}]")

# è®¡ç®— KL penalty å  loss çš„æ¯”ä¾‹
avg_reward = 0.5  # ä»æ—¥å¿—ä¸­è·å–
avg_kl = np.mean(kl_values)
avg_beta = np.mean(beta_values)

kl_term = avg_beta * avg_kl
reward_term = avg_reward
total = abs(reward_term) + abs(kl_term)

print(f"\\nLoss Composition:")
print(f"  Reward term: {reward_term:.4f} ({abs(reward_term)/total*100:.1f}%)")
print(f"  KL term: {kl_term:.4f} ({abs(kl_term)/total*100:.1f}%)")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - KL mean < 0.01: ğŸ”´ æ¨¡å‹è¢«"é”æ­»"ï¼Œä¸æ•¢æ¢ç´¢
  - KL term å æ¯” > 70%: ğŸ”´ KL penalty ä¸»å¯¼ï¼Œreward ä¿¡å·å¤ªå¼±
  - Beta > 0.5: ğŸŸ¡ å¯èƒ½è¿‡å¼º

ğŸ’¡ å¦‚æœ KL è¿‡å¼º:
  - é™ä½ betaï¼ˆ0.15 â†’ 0.05ï¼‰
  - æˆ–å®Œå…¨ç§»é™¤ KL penaltyï¼ˆå®éªŒæ€§ï¼‰
  - æˆ–å¢å¤§ reward scaleï¼ˆè®© reward ä¿¡å·æ›´å¼ºï¼‰
""")

# ============================================================================
# è¯Šæ–­ 8: Logits æ ¹æºé—®é¢˜æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 8: Logits æ ¹æºé—®é¢˜ï¼ˆæ˜¯å¦æ¥è‡ª base modelï¼‰")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ base model è¾“å‡ºçš„ logits æœ¬èº«å°±æåº¦å°–é”ï¼Œé‚£ä¹ˆé—®é¢˜åœ¨æ ¹æºã€‚

æ£€æŸ¥æ–¹æ³•ï¼šæ¯”è¾ƒ base_model å’Œ policy_model çš„ logits

è¯·åœ¨ GRPO è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼ˆçº¦ 2800 è¡Œï¼‰ï¼š

```python
# åœ¨è®¡ç®— log_probs æ—¶ï¼ŒåŒæ—¶è®°å½• base model çš„ logits
with torch.no_grad():
    base_outputs = base_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
    base_logits = base_outputs.logits

policy_outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
policy_logits = policy_outputs.logits

# å¯¹æ¯”åˆ†æ
for i in range(min(2, len(batch_input_ids))):  # åªçœ‹å‰2ä¸ªæ ·æœ¬
    base_scores = base_logits[i, -1, :]  # æœ€åä¸€ä¸ªtokençš„logits
    policy_scores = policy_logits[i, -1, :]

    base_top5 = torch.topk(base_scores, 5)
    policy_top5 = torch.topk(policy_scores, 5)

    base_gap = (base_top5.values[0] - base_top5.values[1]).item()
    policy_gap = (policy_top5.values[0] - policy_top5.values[1]).item()

    print(f"\\n[Logits å¯¹æ¯” @sample{i}]")
    print(f"  Base model gap: {base_gap:.3f}")
    print(f"  Policy model gap: {policy_gap:.3f}")
    print(f"  Difference: {policy_gap - base_gap:.3f}")
```

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - Base gap > 7: ğŸ”´ é—®é¢˜æ¥è‡ª base model
  - Policy gap â‰ˆ Base gap: ğŸ”´ Policy model æ²¡å­¦åˆ°æ–°ä¸œè¥¿
  - Policy gap > Base gap: ğŸ”´ è®­ç»ƒè®©é—®é¢˜æ›´ä¸¥é‡äº†

ğŸ’¡ å¦‚æœæ˜¯æ ¹æºé—®é¢˜:
  - æ¢ä¸€ä¸ªä¸åŒçš„ base model
  - æˆ–åœ¨ SFT é˜¶æ®µå°±æ·»åŠ  entropy bonus
  - æˆ–åœ¨é¢„å¤„ç†æ—¶å¯¹ base model åš temperature scaling
""")

# ============================================================================
# è¯Šæ–­ 9: Repetition Penalty å‰¯ä½œç”¨æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 9: Repetition Penalty å‰¯ä½œç”¨æ£€æŸ¥")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šREP_PENALTY=1.18 å¯èƒ½å¤ªå¼ºï¼Œå¯¼è‡´æ¨¡å‹ä¸æ•¢ç”¨å¸¸è§è¯ï¼Œ
åªèƒ½è¾“å‡º"æœ€å®‰å…¨"çš„ä½é¢‘è¯ï¼Œåè€Œé™ä½å¤šæ ·æ€§ã€‚

å®éªŒæ–¹æ³•ï¼šä¸´æ—¶ç¦ç”¨ repetition penaltyï¼Œçœ‹ entropy æ˜¯å¦æ¢å¤

è¯·åœ¨ trainer.py ä¸­ä¸´æ—¶ä¿®æ”¹ï¼š

```python
# ç¬¬ 231 è¡Œé™„è¿‘
REP_PENALTY_TRAIN = 1.18 â†’ 1.0  # å®Œå…¨ç¦ç”¨

# æˆ–è€…åœ¨ generate() è°ƒç”¨æ—¶
out = model.generate(
    ...
    repetition_penalty=1.0,  # å¼ºåˆ¶è¦†ç›–
    ...
)
```

è¿è¡Œ 5-10 æ­¥ï¼Œè§‚å¯Ÿ Entropy æ˜¯å¦æœ‰å˜åŒ–ã€‚

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - ç¦ç”¨å Entropy ä¸Šå‡ > 0.1: ğŸ”´ REP_PENALTY æ˜¯ç½ªé­ç¥¸é¦–
  - ç¦ç”¨åæ— æ˜æ˜¾å˜åŒ–: âœ… REP_PENALTY ä¸æ˜¯ä¸»å› 

ğŸ’¡ å¦‚æœæ˜¯ REP_PENALTY é—®é¢˜:
  - é™ä½åˆ° 1.05ï¼ˆè½»å¾®æƒ©ç½šï¼‰
  - æˆ–å®Œå…¨ç¦ç”¨ï¼ˆ1.0ï¼‰
  - æ”¹ç”¨ frequency_penaltyï¼ˆæ›´æ¸©å’Œï¼‰
""")

# ============================================================================
# è¯Šæ–­ 10: æ•°æ®æ³„éœ²æ£€æŸ¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š è¯Šæ–­ 10: æ•°æ®æ³„éœ²æ£€æŸ¥ï¼ˆè®­ç»ƒ/GRPO æ˜¯å¦ç”¨åŒä¸€æ‰¹æ•°æ®ï¼‰")
print("="*80)

print("""
æ ¸å¿ƒé—®é¢˜ï¼šå¦‚æœ GRPO è®­ç»ƒæ—¶æŠ½åˆ°çš„æ ·æœ¬éƒ½æ˜¯ SFT è§è¿‡çš„ï¼Œæ¨¡å‹å¯èƒ½ç›´æ¥"èƒŒè¯µ"ã€‚

æ£€æŸ¥æ–¹æ³•ï¼š

1. æ£€æŸ¥æ•°æ®é‡‡æ ·é€»è¾‘ï¼ˆtrainer.py çº¦ 2666 è¡Œï¼‰
   - SFT å’Œ GRPO æ˜¯å¦ä»åŒä¸€ä¸ª dataset é‡‡æ ·ï¼Ÿ
   - æ˜¯å¦æœ‰ train/val splitï¼Ÿ

2. æ‰“å° GRPO è®­ç»ƒæ—¶çš„æ ·æœ¬ IDï¼š

```python
# åœ¨ GRPO è®­ç»ƒå¾ªç¯ä¸­
for step in range(config.GRPO_STEPS):
    batch = dataset.get_balanced_batch(config.GRPO_BATCH_SIZE)

    # æ·»åŠ è¯Šæ–­
    sample_ids = [s.id for s in batch]
    print(f"[GRPO Step {step}] Sample IDs: {sample_ids[:5]}...")  # æ‰“å°å‰5ä¸ª
```

3. ä¸ SFT è®­ç»ƒæ—¶çš„æ ·æœ¬ ID å¯¹æ¯”ï¼Œçœ‹æ˜¯å¦é‡å¤ã€‚

âš ï¸ åˆ¤æ–­æ ‡å‡†:
  - GRPO æ ·æœ¬ ID ä¸ SFT 100% é‡å : ğŸ”´ å®Œå…¨æ³„éœ²
  - é‡å  > 80%: ğŸŸ¡ ä¸¥é‡æ³„éœ²
  - é‡å  < 20%: âœ… å¯æ¥å—

ğŸ’¡ å¦‚æœæœ‰æ•°æ®æ³„éœ²:
  - å®ç° train/val splitï¼ˆ8:2ï¼‰
  - SFT ç”¨ trainï¼ŒGRPO ç”¨ val
  - æˆ–å¢åŠ æ•°æ®é‡ï¼Œé™ä½é‡å¤æ¦‚ç‡
""")

# ============================================================================
# æœ€ç»ˆæ€»ç»“
# ============================================================================
print("\n" + "="*80)
print("ğŸ’¡ è¯Šæ–­æµç¨‹å»ºè®®")
print("="*80)

print("""
æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡æ£€æŸ¥ï¼š

1. ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆæœ€å¯èƒ½ï¼‰:
   - è¯Šæ–­ 2: Base model entropyï¼ˆå¦‚æœ < 0.5ï¼Œç›´æ¥æ¢æ¨¡å‹ï¼‰
   - è¯Šæ–­ 3: Reward é€€åŒ–ï¼ˆå¦‚æœ std < 0.1ï¼Œä¿®å¤ judgeï¼‰
   - è¯Šæ–­ 4: Advantage é€€åŒ–ï¼ˆå¦‚æœ std < 0.2ï¼Œæ£€æŸ¥è®¡ç®—é€»è¾‘ï¼‰
   - è¯Šæ–­ 6: Temperature å¤±æ•ˆï¼ˆå¦‚æœæ²¡ç”Ÿæ•ˆï¼Œå¼ºåˆ¶åº”ç”¨ï¼‰

2. ğŸŸ¡ ä¸­ä¼˜å…ˆçº§:
   - è¯Šæ–­ 1: SFT è¿‡æ‹Ÿåˆï¼ˆå¦‚æœ loss < 0.1ï¼Œå‡å°‘æ­¥æ•°ï¼‰
   - è¯Šæ–­ 5: LoRA æ¢¯åº¦æ¶ˆå¤±ï¼ˆå¦‚æœ norm < 1e-6ï¼Œæ£€æŸ¥æ¢¯åº¦æµï¼‰
   - è¯Šæ–­ 7: KL è¿‡å¼ºï¼ˆå¦‚æœ KL < 0.01ï¼Œé™ä½ betaï¼‰

3. ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯èƒ½æ€§è¾ƒå°ï¼‰:
   - è¯Šæ–­ 8: Logits æ ¹æºé—®é¢˜
   - è¯Šæ–­ 9: REP_PENALTY å‰¯ä½œç”¨
   - è¯Šæ–­ 10: æ•°æ®æ³„éœ²

å»ºè®®ï¼š
1. å…ˆè¿è¡Œè¯Šæ–­ 2ï¼ˆæœ€å¿«ï¼Œåªéœ€ç”Ÿæˆä¸€æ¬¡ï¼‰
2. ç„¶åè¿è¡Œè¯Šæ–­ 3-4ï¼ˆä»æ—¥å¿—æå–æ•°æ®ï¼‰
3. å¦‚æœè¿˜æ²¡æ‰¾åˆ°åŸå› ï¼Œå†é€ä¸€æ·»åŠ è¯Šæ–­ä»£ç åˆ° trainer.py

æ¯å®Œæˆä¸€ä¸ªè¯Šæ–­ï¼Œè¯·æŠŠç»“æœå‘ç»™æˆ‘ï¼Œæˆ‘ä¼šå¸®æ‚¨åˆ†æï¼
""")

print("\n" + "="*80)
print("âœ… è¯Šæ–­è„šæœ¬å‡†å¤‡å®Œæˆ")
print("="*80)
