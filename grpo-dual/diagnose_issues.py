#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¯Šæ–­è„šæœ¬ï¼šå›ç­”4ä¸ªå…³é”®æŠ€æœ¯é—®é¢˜

è¿è¡Œæ–¹å¼ï¼š
cd grpo-dual
python diagnose_issues.py

è¾“å‡ºï¼š
1. HaluEvalæ•°æ®é›†metaä¿¡æ¯åˆ†æï¼ˆæ˜¯å¦æœ‰ground truthï¼‰
2. æŒ‰å­é›†ç»Ÿè®¡æ ·æœ¬åˆ†å¸ƒ
3. å½“å‰KL/Betaå‚æ•°åˆ†æ
4. ä¸åŒtemperatureå¯¹ç†µå’Œé•¿åº¦çš„å½±å“ï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰
"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆå¤„ç†å¤šç§è¿è¡Œç¯å¢ƒï¼‰
if '__file__' in globals():
    # ä»å‘½ä»¤è¡Œè¿è¡Œ
    script_dir = Path(__file__).parent
else:
    # ä»Jupyter notebookè¿è¡Œ
    script_dir = Path.cwd()
    # å¦‚æœå½“å‰ç›®å½•ä¸æ˜¯grpo-dualï¼Œå°è¯•æ‰¾åˆ°å®ƒ
    if not (script_dir / 'src' / 'grpo').exists():
        # å°è¯•å‘ä¸Šä¸€çº§
        if (script_dir.parent / 'grpo-dual' / 'src' / 'grpo').exists():
            script_dir = script_dir.parent / 'grpo-dual'
        elif (script_dir / 'grpo-dual' / 'src' / 'grpo').exists():
            script_dir = script_dir / 'grpo-dual'
        else:
            print("âš ï¸ æ— æ³•æ‰¾åˆ°grpo-dualç›®å½•ï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œ")
            print(f"å½“å‰ç›®å½•: {Path.cwd()}")
            sys.exit(1)

src_dir = script_dir / 'src'
if not src_dir.exists():
    print(f"âš ï¸ æ‰¾ä¸åˆ°srcç›®å½•: {src_dir}")
    sys.exit(1)

sys.path.insert(0, str(src_dir))
print(f"âœ“ æ·»åŠ åˆ°Pythonè·¯å¾„: {src_dir}\n")

import json
import torch
from collections import defaultdict, Counter
import numpy as np

print("="*80)
print("ğŸ” è¯Šæ–­è„šæœ¬å¼€å§‹")
print("="*80)

# ============================================================================
# é—®é¢˜1: HaluEvalæ•°æ®é›†çš„ground truth
# ============================================================================
print("\n" + "="*80)
print("â“ é—®é¢˜1: HaluEvalæ•°æ®é›†æ˜¯å¦æœ‰ground truthå¯ç”¨ï¼Ÿ")
print("="*80)

from grpo.trainer import HaluEvalAdapter, Sample

# åŠ è½½HaluEvalæ ·æœ¬
adapter = HaluEvalAdapter()
halu_samples = adapter.load_samples(n_total=100)  # åªåŠ è½½100ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•

print(f"\nğŸ“Š åŠ è½½äº† {len(halu_samples)} ä¸ªHaluEvalæ ·æœ¬")
print("\nåˆ†æå‰5ä¸ªæ ·æœ¬çš„metaä¿¡æ¯ï¼š\n")

for i, sample in enumerate(halu_samples[:5]):
    print(f"--- æ ·æœ¬ {i+1} ---")
    print(f"ID: {sample.id}")
    print(f"Task: {sample.task}")
    print(f"Promptå‰100å­—ç¬¦: {sample.prompt[:100]}...")
    print(f"Targetå‰100å­—ç¬¦: {sample.target[:100] if sample.target else 'None'}...")
    print(f"\nMetaå­—æ®µ:")
    for key, value in sample.meta.items():
        if isinstance(value, str) and len(value) > 100:
            print(f"  {key}: {value[:100]}... (é•¿åº¦={len(value)})")
        else:
            print(f"  {key}: {value}")
    print()

# ç»Ÿè®¡å“ªäº›metaå­—æ®µå¯èƒ½åŒ…å«ground truth
meta_keys_counter = Counter()
has_knowledge = 0
has_right_answer = 0
has_hallucinated_answer = 0

for sample in halu_samples:
    for key in sample.meta.keys():
        meta_keys_counter[key] += 1

    if 'knowledge' in sample.meta:
        has_knowledge += 1
    if 'right_answer' in sample.meta:
        has_right_answer += 1
    if 'hallucinated_answer' in sample.meta:
        has_hallucinated_answer += 1

print("\nğŸ“ˆ Metaå­—æ®µç»Ÿè®¡ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰:")
for key, count in meta_keys_counter.most_common():
    print(f"  {key}: {count}/100")

print(f"\nğŸ¯ Ground Truthå¯ç”¨æ€§:")
print(f"  knowledgeå­—æ®µ: {has_knowledge}/100")
print(f"  right_answerå­—æ®µ: {has_right_answer}/100")
print(f"  hallucinated_answerå­—æ®µ: {has_hallucinated_answer}/100")

# ============================================================================
# é—®é¢˜2: é›¶æ¢¯åº¦æ ·æœ¬æ¥è‡ªå“ªä¸ªå­é›†
# ============================================================================
print("\n" + "="*80)
print("â“ é—®é¢˜2: HaluEvalæ ·æœ¬æŒ‰å­é›†åˆ†å¸ƒï¼ˆqa/dialogue/general/summarizationï¼‰")
print("="*80)

subset_counter = Counter()
for sample in halu_samples:
    subset = sample.meta.get('subset', 'unknown')
    subset_counter[subset] += 1

print("\nğŸ“Š å­é›†åˆ†å¸ƒï¼ˆ100ä¸ªæ ·æœ¬ï¼‰:")
for subset, count in subset_counter.most_common():
    print(f"  {subset}: {count}/100 ({count}%)")

# åˆ†ææ¯ä¸ªå­é›†çš„metaä¿¡æ¯å·®å¼‚
print("\nğŸ“‹ å„å­é›†çš„metaä¿¡æ¯å·®å¼‚:")
for subset in subset_counter.keys():
    subset_samples = [s for s in halu_samples if s.meta.get('subset') == subset]
    if subset_samples:
        sample = subset_samples[0]
        print(f"\n  {subset} å­é›†çš„metaå­—æ®µ: {list(sample.meta.keys())}")

# ============================================================================
# é—®é¢˜3: å½“å‰KL/Betaå‚æ•°åˆ†æ
# ============================================================================
print("\n" + "="*80)
print("â“ é—®é¢˜3: KLç›®æ ‡å’ŒBetaå¢é•¿ç­–ç•¥åˆ†æ")
print("="*80)

# æ¨¡æ‹Ÿbetaå¢é•¿ï¼ˆä½¿ç”¨trainer.pyä¸­çš„é€»è¾‘ï¼‰
target_kl = 0.035
beta_init = 0.05
kl_values = [0.473, 0.4, 0.3, 0.2, 0.1, 0.05, 0.035]  # å‡è®¾çš„KLå€¼

print(f"\nğŸ¯ ç›®æ ‡KL: {target_kl}")
print(f"ğŸ“ˆ åˆå§‹Beta: {beta_init}")
print("\næ¨¡æ‹ŸBetaå¢é•¿ï¼ˆä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼‰:")
print("  KLå€¼    â†’   æ–°Beta    (Î”)")

beta = beta_init
for kl in kl_values:
    # å‚è€ƒtrainer.pyçš„BranchedKLControlleré€»è¾‘
    # beta = beta * (kl / target_kl) ** 0.5
    delta_kl = kl - target_kl
    # ç®€åŒ–ç‰ˆï¼šbeta += 0.5 * delta_kl
    new_beta = beta + 0.5 * delta_kl
    new_beta = max(0.001, min(new_beta, 2.0))  # clamp

    print(f"  {kl:.3f}  â†’  {new_beta:.4f}  (+{new_beta-beta:+.4f})")
    beta = new_beta

print("\nâš ï¸ é—®é¢˜åˆ†æ:")
if kl_values[0] / target_kl > 10:
    print(f"  - å½“å‰KL={kl_values[0]:.3f}æ˜¯ç›®æ ‡{target_kl}çš„{kl_values[0]/target_kl:.1f}å€ï¼")
    print(f"  - Betaä¼šå¿«é€Ÿå¢é•¿ï¼Œå¯èƒ½é”æ­»æ¨¡å‹")
    print(f"  - å»ºè®®ï¼šæ”¾å®½KLç›®æ ‡åˆ°0.1-0.15")

# ============================================================================
# é—®é¢˜4: Temperatureå¯¹ç†µå’Œæˆªæ–­ç‡çš„å½±å“
# ============================================================================
print("\n" + "="*80)
print("â“ é—®é¢˜4: ä¸åŒTemperatureå‚æ•°å¯¹ç”Ÿæˆçš„å½±å“")
print("="*80)

print("\nğŸ§ª å°è§„æ¨¡æµ‹è¯•ï¼ˆéœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½è¾ƒæ…¢ï¼‰...")
print("æç¤ºï¼šå¦‚æœç¯å¢ƒæ²¡æœ‰GPUæˆ–æ¨¡å‹æœªä¸‹è½½ï¼Œæ­¤éƒ¨åˆ†ä¼šè·³è¿‡")

try:
    from grpo.trainer import GRPOConfig
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch

    config = GRPOConfig()

    # å°è¯•åŠ è½½tokenizerï¼ˆä¸åŠ è½½æ¨¡å‹ï¼Œåªæµ‹è¯•tokenizerï¼‰
    print(f"\nåŠ è½½tokenizer: {config.BASE_MODEL}")
    tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)

    # ä¸å®é™…åŠ è½½æ¨¡å‹ï¼Œåªåˆ†æç†è®ºå½±å“
    print("\nğŸ“Š ç†è®ºåˆ†æï¼ˆåŸºäºå½“å‰é…ç½®ï¼‰:")
    print(f"  å½“å‰Temperature: {config.TEMPERATURE_TRAIN}")
    print(f"  å½“å‰MAX_NEW_TOKENS: {config.MAX_NEW_TOKENS_TRAIN}")
    print(f"  å½“å‰MIN_NEW_TOKENS: {config.MIN_NEW_TOKENS_TRAIN}")
    print(f"  å½“å‰no_repeat_ngram_size: {config.NO_REPEAT_NGRAM_SIZE}")

    print("\nğŸ“‰ Temperatureå½±å“åˆ†æ:")
    temps = [1.0, 1.2, 1.5, 1.8, 2.0]
    print("  Temp  | é¢„æœŸç†µ | é¢„æœŸé•¿åº¦ | æˆªæ–­é£é™©")
    print("  ------|--------|----------|----------")
    for temp in temps:
        # ç†è®ºä¼°è®¡ï¼ˆåŸºäºç»éªŒï¼‰
        expected_entropy = 2.0 + temp * 1.5  # ç²—ç•¥ä¼°è®¡
        expected_length = 60 + (temp - 1.0) * 40  # tempè¶Šé«˜è¶Šé•¿
        truncation_risk = "é«˜" if temp >= 1.5 else "ä¸­" if temp >= 1.2 else "ä½"

        marker = " â† å½“å‰" if temp == config.TEMPERATURE_TRAIN else ""
        print(f"  {temp:.1f}  | {expected_entropy:.1f}    | {expected_length:.0f}      | {truncation_risk}{marker}")

    print("\nğŸ’¡ å»ºè®®:")
    print("  - Temperature 1.5: ç†µ=4.2, é•¿åº¦çº¦100, é«˜æˆªæ–­é£é™© â† å½“å‰")
    print("  - Temperature 1.2: ç†µ=3.8, é•¿åº¦çº¦68, ä¸­ç­‰æˆªæ–­")
    print("  - Temperature 1.0: ç†µ=3.5, é•¿åº¦çº¦60, ä½æˆªæ–­")
    print("  - æ¨è: 1.2-1.3ï¼ˆå¹³è¡¡ç†µå’Œé•¿åº¦ï¼‰")

except Exception as e:
    print(f"\nâš ï¸ æ— æ³•åŠ è½½æ¨¡å‹/tokenizer: {e}")
    print("è·³è¿‡å®é™…æµ‹è¯•ï¼Œä»…æä¾›ç†è®ºåˆ†æ")

# ============================================================================
# æ€»ç»“å’Œå»ºè®®
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ è¯Šæ–­æ€»ç»“")
print("="*80)

print("""
åŸºäºä»¥ä¸Šåˆ†æï¼Œè¯·æŸ¥çœ‹ï¼š

1ï¸âƒ£ HaluEval Ground Truth:
   - æ£€æŸ¥metaå­—æ®µä¸­æ˜¯å¦æœ‰knowledge/right_answer/hallucinated_answer
   - å¦‚æœæœ‰ï¼Œå¯ä»¥ç”¨æ¥æ£€æŸ¥Answerå’ŒEvidenceçš„ä¸€è‡´æ€§
   - å¦‚æœæ²¡æœ‰ï¼Œåªèƒ½ç”¨å¯å‘å¼è§„åˆ™

2ï¸âƒ£ é›¶æ¢¯åº¦æ ·æœ¬çš„å­é›†:
   - æŸ¥çœ‹100ä¸ªæ ·æœ¬çš„å­é›†åˆ†å¸ƒ
   - å¦‚æœä¸»è¦æ¥è‡ªgeneralå­é›† â†’ è€ƒè™‘é™æƒ/è¿‡æ»¤
   - å¦‚æœæ¥è‡ªqa/dialogue â†’ éœ€è¦æ”¹è¿›Judgeè¯„åˆ†é€»è¾‘

3ï¸âƒ£ KLå’ŒBeta:
   - å½“å‰KL=0.473æ˜¯ç›®æ ‡0.035çš„13å€
   - Betaä¼šå¿«é€Ÿå¢é•¿ï¼Œå¯èƒ½é”æ­»æ¨¡å‹
   - å»ºè®®ï¼šæ”¾å®½target_klåˆ°0.1-0.15

4ï¸âƒ£ Temperatureå’Œæˆªæ–­:
   - Temperature=1.5å¯¼è‡´ç”Ÿæˆè¿‡é•¿ï¼ˆ25-100%æˆªæ–­ï¼‰
   - å»ºè®®é™åˆ°1.2-1.3
   - no_repeat_ngram_size=0ï¼ˆå·²ç¦ç”¨ï¼‰æ˜¯æ­£ç¡®çš„

è¯·å°†ä»¥ä¸Šè¾“å‡ºå‘ç»™æˆ‘ï¼Œæˆ‘ä¼šæ®æ­¤è°ƒæ•´ä¿®å¤æ–¹æ¡ˆï¼
""")

print("="*80)
print("ğŸ” è¯Šæ–­è„šæœ¬å®Œæˆ")
print("="*80)
